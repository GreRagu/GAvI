<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="brief-report"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Neuroimage</journal-id><journal-id journal-id-type="iso-abbrev">Neuroimage</journal-id><journal-title-group><journal-title>Neuroimage</journal-title></journal-title-group><issn pub-type="ppub">1053-8119</issn><issn pub-type="epub">1095-9572</issn><publisher><publisher-name>Academic Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">18387821</article-id><article-id pub-id-type="pmc">2643093</article-id><article-id pub-id-type="publisher-id">YNIMG5240</article-id><article-id pub-id-type="doi">10.1016/j.neuroimage.2008.02.005</article-id><article-categories><subj-group subj-group-type="heading"><subject>Technical Note</subject></subj-group></article-categories><title-group><article-title>Diffusion-based spatial priors for functional magnetic resonance images</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Harrison</surname><given-names>L.M.</given-names></name><email>l.harrison@fil.ion.ucl.ac.uk</email><xref rid="cor1" ref-type="corresp">&#x0204e;</xref></contrib><contrib contrib-type="author"><name><surname>Penny</surname><given-names>W.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Daunizeau</surname><given-names>J.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Friston</surname><given-names>K.J.</given-names></name></contrib></contrib-group><aff>Wellcome Trust Centre for Neuroimaging, UCL, London, UK</aff><author-notes><corresp id="cor1"><label>&#x0204e;</label>Corresponding author. Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London, 12 Queen Square, London, WC1N 3BG, UK. Fax: +44 207 813 1445. <email>l.harrison@fil.ion.ucl.ac.uk</email></corresp></author-notes><pub-date pub-type="pmc-release"><day>1</day><month>6</month><year>2008</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date pub-type="ppub">.--><pub-date pub-type="ppub"><month>6</month><year>2008</year></pub-date><volume>41</volume><issue>2-3</issue><fpage>408</fpage><lpage>423</lpage><history><date date-type="received"><day>20</day><month>11</month><year>2007</year></date><date date-type="rev-recd"><day>25</day><month>1</month><year>2008</year></date><date date-type="accepted"><day>1</day><month>2</month><year>2008</year></date></history><permissions><copyright-statement>&#x000a9; 2008 Elsevier Inc.</copyright-statement><copyright-year>2008</copyright-year><copyright-holder/><license><license-p>This document may be redistributed and reused, subject to <ext-link ext-link-type="uri" xlink:href="http://www.elsevier.com/wps/find/authorsview.authors/supplementalterms1.0">certain conditions</ext-link>.</license-p></license></permissions><abstract><p>We recently outlined a Bayesian scheme for analyzing fMRI data using diffusion-based spatial priors [Harrison, L.M., Penny, W., Ashburner, J., Trujillo-Barreto, N., Friston, K.J., 2007. Diffusion-based spatial priors for imaging. NeuroImage 38, 677&#x02013;695]. The current paper continues this theme, applying it to a single-subject functional magnetic resonance imaging (fMRI) study of the auditory system. We show that spatial priors on functional activations, based on diffusion, can be formulated in terms of the eigenmodes of a graph Laplacian. This allows one to discard eigenmodes with small eigenvalues, to provide a computationally efficient scheme. Furthermore, this formulation shows that diffusion-based priors are a generalization of conventional Laplacian priors [Penny, W.D., Trujillo-Barreto, N.J., Friston, K.J., 2005. Bayesian fMRI time series analysis with spatial priors. NeuroImage 24, 350&#x02013;362]. Finally, we show how diffusion-based priors are a special case of Gaussian process models that can be inverted using classical covariance component estimation techniques like restricted maximum likelihood [Patterson, H.D., Thompson, R., 1974. Maximum likelihood estimation of components of variance. Paper presented at: 8th International Biometrics Conference (Constanta, Romania)]. The convention in SPM is to smooth data with a fixed isotropic Gaussian kernel before inverting a mass-univariate statistical model. This entails the strong assumption that data are generated smoothly throughout the brain. However, there is no way to determine if this assumption is supported by the data, because data are smoothed <italic>before</italic> statistical modeling. In contrast, if a spatial prior is used, smoothness is estimated given non-smoothed data. Explicit spatial priors enable formal model comparison of different prior assumptions, e.g., that data are generated from a stationary (i.e., fixed throughout the brain) or non-stationary spatial process. Indeed, for the auditory data we provide strong evidence for a non-stationary process, which concurs with a qualitative comparison of predicted activations at the boundary of functionally selective regions.</p></abstract><kwd-group><title>Keywords</title><kwd>Single-subject fMRI</kwd><kwd>Spatial priors</kwd><kwd>Weighted graph Laplacian</kwd><kwd>[non-]stationary spatial process</kwd><kwd>Diffusion kernel</kwd><kwd>Eigenmodes</kwd><kwd>Covariance components</kwd><kwd>Matrix-variate normal density</kwd><kwd>Bayesian model comparison</kwd><kwd>Expectation maximization</kwd><kwd>Fisher-scoring</kwd></kwd-group></article-meta></front><body><sec><title>Introduction</title><p>Imaging neuroscience now pervades nearly every aspect of neurobiology; from cognitive psychology to neurogenetics. Its principal strength is the ability to make inferences about <italic>structure&#x02013;function relationships</italic> in the brain. However, statistical parametric mapping (SPM) (<xref rid="bib18" ref-type="bibr">Friston et al., 2006</xref>), one of the most widely used analyses of brain imaging data, does not support explicit inferences about the spatial aspects of functional anatomy. This is because it uses a mass-univariate approach, which models each voxel (i.e., point in the brain) separately. The need for models that consider influences among voxels, or multivariate models, stems from the fact that neuroimaging data are generated by spatially extended structures that necessarily involve more than one voxel, for example, the organization of retinotopically mapped responses in visual cortex is segregated into distinct cytoarchitectonic areas with defined boundaries. Despite this, it is currently not possible to infer whether a model with non-stationary smoothness (i.e., with boundaries) of functionally selective responses is better than a model with stationary smoothness (i.e., without boundaries).</p><p>This paper generalizes and finesses the framework described in <xref rid="bib25" ref-type="bibr">Harrison et al. (2007)</xref> that allows one to infer the presence of spatially organized responses and evaluate the evidence of different multivariate models of these responses. Critically, we now formulate a spatial prior in terms of the eigensystem of the diffusion kernel of a weighted graph Laplacian (<xref rid="bib5" ref-type="bibr">Chung and Yau, 2000</xref>). This reduces the computational complexity of the scheme substantially and discloses a clear link with two important methods used to analyze brain imaging data; (i) restricted maximum likelihood (ReML) (<xref rid="bib33" ref-type="bibr">Patterson and Thompson, 1974</xref>), used to estimate covariance components of a general linear model (GLM) (<xref rid="bib17" ref-type="bibr">Friston et al., 2002b</xref>) and (ii) Bayesian schemes based on Markov random field (MRF) theory (<xref rid="bib3" ref-type="bibr">Bishop, 2006</xref>); e.g., the Laplacian priors used in <xref rid="bib34" ref-type="bibr">Penny et al. (2005)</xref>. Furthermore, we generalize the scheme to spatiotemporal models of evoked responses. We demonstrate this by inverting models of functional magnetic resonance imaging (fMRI) time-series data, as opposed to the (second-level) GLMs of static data considered in <xref rid="bib25" ref-type="bibr">Harrison et al. (2007)</xref>. We formulate the problem in terms of diffusion kernels on arbitrary graphs (<xref rid="bib22" ref-type="bibr">Grady and Schwartz, 2003</xref>) and use them as constraints or empirical priors on the causes (i.e., model parameters) of observed data within a hierarchical Bayesian model. The diffusion kernel can be considered as the covariance of a Gaussian process prior (GPP) (<xref rid="bib29" ref-type="bibr">MacKay, 1998</xref>). In general, this prior is non-Gaussian as it is embedded on a surface, which encodes local (spatial) geometry of the functional anatomy, i.e., GLM parameter estimates. In this paper, we apply this framework to standard resolution (3&#x000a0;&#x000d7;&#x000a0;3&#x000a0;&#x000d7;&#x000a0;3&#x000a0;mm) fMRI data; however, we expect it to benefit analyses of, for example, high resolution fMRI, diffusion tensor imaging (DTI) and magneto-encephalographic (MEG) data. Indeed, ideas from <xref rid="bib25" ref-type="bibr">Harrison et al. (2007)</xref> for stationary processes have already been implemented in a model for source reconstruction of MEG data in SPM (<xref rid="bib20" ref-type="bibr">Friston et al., 2008</xref>).</p><p>Critically, this work provides a hypothesis-driven framework; in that a formal model embodies a hypothesis about how we think data are caused. This is important as we develop models that explicitly include spatiotemporal aspects of functional and anatomical principles. These aspects form the basis of <italic>empirical priors</italic> that are optimized in an informed way using the data. In addition, this enables us to formalise the question, &#x0201c;which model do our data support?&#x0201d; using <italic>Bayesian model comparison</italic>. Within a Bayesian paradigm, the intuition is that data are best explained using an optimal balance between model accuracy and complexity. For example, a fine-scaled temporal model of fMRI data is unlikely to enhance temporal feature detection, as its complexity is inappropriate for the coarse sampling rate of fMRI. Bayesian spatiotemporal models allow us to compare models with and without spatially coherent responses and ask whether this coherence is stationary (i.e., the same over space) or not. This sort of inference is central to asking questions about the nature of functional segregation in the cortex, or indeed subcortical structures, such as the amygdala or thalamus.</p><p>The potential benefits of this approach are far reaching in that it promises to answer questions, with a measured degree of certainty, about the &#x02018;texture&#x02019; and &#x02018;shape&#x02019; of functional responses. These questions are becoming increasingly important in imaging neuroscience, for example, investigating midbrain structures such as the periaqueductal gray (<xref rid="bib31" ref-type="bibr">Mobbs et al., 2007</xref>) in anxiety-related disorders, superior colliculus (<xref rid="bib41 bib49" ref-type="bibr">Schneider and Kastner, 2005; Sylvester et al., 2007</xref>), retinotopic maps of the visual cortex (<xref rid="bib9 bib10 bib42 bib54" ref-type="bibr">DeYoe et al., 1994; Engel et al., 1994; Sereno et al., 1994; Warnking et al., 2002</xref>) and lateral geniculate nucleus (<xref rid="bib27" ref-type="bibr">Haynes et al., 2005</xref>), and the fine functional structure within fusiform face area (<xref rid="bib23" ref-type="bibr">Grill-Spector et al., 2006</xref>). This last example is important as the correspondence that followed this paper indicated that the simple rules used to evaluate the &#x02018;texture&#x02019; of response were not correctly formulated, leading to serious criticism of some of their results (<xref rid="bib2 bib45" ref-type="bibr">Baker et al., 2007; Simmons et al., 2007</xref>). A more suitable analysis would be one that models explicitly the spatial features, or geometries, of neuronal responses we want to make inference about.</p><p>The paper is organized as follows: the first section motivates the use of multivariate, spatial models in relation to the mass-univariate approach, followed by a brief description of the theoretical fundaments of our approach. We then describe the model in detail with emphasis on using diffusion (heat) kernels to represent covariances within a hierarchical observation model. We provide intuition using synthetic data before applying the approach to fMRI data acquired during auditory processing. We end the paper by discussing some issues with the current implementation and future developments. Details regarding the implementation of the algorithm are given in <xref rid="app1" ref-type="sec">Appendices A and B</xref>.</p></sec><sec><title>Theoretical background</title><p>To highlight the importance of explicit spatial modeling of neuronal responses, we first consider the mass-univariate approach. A schematic of the data processing stream in SPM (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>) is shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. This is (excluding the pre-processing steps of realignment, co-registration and normalization) a three-stage procedure. The central panel contains a model of responses at one voxel that can explain data by, and only by, the explanatory variables in the design matrix (upper central panel). As this model is applied to each voxel independently, two extra processing stages are required to accommodate spatial dependencies; smoothing data (left panel) with a user specified kernel and <italic>post hoc</italic> adjustment of <italic>p</italic>-values (right panel), to model spatial dependencies. In this three-stage procedure, spatial properties (that necessarily involve more than one voxel) of neuronal responses are considered before and after modeling <italic>per se</italic>.</p><p>Spatially correlated fMRI data cannot be generated from this model, as there are no spatial parameters. As such it is not a generative model of spatially distributed changes in signal. This may seem trivial; however, it entails a deeper issue: in order to test a hypothesis, a data model has to be formulated, which can generate features that are salient to that hypothesis (e.g., temporally structured activity in spatially segregated and functionally selective brain regions). Given this, a prior over GLM parameters (and observation error) can be specified that encodes spatial dependence. The benefit of having an explicit spatial model of GLM parameters is that the three-stage procedure can be subsumed into one generative model. This allows comparison of different spatial models (e.g., stationary vs. non-stationary) and asks which of these has an optimal balance between accuracy (i.e., the expected log likelihood of the model) and complexity (i.e., the number of and uncertainty about parameter estimates). The challenge for requisite <italic>multivariate</italic> models is to embody the general organizational principles of functional segregation and integration (<xref rid="bib14" ref-type="bibr">Friston, 2002</xref>) into <italic>spatial</italic> models of how data are generated.</p><p>This has led to the development of more sophisticated models of fMRI data. Current Bayesian formulations of fMRI spatial models include the stationary Markov random field (MRF) priors of <xref rid="bib34" ref-type="bibr">Penny et al. (2005)</xref>. However, given the convoluted nature of gray matter and patchy functional segregation, a non-stationary model, where the degree of smoothness can depend on spatial location, may be required to model spatial features optimally. A step in this direction has been the use of the multiscale properties of wavelets as a fixed basis set (<xref rid="bib11" ref-type="bibr">Flandin and Penny, 2007</xref>); however, basis functions that adapt, given local geometric information may provide a more general framework. Non-Bayesian approaches include non-stationary filtering using scale (<xref rid="bib44" ref-type="bibr">Siegmund and Worsley, 1995</xref>) and rotation spaces (<xref rid="bib43" ref-type="bibr">Shafie et al., 2003</xref>), Canonical Correlation Analysis (<xref rid="bib13" ref-type="bibr">Friman et al., 2003</xref>) and edge-preserving bilateral filter kernels; closely related to the diffusion kernel used in this paper, via the Laplace&#x02013;Beltrami operator ((<xref rid="bib36 bib50 bib51" ref-type="bibr">Polzehl and Spokoiny, 2001; Tabelow et al., 2006; Walker et al., 2006</xref>). Although we consider only the simplest noise model in this paper, more realistic models in the literature include non-stationary spatial (<xref rid="bib57" ref-type="bibr">Worsley et al., 1999</xref>) and stationary spatiotemporal autoregressive models (<xref rid="bib35 bib55" ref-type="bibr">Penny et al., 2007; Woolrich et al., 2004</xref>).</p><sec><title>Spatiotemporal models for fMRI</title><p>The framework we propose has its roots in random field theory (RFT) (<xref rid="bib1 bib3" ref-type="bibr">Adler, 1981; Bishop, 2006</xref>), image processing (<xref rid="bib21 bib47 bib58" ref-type="bibr">Geman and Geman, 1984; Sochen et al., 1998; Zhang and Hancock, 2005</xref>) and machine learning (<xref rid="bib29 bib40" ref-type="bibr">MacKay, 1998; Rasmussen and Williams, 2006</xref>). As such, we consider <italic>parameter values</italic> of a GLM as a multi-dimensional random field over anatomical space and use graph-based models of diffusion to represent spatial dependence between voxels in a hierarchical Gaussian model. Basically, this entails estimating model parameters of imaging data in the usual way, but coupling the estimation of neighbouring parameters on a graph. This spatial coupling is represented by a spatial prior over nodes (i.e., voxels). Its covariance matrix is given by the diffusion kernel of a graph Laplacian, whose hyperparameters, e.g., dispersion of the kernel, are themselves learnt to provide an anisotropic, non-stationary spatial coherence that is optimized in relation to data.</p><p>We use a combinatorial approach to represent a discrete random field instead of discretising a continuous field. The advantage is that we can use standard results from graph theory to formulate the spatial covariance matrix, i.e., the matrix exponential of a graph (or combinatorial) Laplacian (<xref rid="bib4" ref-type="bibr">Chung, 1997</xref>), which, we think, simplifies the approach and avoids discretising a continuous operator over space. This combination of diffusion on graphs and hierarchical models provides a principled spatial model of the causes of data. It is a natural formulation in terms of kernel methods and probability densities that dissolves the multiple comparisons problem, because there is only one model of the entire image. In this way, we are able to fold pre-process smoothing and <italic>post hoc</italic> correction of <italic>p</italic>-values into the statistical model, i.e., the left and right panels into the central panel in <xref rid="fig1" ref-type="fig">Fig. 1</xref>.</p></sec><sec><title>Random fields, Gaussian processes and diffusion</title><p>A few words are required in order to explain some of the terminology used above. A &#x02018;random field&#x02019; refers to a collection of random variables, typically, over more than one dimension. They can be discrete, e.g., Markov random field, or continuous, e.g., a Gaussian random field, which is specified by a mean and covariance function. This idea can be extended to multi-dimensional random fields, where one or more numbers describe the field at each point in space, e.g., flow. Generalizing further, the field can be on a curved surface, e.g., temperature fluctuations on the two-dimensional surface of an object. This is an example of a continuous random field on a curved manifold. Random fields are exactly the same objects that provide distributional models for the statistics in SPMs and are used to adjust <italic>p</italic>-values in classical mass-univariate analyses of imaging data.</p><p>A Gaussian process prior <italic>is</italic> a continuous random field that is used within a Bayesian framework to constrain the estimation of parameters in an observation model e.g., autocorrelation functions over time or GLM parameters over space in a brain volume. GPPs are powerful as they provide (exact) analytic solutions. They are easily generalized to model non-Gaussian processes through specifying a transformation, e.g., log-transform to model a random field of strictly positive numbers (<xref rid="bib46" ref-type="bibr">Snelson and Ghahramani, 2007</xref>). These have been referred to as &#x02018;warped&#x02019; GPPs in the machine learning literature. Generalizing this notion further, a GPP can be defined on any arbitrary surface (sub-manifold), e.g., a cortical surface. We refer to this as an &#x02018;embedded&#x02019; GPP.</p><p>Diffusion occurs due to the random motion of &#x02018;particles&#x02019; within a random field, e.g., molecules in air, and is an example of a local Gaussian process. Diffusion in a continuous media has a discrete analogue on a graph (<xref rid="bib4" ref-type="bibr">Chung, 1997</xref>) that is comprised of a set of nodes and weighted edges. The Laplacian of a graph is computed using the edge weights, and the diffusion kernel is obtained from the matrix exponential of the Laplacian. This kernel is the solution of the heat equation that propagates a function on nodes of the graph from one moment to another. In other words, the diffusion kernel defines what the function will be at a later time. If the nodes are distributed over space this kernel contains spatial information and can be used as a spatial covariance matrix of a Gaussian density, thereby providing a representation of a discrete random field.</p></sec><sec><title>Hierarchal models and inference</title><p>Hierarchical models are at the heart of empirical Bayesian methods used in the analysis of neuroimaging data (<xref rid="bib16 bib17" ref-type="bibr">Friston et al., 2002a,b</xref>). Their appeal is that they provide an intuitive and easily implemented scheme to learn priors, given data. The central idea is that a prior over model parameters can be optimized (or learnt) through further constraints at a higher level. This leads to an observation model comprising levels, or a hierarchy, where each level provides constraints for the one below. Upward and downward passes of sufficient statistics enable learning of priors, given data and as such are called <italic>empirical</italic> priors. Hierarchical models are also used for efficient implementation of model inversion schemes, specifically with large data sets.</p><p>RFT is used for topological inference in neuroimaging; i.e., inference about topological features such as at peaks or the Euler characteristic (<xref rid="bib56" ref-type="bibr">Worsley et al., 1996</xref>). This considers the statistical field, e.g., of classical <italic>t</italic>-values, as deriving from a random field model of the data, where the error terms have a known (or estimable) spatial covariance function. Under this model, null distributions for topological measures (e.g., the Euler characteristic) can be derived and used to adjust associated <italic>p</italic>-values (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>). This implicitly controls false positive rates over the search volume. In our Bayesian setting we formulate a model to include a covariance function (matrix for a graph) over both GLM parameters and errors.</p><p>The use of RFT, in SPM, can be extended to consider <italic>parameter values</italic> of a generative model as a random field. This acts as a constraint on parameter estimates within a model of data, which itself has to be optimized or learnt; the random field has to be able to change shape for learning to occur, which is enabled by formulating it in terms of a diffusion process. As diffusion processes are locally Gaussian we can treat them as a GPP, which has been used to analyze many diverse types of spatial and temporal (<xref rid="bib53" ref-type="bibr">Wang et al., 2005</xref>) data, e.g., geostatistics of global weather (<xref rid="bib7" ref-type="bibr">Cornford et al., 2005</xref>). The appeal is that hierarchies of GPPs can be built within an analytically tractable probabilistic model; a Gaussian process model (<xref rid="bib40" ref-type="bibr">Rasmussen and Williams, 2006</xref>). In addition they can be used to implement efficient model inversion schemes for large data sets (<xref rid="bib39" ref-type="bibr">Quinonero-Candela and Rasmussen, 2005</xref>), which make them attractive for modeling neuroimaging data. They can be formulated in terms of graph-theoretic ideas, which provide a discrete representation of a continuous random field on an arbitrary manifold through the weights on a graph. As the graph has a finite number of nodes, this corresponds to a degenerate GPP (<xref rid="bib40" ref-type="bibr">Rasmussen and Williams, 2006</xref>).</p><p>A simulated volume of brain data is obtained by sampling from the probability density induced by a hierarchical model. A graphical representation of the generative and implicit recognition models used in this paper is shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>. Nodes and arrows represent random variables and conditional dependence respectively. The model, <italic>m</italic><sub><italic>k</italic></sub>, represents the structure and probability densities of the graph, which is a hypothesis of how data are generated. Parameters of a model, <italic>&#x003b2;</italic>, weight <italic>temporal</italic> explanatory variables are contained in a design matrix. These encode experimental conditions such as auditory stimulus presentation. Each voxel contains a vector resulting in a field of vectors. Hyperparameters, <italic>&#x003b1;</italic>, control the density over these parameters e.g., its <italic>spatial</italic> smoothness. These models can generate synthetic data that contain features similar to those observed in real data. By &#x02018;reversing&#x02019; the arrows we can invert the model and use it to <italic>recognize</italic> parameters of the model, given data. This recognition is shown in the lower panel of <xref rid="fig2" ref-type="fig">Fig. 2</xref>. The aim is, given data and a model, to estimate the probability density of the causes of data (i.e., model parameters).</p><p>This strategy is used to compute the posterior densities over parameters, hyperparameters and the model itself. The latter can be used to compare different models (i.e., hypotheses) of how the data were caused. A simple example of models we would like to compare is stationary vs. non-stationary spatial models. This is important, as it provides a quantitative measure of evidence in favour of one model compared with a competing hypothesis. The posterior over parameters encodes not only the most likely response, over anatomical space, but also a <italic>measure of uncertainty</italic> about the parameters, given data. This probability density can be used to identify patterns of response using posterior probability maps (PPMs) (<xref rid="bib15" ref-type="bibr">Friston and Penny, 2003</xref>). These are used to visualize structure&#x02013;function relationships that include a measure of uncertainty after fitting data. Thresholding the posterior density produces a map that represents regions of anatomical space where the probability of parameter values above a threshold has a specified degree of certainty, e.g., regions that have parameter values above zero with probability greater than 0.95. Examples are shown in <xref rid="fig4 fig5" ref-type="fig">Figs. 4b and 5</xref>e for synthetic and real data respectively. PPMs are important, as they are the basis for inference and hypothesis testing.</p></sec></sec><sec><title>A spatial model for fMRI</title><p>In this section, we formulate a two-level GLM in terms of matrix-variate<xref rid="fn1" ref-type="fn">1</xref> normal densities (<xref rid="bib24" ref-type="bibr">Gupta and Nagar, 2000</xref>). In what follows, we will denote a vectorised matrix with an arrow vec(X)=<italic>X</italic>&#x02192;. Our focus is the formulation of a multivariate normal model, with emphasis on covariance components and their hyperparameters. We start with a linear model, under Gaussian assumptions, of the form<disp-formula id="fd1"><label>(1)</label><mml:math id="M1" altimg="si26.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mi>&#x003b2;</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x0025b;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x0025b;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mo>&#x021d2;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>X</mml:mi><mml:mi>&#x003b2;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&#x0025b;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x0223c;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The left-hand expressions specify a hierarchical linear model and the right-hand defines the implicit generative density in terms of a likelihood, <italic>p</italic>(<italic>Y</italic>|<italic>X</italic>, <italic>&#x003b2;</italic>) and prior, <italic>p</italic>(<italic>&#x003b2;</italic>). <italic>N</italic><sub><italic>r,c</italic></sub> stands for a matrix-variate normal density, where the matrix A&#x000a0;&#x02208;&#x000a0;<inline-formula><mml:math id="M2" altimg="si27.gif" overflow="scroll"><mml:mi mathvariant="fraktur">R</mml:mi></mml:math></inline-formula><sup>r&#x000a0;&#x000d7;&#x000a0;c</sup>, has probability density function (pdf), <italic>p</italic>(<italic>A</italic>)&#x000a0;~&#x000a0;<italic>N</italic><sub><italic>r</italic></sub><sub>,<italic>c</italic></sub>(<italic>M</italic>,<italic>S</italic>&#x02297;<italic>K</italic>), with mean, <italic>M</italic>, of size <italic>r&#x000a0;</italic>&#x000d7;&#x000a0;<italic>c</italic>, and two covariances, <italic>S</italic> and <italic>K</italic>, of size <italic>r&#x000a0;</italic>&#x000d7;&#x000a0;<italic>r</italic> and <italic>c&#x000a0;</italic>&#x000d7;&#x000a0;<italic>c</italic>, for rows and columns respectively. Here, Y is a <italic>T&#x000a0;</italic>&#x000d7;&#x000a0;<italic>N</italic> data matrix and X is a <italic>T&#x000a0;</italic>&#x000d7;&#x000a0;<italic>P</italic> design matrix with an associated unknown <italic>P&#x000a0;</italic>&#x000d7;&#x000a0;<italic>N</italic> parameter matrix, <italic>&#x003b2;</italic>, so that <italic>r</italic><sub>1</sub>&#x000a0;=&#x000a0;<italic>T</italic>, <italic>r</italic><sub>2</sub>&#x000a0;=&#x000a0;<italic>P</italic>, <italic>c</italic><sub>1</sub>&#x000a0;=&#x000a0;<italic>c</italic><sub>2</sub>&#x000a0;=&#x000a0;<italic>N</italic>.</p><p>The errors at both levels have covariance <italic>S</italic><sub><italic>i</italic></sub> over rows i.e., time or regressors and <italic>K</italic><sub><italic>i</italic></sub> over columns i.e., voxels. Eq. <xref rid="fd1" ref-type="disp-formula">(1)</xref> is a typical model used in the analysis of fMRI data comprising <italic>T</italic> scans, <italic>N</italic> voxels and <italic>P</italic> parameters. The addition of the second level places empirical shrinkage priors on the parameters. This model can now be simplified by vectorising each component using the identity vec(<italic>ABC</italic>) = (<italic>C</italic><sup><italic>T</italic></sup> &#x02297; <italic>A</italic>)<italic>B</italic>&#x02192; (see <xref rid="app1" ref-type="sec">Appendix A</xref> and <xref rid="bib26" ref-type="bibr">Harville, 1997</xref>).<disp-formula id="fd2"><label>(2)</label><mml:math id="M3" altimg="si28.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>Z</mml:mi><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x0223c;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Where <italic>y</italic> = <italic>&#x00059;&#x020d7;,</italic><inline-formula><mml:math id="M4" altimg="si29.gif" overflow="scroll"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02297;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula>, <italic>b</italic> = <italic>&#x003b2;</italic>&#x02192;&#x020d7;, e<sub>i</sub> = <italic>&#x003b5;</italic>&#x02192;&#x020d7;<sub>i</sub>, <inline-formula><mml:math id="M5" altimg="si30.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M6" altimg="si31.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. &#x02297; is the Kronecker product of two matrices and <bold>I</bold><sub><italic>N</italic></sub> is the identity matrix of size <italic>N</italic>. The unknown covariances of the first and second level errors, <italic>&#x003a3;</italic>(<italic>&#x003b1;</italic>)<sub>1</sub> and <italic>&#x003a3;</italic>(<italic>&#x003b1;</italic>)<sub>2</sub>, depend on hyperparameters, <italic>&#x003b1;</italic>. The model parameters and hyperparameters are estimated using expectation maximization (EM) by maximizing a lower bound <italic>F</italic>, on the log-marginal likelihood<disp-formula id="fd3"><label>(3)</label><mml:math id="M7" altimg="si32.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>ln</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>&#x02265;</mml:mo><mml:mi>F</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="true">(</mml:mo><mml:mi>ln</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>&#x003a3;</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>&#x003a3;</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi>ln</mml:mi><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mi>&#x003a3;</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>Z</mml:mi><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>with respect to the parameters, <italic>b</italic>, in the E-step and the covariance hyperparameters, <italic>&#x003b1;</italic>, in the M-step. Here, <italic>&#x003a3;</italic>(<italic>&#x003b1;</italic>) represents the covariance of the data induced by both levels of the model. Although the bound in Eq. <xref rid="fd3" ref-type="disp-formula">(3)</xref> appears to be only a function of the hyperparameters, we will see later that the form of <italic>&#x003a3;</italic>(<italic>&#x003b1;</italic>) can depend on the parameters.</p><p>Confounds, such as scanner drift, and mean signal can be conveniently accommodated into the model above by transforming the data. Consider a GLM containing two partitions; one for the signal of interest, <italic>X</italic><sub>1</sub>, i.e., experimental design matrix, and confounds, <italic>X</italic><sub>2</sub>, containing a discrete cosine set and column of ones.<disp-formula id="fd4"><label>(4)</label><mml:math id="M8" altimg="si33.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">Y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x0025b;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></disp-formula></p><p>We can use the change of variables formula (second line, left side of Eq. <xref rid="fd5" ref-type="disp-formula">(5)</xref>) to transform this into a more convenient form. Given a function of data, <italic>R</italic>(<italic>Y</italic>), the lower bound is given by<disp-formula id="fd5"><label>(5)</label><mml:math id="M9" altimg="si34.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">Y</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mo stretchy="true">|</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="normal">Y</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mi>J</mml:mi><mml:msup><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mo>&#x021d2;</mml:mo><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="true">(</mml:mo><mml:mi>ln</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mover accent="true"><mml:mi>&#x003a3;</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">|</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:mover accent="true"><mml:mi>&#x003a3;</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi>ln</mml:mi><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mi>ln</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>which now includes an extra term, the Jacobian of the data transformation, <italic>J</italic> = |&#x02202;<italic>Y</italic>/<italic>&#x01ef8;</italic>|. Given the transformation, <italic>R</italic>(Y)&#x000a0;=&#x000a0;<italic>P</italic><sub><italic>r</italic></sub>Y<italic>P</italic><sub><italic>c</italic></sub>, its Jacobian is <italic>J</italic>&#x000a0;=&#x000a0;|<italic>P</italic><sub><italic>r</italic></sub>|<sup>&#x02212;&#x000a0;<italic>c</italic></sup>|<italic>P</italic><sub><italic>c</italic></sub>|<sup>&#x02212;&#x000a0;<italic>r</italic></sup>. If we chose <italic>P</italic><sub><italic>r</italic></sub>&#x000a0;=&#x000a0;<italic>I</italic><sub><italic>T&#x000a0;</italic></sub>&#x02212;&#x000a0;<italic>X</italic><sub>2</sub>(<italic>X</italic><sub>2</sub><italic><sup>T</sup>X</italic><sub>2</sub>)<sup>&#x02212;&#x000a0;1</sup><italic>X</italic><sub>2</sub><sup><italic>T</italic></sup>, i.e., the projection matrix to the null space of the confounds, and <italic>P</italic><sub><italic>c</italic></sub>&#x000a0;=&#x000a0;<bold>I</bold><sub><italic>N</italic></sub>, the model reduces conveniently to one partition, i.e., <italic>&#x01ef8;</italic>&#x000a0;=&#x000a0;<italic>&#x00058;&#x00303;</italic><sub>1</sub><italic>&#x003b2;&#x00303;</italic><sub>1</sub>&#x000a0;+&#x000a0;<italic>&#x003b5;&#x00303;</italic>&#x002dc;<sub>1</sub>, where<disp-formula id="fd6"><label>(6)</label><mml:math id="M10" altimg="si35.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mi mathvariant="bold">Y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>&#x0025b;</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x0223c;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>In this case, the Jacobian is constant and so we drop the tilde (i.e., by projecting the data and models onto the null space of the confounds, we can proceed as if there were no confounds). However, in general, a data transformation can be parameterized, in which case this term needs to be included in the objective function. The model inversion with EM will be described later (see also <xref rid="app1" ref-type="sec">Appendix A</xref>). First, we look at the hyperparameterization of the spatial covariances and the specific forms of <italic>K</italic>(<italic>&#x003b1;</italic>)<sub><italic>i</italic></sub> entailed by <italic>&#x003a3;</italic><sub><italic>i</italic></sub>&#x000a0;=&#x000a0;<italic>K</italic><sub><italic>i</italic></sub>&#x000a0;&#x02297;&#x000a0;<italic>S</italic><sub><italic>i</italic></sub>.</p><sec><title>The spatial priors</title><p>In the previous section, we reduced the problem of inverting a linear empirical Bayesian model to optimizing prior covariance components for noise and signal (<italic>i.e</italic>., optimizing the lower bound <italic>F</italic> with respect to the covariance parameters). In this section, we describe diffusion-based priors (<xref rid="bib25" ref-type="bibr">Harrison et al., 2007</xref>) and consider adaptive priors that are functions of the GLM parameters. In brief, we will assume that the error or noise covariance is spatially unstructured; i.e., <italic>&#x003a3;</italic><sub>1</sub>&#x000a0;=&#x000a0;<italic>K</italic><sub>1</sub>&#x000a0;&#x02297;&#x000a0;<italic>S</italic><sub>1</sub>, where <italic>K</italic>(<italic>&#x003b1;</italic>)<sub>1</sub>&#x000a0;=&#x000a0;<italic>&#x003c5;</italic><bold>I</bold><sub><italic>N</italic></sub> and <italic>S</italic><sub>1</sub>&#x000a0;=&#x000a0;<italic>P<sub>r</sub>P<sub>r</sub><sup>T</sup></italic>&#x000a0;=&#x000a0;<italic>P</italic><sub><italic>r</italic></sub> (i.e., projection is an idempotent transformation). For simplicity, we will assume that this is fixed over voxels; however, it is easy to specify a component for each voxel, as in conventional mass-univariate analyses.</p><p>For the neuronal activity (i.e., signal), we adopt an adaptive prior using a non-stationary diffusion kernel, which is based on a weighted graph Laplacian (<xref rid="bib4" ref-type="bibr">Chung, 1997</xref>), L(<italic>&#x003bc;</italic>,<italic>H</italic>), which is a function of the conditional expectation<xref rid="fn2" ref-type="fn">2</xref> of parameters, <italic>&#x003bc;</italic>&#x000a0;=&#x000a0;&#x02329;<italic>b</italic>&#x0232a;, and the embedding space metric, <italic>H</italic> (see next section).<disp-formula id="fd7"><label>(7)</label><mml:math id="M11" altimg="si36.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>S</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b7;</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The matrix L is a weighted graph Laplacian, which is a discrete analogue of the Laplace&#x02013;Beltrami operator used to model diffusion processes on a Riemannian manifold. An example of the latter is the dispersion of heat from a source on the curved surface of a thermally conductive material. Heuristically, this operator propagates quantities locally by dispersing a fixed proportion from each point on a surface or manifold to neighbouring locations. The manifold may itself be embedded in a higher-dimensional space, so that the ensuing diffusion can appear quite complicated. The diffusion kernel is computed using the matrix exponential, which we use as the covariance matrix of a spatial prior. Generally, during optimization the Laplacian is a function of the current image (parameter expectations) and parameters of the embedding space, L(<italic>&#x003bc;</italic><sup>(<italic>m</italic>)</sup>,<italic>H</italic><sup>(<italic>m</italic>)</sup>), where the superscript indicates the <italic>m</italic>th iteration. However, if the Laplacian is approximately constant then <italic>K</italic><sub>2</sub><sup>(<italic>m</italic>)</sup> can be evaluated much more simply (<xref rid="bib25" ref-type="bibr">Harrison et al., 2007</xref>). This approximation retains the edge-preserving character of the diffusive flow, without incurring the computational cost of reevaluating the Laplacian and its eigensystem. In our experience, weighted graph Laplacians based on the OLS estimate, <italic>&#x003bc;</italic><sub>ols</sub>, and an embedding space metric based on its covariance (see <xref rid="app1" ref-type="sec">Appendix B</xref>) give reasonable results.</p><p>Hyperparameters of this model comprise, <italic>&#x003b1;</italic>&#x000a0;=&#x000a0;{<italic>&#x003c5;</italic>,<italic>&#x003c4;</italic>,<italic>&#x003b7;</italic>}, where the first hyperparameter controls a stationary independent and identical (i.i.d.) noise component, the second the dispersion of the parameter image and third its amplitude. The row covariance <italic>&#x003b7;</italic> is in general <italic>P&#x000a0;</italic>&#x000d7;&#x000a0;<italic>P</italic>, where <italic>P</italic>&#x000a0;=&#x000a0;1 for all models in this paper. In the next section, we review graph Laplacians and the diffusion model in more detail and then conclude with a summary of the EM scheme used for optimization.</p></sec><sec><title>Diffusion on graphs</title><p>Here, we describe diffusion on a graph and illustrate how this is used in a spatial prior. This formulation is useful as it is easily extended to vector and matrix-valued images, which are necessary when modeling a general vector field of parameter estimates, e.g., for a factorial design. We start with some basic graph theory and then discuss diffusion in terms of graph Laplacians. The end point of this treatment is the form of the diffusion kernel, <italic>K</italic><sub>2</sub>, of the previous section. We will see that this is a function of the parameters that enables the prior smoothness to adapt locally to non-stationary features in the image of parameter estimates.</p><p>We consider a graph with vertices (nodes) and edges, <italic>&#x00393;</italic>&#x000a0;=&#x000a0;(<italic>V</italic>,<italic>E</italic>). The vertex and edge sets are <italic>V</italic> and <italic>E&#x000a0;</italic>&#x02286;&#x000a0;<italic>V&#x000a0;</italic>&#x000d7;&#x000a0;<italic>V</italic>, respectively. An element of each is <italic>v</italic><sub><italic>k&#x000a0;</italic></sub>&#x02208;&#x000a0;<italic>V</italic> and <italic>e</italic><sub><italic>ij&#x000a0;</italic></sub>&#x02208;&#x000a0;<italic>E</italic> (note that double indices in subscript distinguish an edge from an error term used in Eq. <xref rid="fd2" ref-type="disp-formula">(2)</xref>), where an edge connects two vertices <italic>v</italic><sub><italic>i</italic></sub> and <italic>v</italic><sub><italic>j</italic></sub>. The total number of nodes and edges are <italic>N<sub>V</sub></italic>&#x000a0;=&#x000a0;|<italic>V</italic>| and <italic>N<sub>E</sub></italic>&#x000a0;=&#x000a0;|<italic>E</italic>|, where the horizontal bars indicate cardinality, i.e., number of elements in the set. Neighbouring vertices are denoted by <italic>i&#x000a0;</italic>~&#x000a0;<italic>j</italic>. Each edge has a weight, <italic>w<sub>ij</sub></italic>, given by<disp-formula id="fd8"><label>(8)</label><mml:math id="M12" altimg="si37.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>exp</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mo>-</mml:mo><mml:mtext>ds</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mi>&#x003ba;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>for</mml:mtext><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The weights <italic>w</italic><sub><italic>ij&#x000a0;</italic></sub>&#x02208;&#x000a0;(0,1] encode the relationship between neighbouring voxels and are elements of the weight matrix W, which is symmetric; i.e., <italic>w</italic><sub><italic>ij</italic></sub>&#x000a0;=&#x000a0;<italic>w</italic><sub><italic>ji</italic></sub>. They play the role of conductivities, where a large value enables flow between voxels. <italic>&#x003ba;</italic> is a constant that controls velocity of diffusion, which we set to one. The degree of the <italic>i</italic>th vertex is defined as the sum of all neighbouring edge weights<disp-formula id="fd9"><label>(9)</label><mml:math id="M13" altimg="si38.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mo>&#x02200;</mml:mo><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.166em" height="0.3ex"/><mml:mspace width="0.166em" height="0.3ex"/><mml:mo>&#x02208;</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:math></disp-formula></p><p>The graph Laplacian can be conveniently formulated using results from linear circuit theory (<xref rid="bib22 bib48" ref-type="bibr">Grady and Schwartz, 2003; Strang, 2004</xref>). This has the advantage of representing node and edge spaces explicitly and is defined using the <italic>N</italic><sub><italic>E&#x000a0;</italic></sub>&#x000d7;&#x000a0;<italic>N</italic><sub><italic>V</italic></sub> edge-node (see subscript) incidence matrix<disp-formula id="fd10"><label>(10)</label><mml:math id="M14" altimg="si39.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>and <italic>N</italic><sub><italic>E&#x000a0;</italic></sub>&#x000d7;&#x000a0;<italic>N</italic><sub><italic>E</italic></sub> constitutive matrix, which is diagonal and contains edge weights, e.g., for the <italic>k</italic>th edge, <italic>C</italic><sub><italic>kk</italic></sub>&#x000a0;=&#x000a0;<italic>w</italic><sub><italic>ij</italic></sub>. Given these, the graph Laplacian is<disp-formula id="fd11"><label>(11)</label><mml:math id="M15" altimg="si40.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>C</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:math></disp-formula></p><p>This is equivalent to the un-normalized Laplacian of <italic>&#x00393;</italic>, <italic>L</italic>&#x000a0;=&#x000a0;<italic>D&#x000a0;</italic>&#x02212;&#x000a0;<italic>W</italic>, used in <xref rid="bib25" ref-type="bibr">Harrison et al. (2007)</xref>. The weights are a function of the distance, ds(<italic>v</italic><sub><italic>i</italic></sub>,<italic>v</italic><sub><italic>j</italic></sub>), on the surface of a parameter image, <italic>&#x003bc;</italic>(<italic>u</italic>), between vertices <italic>v</italic><sub><italic>i</italic></sub> and <italic>v</italic><sub><italic>j</italic></sub>. It is this distance that defines the nature of diffusion generated by the graph Laplacian.</p><p>More formally, we specify the distance by choosing a map, <italic>&#x003c7;</italic>, from the surface of the function <italic>&#x003bc;</italic>(<italic>u</italic>) to an embedding space, the Euclidean space of <inline-formula><mml:math id="M16" altimg="si41.gif" overflow="scroll"><mml:msup><mml:mi mathvariant="fraktur">R</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:math></inline-formula>, where <italic>n</italic>&#x000a0;=&#x000a0;<italic>n</italic><sub>d&#x000a0;</sub>+&#x000a0;<italic>n</italic><sub>f</sub> and <italic>n</italic><sub>d</sub> and <italic>n</italic><sub>f</sub> are the number of spatial and feature dimensions respectively (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>; <xref rid="bib25" ref-type="bibr">Harrison et al., 2007</xref>). Each space has a manifold and metric, (<italic>M</italic>,<italic>g</italic>) and (<italic>N</italic>,<italic>h</italic>), respectively.<disp-formula id="fd12"><label>(12)</label><mml:math id="M17" altimg="si42.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>&#x003c7;</mml:mi><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mi>N</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>&#x003c7;</mml:mi><mml:mo>:</mml:mo><mml:mi>u</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi>&#x003c7;</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo stretchy="true">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003c7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003c7;</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo stretchy="true">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003c7;</mml:mi><mml:mn>4</mml:mn></mml:msup><mml:mo stretchy="true">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>n</italic><sub>d</sub>&#x000a0;=&#x000a0;3, i.e., three spatial dimensions, <italic>n</italic><sub>f</sub>&#x000a0;=&#x000a0;1, i.e., a scalar field (for the examples in this paper, though this is easily generalized to vector fields) and (<italic>u</italic><sup>1</sup>,<italic>u</italic><sup>2</sup>,<italic>u</italic><sup>3</sup>) are local coordinates. Choosing a metric, <italic>H</italic>, of the embedding space (see below) and computing the Jacobian, <italic>J</italic>, we can calculate the induced metric, <italic>G</italic>, on <italic>&#x003bc;</italic>(<italic>u</italic>) (<xref rid="bib47" ref-type="bibr">Sochen et al., 1998</xref>). In matrix form<disp-formula id="fd13"><label>(13)</label><mml:math id="M18" altimg="si43.gif" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mtext>d</mml:mtext></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mtext>f</mml:mtext></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>H</italic><sub>d</sub> is the metric tensor (<xref rid="bib12" ref-type="bibr">Frankel, 2004</xref>) of the spatial domain. In this paper, we chose this to be Euclidian, i.e., <inline-formula><mml:math id="M19" altimg="si44.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mtext>d</mml:mtext><mml:mspace width="0.166em" height="0.3ex"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>d</mml:mtext></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, however, it could be arbitrary, e.g., from a cortical mesh used in anatomically informed models of fMRI or MEG source reconstruction. We fix <italic>H</italic><sub>f</sub> to that calculated in <xref rid="app1" ref-type="sec">Appendix B</xref>, based on <italic>&#x003bc;</italic><sub>ols</sub>.</p><p>The Jacobian (note this term refers to the matrix <italic>and</italic> its determinant) of the map is<disp-formula id="fd14"><label>(14)</label><mml:math id="M20" altimg="si45.gif" overflow="scroll"><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>&#x003c7;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:msup><mml:mi>u</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where derivatives are with respect to physical space; i.e., <italic>&#x003bc;</italic><sub><italic>x</italic></sub>&#x000a0;=&#x000a0;&#x02202;<italic>&#x003bc;</italic>&#x000a0;/&#x000a0;&#x02202;x, which are computed using central differences. The induced metric, on the surface of <italic>&#x003bc;</italic>(<italic>u</italic>), is then<disp-formula id="fd15"><label>(15)</label><mml:math id="M21" altimg="si46.gif" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>J</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>H</mml:mi><mml:mi>J</mml:mi></mml:mrow></mml:math></disp-formula>which is used to calculate the squared distance<disp-formula id="fd16"><label>(16)</label><mml:math id="M22" altimg="si47.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mtext>ds</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mtext>du</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>G</mml:mi><mml:mtext>du</mml:mtext></mml:mrow></mml:math></disp-formula>where du&#x000a0;=&#x000a0;(du<sup>1</sup>,du<sup>2</sup>,du<sup>3</sup>)<sup><italic>T</italic></sup> is displacement in anatomical space. As in general the Laplacian depends on geodesic distance on the embedded sub-manifold of an image we call it a geodesic graph Laplacian (GGL). If <inline-formula><mml:math id="M23" altimg="si48.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mtext>d</mml:mtext><mml:mspace width="0.166em" height="0.3ex"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mtext>d</mml:mtext></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <italic>H</italic><sub>f</sub>&#x000a0;=&#x000a0;0 then the Laplacian is based on Euclidean distance in anatomical space. We refer to this as a Euclidean graph Laplacian (EGL). The diffusion kernel can be computed efficiently using the eigenvalue decomposition.<disp-formula id="fd17"><label>(17)</label><mml:math id="M24" altimg="si49.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">L</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003a6;</mml:mi><mml:mi>&#x0039b;</mml:mi><mml:msup><mml:mi>&#x003a6;</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>&#x0039b;</mml:mi><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>&#x003a6;</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="true">[</mml:mo><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="true">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003a6;</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x0039b;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:msup><mml:mi>&#x003a6;</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>f</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x0039b;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mo>-</mml:mo><mml:mi>&#x0039b;</mml:mi><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Where the <italic>i</italic>th eigenvalue and vector of the Laplacian are represented by <italic>&#x003bb;</italic><sub><italic>i&#x000a0;</italic></sub>&#x02265;&#x000a0;0 and <italic>&#x003d5;</italic><sub><italic>i</italic></sub> (a column vector of length <italic>N</italic>) respectively. Given the eigensystem, the matrix exponential can be computed (<xref rid="bib32" ref-type="bibr">Moler and Van Loan, 2003</xref>) with the added benefit that many other computations are simplified. Related work using the eigensystem of a finite element approximation to the Laplace&#x02013;Beltrami operator has been used to smooth structural and fMRI data (<xref rid="bib38" ref-type="bibr">Qiu et al., 2006</xref>) and its diffusion kernel to model cortical thickness and density (<xref rid="bib6" ref-type="bibr">Chung et al., 2007</xref>). It is instructive to look at the eigenmodes to intuit the covariance components they represent. We will do this by relating them to a restricted maximum likelihood (ReML) (<xref rid="bib33" ref-type="bibr">Patterson and Thompson, 1974</xref>) based scheme, where the prior covariance, <italic>K</italic><sub>2</sub>, can be represented using <italic>n</italic> components, <italic>Q</italic><sub><italic>i</italic></sub> (<xref rid="bib17" ref-type="bibr">Friston et al., 2002b</xref>).<disp-formula id="fd18"><label>(18)</label><mml:math id="M25" altimg="si50.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003bb;</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The weight of each component, <inline-formula><mml:math id="M26" altimg="si51.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003bb;</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, can then be estimated, given data, using ReML, where there are <italic>n</italic> weights or hyperparameters to estimate. Compare this to an approximation of the diffusion kernel using <italic>n</italic> eigenmodes, where <italic>n</italic>&#x000a0;&#x0003c;&#x000a0;<italic>N</italic><disp-formula id="fd19"><label>(19)</label><mml:math id="M27" altimg="si52.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:msub><mml:mi>&#x003d5;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>&#x003d5;</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>That is, each eigenmode forms a covariance component, <italic>Q</italic><sub><italic>i</italic></sub>&#x000a0;=&#x000a0;<italic>&#x003d5;<sub>i</sub>&#x003d5;<sub>i</sub><sup>T</sup></italic>, which is weighted by a function of the Laplacian eigenvalue, i.e.,<inline-formula><mml:math id="M28" altimg="si53.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003bb;</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>, parameterized by <italic>&#x003c4;</italic>, which is an eigenvalue of the diffusion kernel. This perspective provides a useful interpretation of the diffusion kernel's eigenspectrum, examples of which are shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>i. Furthermore, it shows that our M-step is formally identical to ReML, when the covariance matrix is given by Eq. <xref rid="fd19" ref-type="disp-formula">(19)</xref>.</p><p>A key difference between the parameterization of the covariance matrices in Eqs. <xref rid="fd18 fd19" ref-type="disp-formula">(18) and (19)</xref> is that only one hyperparameter, <italic>&#x003c4;</italic>, has to be estimated in the latter. This is because a functional form (prescribed by diffusion) has been assumed over the weights. This is not the case for Eq. <xref rid="fd18" ref-type="disp-formula">(18)</xref> where all <italic>n</italic> weights would have to be estimated separately. This could be achieved easily; however, it does not use information about the spatial process encoded in the spectrum of the Laplacian (i.e., it would not conform to a diffusion prior). An additional benefit of Eq. <xref rid="fd19" ref-type="disp-formula">(19)</xref> is that eigenmodes of a GGL represent covariance components that are informed by the (spatial) geometry of GLM parameter estimates (in our case their OLS estimates). We show examples of these eigenmodes (covariance components) for synthetic and real data in <xref rid="fig4 fig5" ref-type="fig">Figs. 4f&#x02013;h and 5h&#x02013;i</xref>.</p><p>As seen in Eq. <xref rid="fd17" ref-type="disp-formula">(17)</xref> the diffusion kernel is a function of the eigensystem (of the Laplacian matrix). Given a form for the spatial prior that is in terms of a function of the Laplacian eigenspectrum, <inline-formula><mml:math id="M29" altimg="si54.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">&#x003b2;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02297;</mml:mo><mml:mi>&#x003a6;</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x0039b;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:msup><mml:mi>&#x003a6;</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>, the Laplacian prior used in <xref rid="bib34" ref-type="bibr">Penny et al. (2005)</xref> is recovered using a EGL and <italic>f</italic>(<italic>&#x0039b;</italic>)&#x000a0;=&#x000a0;<italic>&#x0039b;</italic><sup>&#x02212;&#x000a0;1</sup>, i.e., L is the spatial <italic>precision</italic> matrix, and diffusion-based prior using <italic>f</italic>(<italic>&#x0039b;</italic>)&#x000a0;=&#x000a0;exp(&#x02212;&#x000a0;<italic>&#x0039b;&#x003c4;</italic>), where exp(&#x02212;&#x000a0;L<italic>&#x003c4;</italic>) is a spatial <italic>covariance</italic> matrix. See <xref rid="app1" ref-type="sec">Appendix A</xref> for derivatives, required by the EM scheme, under these priors.</p><p>In this paper, we use a reduced eigensystem of <italic>n</italic>&#x000a0;=&#x000a0;<italic>N</italic>&#x000a0;/&#x000a0;10. Note that the spatial covariance matrix afforded by a diffusion kernel is a very large (non-sparse) matrix covering many voxels. This means any reduction helps enormously, in terms of computational load. This reduction produces reasonable results quickly (one slice ~&#x000a0;2&#x000a0;min using a standard personal computer) and can be motivated gracefully by noting the eigenvalues fall off relatively quickly, due to the fact that diffusion induces smoothness (see <xref rid="fig4" ref-type="fig">Fig. 4</xref>i). In the next section, we review briefly the EM algorithm used to optimize the parameters and covariance hyperparameters.</p></sec><sec><title>Expectation maximization</title><p>Inversion of the multivariate model in Eq. <xref rid="fd2" ref-type="disp-formula">(2)</xref> is straightforward and can be formulated in terms of expectation maximization (EM). EM entails the iterative application of an E-step and M-step (<xref rid="bib8 bib19 bib17" ref-type="bibr">Dempster et al., 1977; Friston et al., 2007; Friston et al., 2002b</xref>). Pseudo-code is given in <xref rid="fig3" ref-type="fig">Fig. 3</xref> and expressions for computing all quantities used in the algorithm are provided in <xref rid="app1" ref-type="sec">Appendix A</xref>. We update hyperparameters using a Fisher-scoring scheme.<xref rid="fn3" ref-type="fn">3</xref><bold>I</bold>(<italic>&#x003b1;</italic>) is the expected information matrix, see <xref rid="bib52" ref-type="bibr">Wand (2002)</xref>, with element <italic>I</italic><sub><italic>kn</italic></sub>, where the expectation, &#x02329; &#x0232a;, is over the marginal likelihood of the data, <italic>&#x025bf;<sub>&#x003b1;</sub>F</italic> is the score, i.e., a vector of gradients (where the <italic>k</italic>th element is &#x02202;<italic>F</italic>&#x000a0;/&#x000a0;&#x02202;<italic>&#x003b1;</italic><sub><italic>k</italic></sub>) with respect to covariance hyperparameters and <italic>&#x003a3;</italic> is the current [restricted] maximum likelihood (ReML) estimate of the data covariance.</p><p>In summary, to invert our model we simply specify the covariances and their derivatives (see <xref rid="tbl1" ref-type="table">Table 1</xref>). These enter an M-step to provide ReML estimates of covariance hyperparameters. <italic>&#x003a3;</italic>(<italic>&#x003b1;</italic>)<sub><italic>i</italic></sub> is then used in the E-step to provide the conditional density of the parameters. E and M-steps are iterated until convergence, after which, <italic>F</italic> can be used as a lower bound approximation to the log-evidence or log-likelihood. This represents the accuracy of a model and its complexity, which depend on the number of free parameters and uncertainty in their conditional estimates (see <xref rid="app1" ref-type="sec">Appendix B</xref>; <xref rid="bib25 bib19" ref-type="bibr">Harrison et al., 2007 and Friston et al., 2007</xref>). This means that if two competing models are equally accurate, but one has more free parameters than the other; the model with less parameters has a greater log-evidence. In this way, the procedure embodies the principle of Occam's Razor, &#x0201c;All things being equal, the simplest solution is the best&#x0201d; (<xref rid="bib30" ref-type="bibr">MacKay, 2003</xref>). This enables comparison of models with a different number of free parameters, as we will see later when comparing models based on different spatial priors. By convention, one requires the difference in log-evidence to be greater then three (i.e., a relative likelihood of about 20:1).</p><p>We now have all the components of a generative model that, when inverted, provides parameter estimates that are adaptively smooth, with edge-preserving characteristics. Furthermore, this smoothing is chosen automatically and optimizes the evidence of the model.</p></sec></sec><sec><title>Model comparison</title><p>In this section, we compare the performance of three different models of the same data. These models differed in the form of spatial covariance of the prior over voxels; (i) global shrinkage priors (GSP) that are spatially independent, i.e., <italic>K</italic><sub>2</sub>&#x000a0;=&#x000a0;<bold>I</bold><sub><italic>N</italic></sub>; (ii) diffusion kernel of a Euclidean graph Laplacian (EGL) and (iii) diffusion kernel of a geodesic graph Laplacian (GGL). Each model was optimized given synthetic and real fMRI data using the EM algorithm described above. Parameter estimates, posterior probability maps<xref rid="fn4" ref-type="fn">4</xref> (PPMs) and model evidences were compared as described next.</p><sec><title>Synthetic data</title><p>Synthetic data are shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>, where a known two-dimensional spatial signal (shown on the left of <xref rid="fig4" ref-type="fig">Fig. 4</xref>a) and design matrix containing temporal components (top of 4a), were used to simulate data. The design matrix contains two partitions; the first column is the effect of interest, which is weighted by the known spatial signal, while the remaining columns represent confounds and contain low frequency oscillations used to simulate scanner drift and a mean term. An example of an observed time-series (blue) from the marked pixel is shown below. This comprised a component of interest (red), confounds (green) and i.i.d. Gaussian noise. The ordinary least squares (OLS) estimates of the signal of interest, i.e., image of parameter estimates for the first column, are shown on the right of <xref rid="fig4" ref-type="fig">Fig. 4</xref>a. Compare this with posterior mean estimates from GSP, EGL and GGL-based spatial priors shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>b, along with PPMs, thresholds at <italic>p</italic>(<italic>b</italic>&#x000a0;&#x0003e;&#x000a0;0.33)&#x000a0;&#x0003e;&#x000a0;0.95. The differences are clear, with poor recovery using GSP, blurred mean with rounded edges of the central image with EGL and preservation of the majority of this edge using GGL. Two kernels,<xref rid="fn5" ref-type="fn">5</xref> which encode spatial dependences between a pixel (marked with an open circle) and others in its neighbourhood, are shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>c along with a plot of edge weights that uses the second and third eigenvectors as coordinates. Each line segment of this plot represents an edge of the graph and is a useful way to view the [an]-isotropy of a Laplacian matrix. These reveal spatial features of the OLS parameter estimates encoded in the GGL that are not present for EGL. Predictions from the two marked pixels in <xref rid="fig4" ref-type="fig">Fig. 4</xref>c are shown in <xref rid="fig4" ref-type="fig">Figs. 4</xref>d and e. <xref rid="fig4" ref-type="fig">Fig. 4</xref>e demonstrates detection of spurious signal that is not present in the data using GSP and EGL. This does not occur using GGL.</p><p>Eigenmodes of EGL and GGL are shown in <xref rid="fig4" ref-type="fig">Figs. 4</xref>f and g respectively (formatted as images). Note that the first eigenmode is not shown as this is constant over the graph. These can be regarded as components of the empirical prior covariance over voxels. They provide insight into the feature preserving nature of GGL; note the central region of the OLS parameter image is encoded in its eigenmodes, which means that parameters at two locations (a fixed distance apart) within the central region are more likely to covary, compared to when one location is outside this region. As such, they are non-stationary functions over the graph. This is not the case for EGL, whose eigenmodes are stationary. An example covariance component (fourth eigenmode) is shown in the top row of 4h and full diffusion kernel (i.e. the sum of all eigenmodes weighted by their eigenvalues) below. Spectra, i.e. eigenvalues, <inline-formula><mml:math id="M30" altimg="si56.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, of the EGL diffusion kernel are shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>i for two different values of <italic>&#x003c4;</italic>. This shows dependence of the spectrum on <italic>&#x003c4;</italic>. Note the rapid decay with larger <italic>&#x003c4;</italic>. Eigenmodes with small eigenvalues contribute little to the total covariance matrix; this is the rationale for using a reduced eigensystem. The results of Bayesian model comparison are given in <xref rid="tbl2" ref-type="table">Table 2</xref> and confirm that the evidence for the GGL-based prior is largest, which concurs with the known non-stationarity of the data set.</p></sec><sec><title>fMRI data</title><p>Results for fMRI data collected during auditory stimulation are shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>. These data are available freely at <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/data/auditory.html">http://www.fil.ion.ucl.ac.uk/spm/data/auditory.html</ext-link> and were pre-processed as described in the SPM manual, with the exception of not smoothing data. A simple design matrix with two partitions (auditory stimulus and confounds) was used (see design matrix in upper right of <xref rid="fig5" ref-type="fig">Fig. 5</xref>a). This is a very simple experimental design, with the effect of interest encoded in the first column. This means that parameter estimates of this effect form a scalar field over anatomical space. The main effects of auditory input (first column), from two slices (22 and 23 of 46) through the auditory cortex, are shown in <xref rid="fig5" ref-type="fig">Figs. 5</xref>a&#x02013;c for GSP, EGL and GGL-based spatial priors respectively. For comparison, we include mass-univariate parameter estimates in <xref rid="fig5" ref-type="fig">Fig. 5</xref>d, using the conventional practice of smoothing data with a 6&#x000a0;mm Gaussian kernel. Compared with conventional smoothing of the data, differences in estimated responses in <xref rid="fig5" ref-type="fig">Figs. 5</xref>a&#x02013;c are clear, with noisy estimates in <xref rid="fig5" ref-type="fig">Fig. 5</xref>a, smooth parameter images in <xref rid="fig5" ref-type="fig">Fig. 5</xref>b and less attenuation of signal at peaks in <xref rid="fig5" ref-type="fig">Fig. 5</xref>c, along with smooth estimates within quiescent regions (the colour scale beneath the images indicates percent signal change). This is due to the border-preserving nature of the non-stationary prior, which allows the degree of smoothness of a parameter image to vary over space. This means that parameter images look sharper, as edges between functionally segregated regions are preserved and not blurred by the constraint of stationarity.</p><p>Bayesian model comparison revealed the non-stationary GGL model in <xref rid="fig5" ref-type="fig">Fig. 5</xref>c had the greatest evidence (see Table 2). This model was able to extract the structured deployment of cortical responses that are otherwise blurred by EGL. Note that this comparison could not have been made if data were smoothed outside the statistical model. Local kernels and PPMs, i.e., maps of voxels where the model is 95% sure that the effect size is greater than 2% of the global mean (for slice 22 of 46), are shown for EGL and GGL in <xref rid="fig5" ref-type="fig">Fig. 5</xref>e. PPMs represent statistical inferences with clear differences in that &#x02018;active&#x02019; voxels using EGL are reduced to &#x02018;blobs&#x02019;, whereas filamentous responses are recovered for GGL, corresponding to their genesis in gray matter. This difference is crucial as decisions regarding data are based on such inferences. The PPM using GGL is shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>f overlaid on an anatomical image (at the same resolution as functional data). White matter has, in general, a lighter shade in this image, which shows &#x02018;activations&#x02019; adjacent to white matter and concurs qualitatively with our expectation that BOLD signal has a cortical origin.</p><p>Predictions from EGL and GGL-based models are shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>g at the boundary of response in the left auditory cortex (at the location of the green kernel in <xref rid="fig5" ref-type="fig">Fig. 5</xref>e). These show a poor fit for EGL suggesting that the isotropy assumption is inappropriate for these data. Eigenmodes (in image format) from EGL and GGL in <xref rid="fig5" ref-type="fig">Figs. 5</xref>h&#x02013;i show peaks in the auditory regions for GGL, but not for EGL. Again, these reveal the non-stationary nature of the GGL-based spatial covariance, compared with EGL.</p></sec></sec><sec sec-type="discussion"><title>Discussion</title><p>We have outlined a Bayesian scheme to estimate the optimal smoothing of conditional parameter estimates, given a diffusion-based spatial prior and have applied it to single-subject fMRI data. The contrast between stationary and non-stationary spatial models is remarkable and suggests that the isotropic assumption implicit in conventional smoothing is not appropriate for these data. We have shown this formally using Bayesian model comparison and qualitatively by comparing predictions at a functional boundary. Our approach provides a principled way to compare assumptions about the spatial nature of data that would otherwise not be possible using the standard approach of smoothing data at a pre-processing stage of analysis. Diffusion-based spatial priors allow the strong assumption of isotropy to be relaxed. This is important as the brain is comprised of functional structures that have different spatial scales e.g., cortical and subcortical.</p><p>Formulating the model in terms of the eigenmodes of a weighted graph Laplacian allows us to make contact with classical covariance component estimation, i.e., ReML-based schemes (<xref rid="bib17 bib33" ref-type="bibr">Friston et al., 2002b; Patterson and Thompson, 1974</xref>) and conventional Laplacian priors (<xref rid="bib34" ref-type="bibr">Penny et al., 2005</xref>). Given these eigenmodes, the emphasis is then on finding a parameterized function of their eigenvalues that best explains data; for example, the diffusion-based prior in this paper uses a function parameterized by <italic>&#x003c4;</italic>, i.e., <italic>f</italic>(<italic>&#x0039b;</italic>,<italic>&#x003c4;</italic>)&#x000a0;=&#x000a0;exp(&#x02212;&#x000a0;<italic>&#x0039b;&#x003c4;</italic>). This diffusion kernel specifies a spatial process where the shape of a local neighbourhood is represented by edge weights and whose scale is controlled by <italic>&#x003c4;</italic>. This reduces the problem to optimizing <italic>&#x003c4;</italic>, which also produces compelling results of the sort reported above.</p><p>The usefulness of the Laplacian eigensystem has also been explored in regularization schemes for image restoration and smoothing. However, there is a substantial distinction between regularization and Bayesian modeling. Regularization parameters control the effective complexity of a model and determine the degree of over-fitting (<xref rid="bib3" ref-type="bibr">Bishop, 2006</xref>), whereas Bayesian schemes provide a principled approach to represent and estimate uncertainty of such parameters, using hierarchical models. As such the Bayesian paradigm provides a powerful framework, where model complexity is included in an estimate of the probability of data, given the model, e.g., where models may differ depending on the form of prior used to embody a hypothesis about how data are generated.</p><p>In our scheme, data are not regularized (smoothed). Instead model parameters are represented as random fields that have, in general, non-stationary smoothness. The aim is not only to estimate a posterior density on these fields, but also to estimate optimal regularization parameters, such as the dispersion of a diffusion kernel. This enables comparison of different hypotheses about the data; e.g., what are the odds that a non-stationary spatial process generated the data compared with a stationary process. Given this, we consider the material in this paper to go beyond simple regularization schemes based on the Laplace&#x02013;Beltrami operator.</p><p>We have reported only two slices of data analyzed using our approach, which reflects an outstanding issue. As there is only one model of the data, there is just one Laplacian, which is over all voxels in the brain. The associated spatial prior corresponds to a covariance matrix of the order 10<sup>5</sup>, which is computationally prohibitive for current standard personal computers. This is a general implementation issue for Gaussian process priors that require inversion of large matrices. The current implementation of Penny et al.'s algorithm in SPM processes one slice at a time, meaning that a 2D Laplacian is used instead of 3D. While data are measured slice by slice, the underlying functional anatomy is in general 3D, which suggests that 3D models are appropriate. A possible solution is to use a weighted graph Laplacian to partition (<xref rid="bib22 bib37" ref-type="bibr">Grady and Schwartz, 2003; Qui and Hancock, 2005</xref>) a brain volume into computationally manageable pieces. A diffusion-based prior would then be used for each partition independently. Another approach, which we are currently exploring, is to generate data on, and only on, the cortical surface. This generative model could be used to explain observed responses that have been assigned to the cortical mesh using anatomically informed basis functions (<xref rid="bib28" ref-type="bibr">Kiebel et al., 2000</xref>). Alternatively, the model could generate 3D data by diffusing the 2D cortical response over a 3D mesh. This would have the advantage of conforming to the known anatomical generation of BOLD signal, requiring smaller prior covariance matrices, while modeling full 3D image data.</p></sec></body><back><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Adler</surname><given-names>R.J.</given-names></name></person-group><chapter-title>The Geometry of Random Fields</chapter-title><year>1981</year><publisher-name>Wiley</publisher-name><publisher-loc>London</publisher-loc></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>C.I.</given-names></name><name><surname>Hutchison</surname><given-names>T.L.</given-names></name><name><surname>Kanwisher</surname><given-names>N.</given-names></name></person-group><article-title>Does the fusiform face area contain subregions highly selective for nonfaces?</article-title><source>Nat Neurosci</source><volume>10</volume><year>2007</year><fpage>3</fpage><lpage>4</lpage><pub-id pub-id-type="pmid">17189940</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>C.</given-names></name></person-group><chapter-title>Pattern recognition for machine learning</chapter-title><year>2006</year><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>F.</given-names></name></person-group><chapter-title>Spectral graph theory</chapter-title><year>1997</year><publisher-name>Providence</publisher-name><comment>Rhode Island, American mathematics society</comment></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>F.</given-names></name><name><surname>Yau</surname><given-names>S.T.</given-names></name></person-group><article-title>Discrete Green's functions</article-title><source>J. Comb. Theory, Ser. A</source><volume>91</volume><year>2000</year><fpage>191</fpage><lpage>214</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>M.K.</given-names></name><name><surname>Dalton</surname><given-names>K.M.</given-names></name><name><surname>Shen</surname><given-names>L.</given-names></name><name><surname>Evans</surname><given-names>A.C.</given-names></name><name><surname>Davidson</surname><given-names>R.J.</given-names></name></person-group><article-title>Weighted fourier series representation and its application to quantifying the amount of gray matter</article-title><source>IEEE Trans. Med. Imag.</source><volume>26</volume><year>2007</year><fpage>566</fpage><lpage>581</lpage></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cornford</surname><given-names>D.</given-names></name><name><surname>Csato</surname><given-names>L.</given-names></name><name><surname>Opper</surname><given-names>M.</given-names></name></person-group><article-title>Sequential, Bayesian geostatistics: a principled method for large data sets</article-title><source>Geogr. Anal.</source><volume>37</volume><year>2005</year><fpage>183</fpage><lpage>199</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dempster</surname><given-names>A.</given-names></name><name><surname>Laird</surname><given-names>N.</given-names></name><name><surname>Rubin</surname><given-names>D.</given-names></name></person-group><article-title>Maximum likelihood from incomplete data via the EM algorithm</article-title><source>J. R. Stat. Soc. Ser. B</source><volume>39</volume><year>1977</year><fpage>1</fpage><lpage>38</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeYoe</surname><given-names>E.A.</given-names></name><name><surname>Bandettini</surname><given-names>P.</given-names></name><name><surname>Neitz</surname><given-names>J.</given-names></name><name><surname>Miller</surname><given-names>D.</given-names></name><name><surname>Winans</surname><given-names>P.</given-names></name></person-group><article-title>Functional magnetic resonance imaging (FMRI) of the human brain</article-title><source>J. Neurosci. Methods</source><volume>54</volume><year>1994</year><fpage>171</fpage><lpage>187</lpage><pub-id pub-id-type="pmid">7869750</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>S.A.</given-names></name><name><surname>Rumelhart</surname><given-names>D.E.</given-names></name><name><surname>Wandell</surname><given-names>B.A.</given-names></name><name><surname>Lee</surname><given-names>A.T.</given-names></name><name><surname>Glover</surname><given-names>G.H.</given-names></name><name><surname>Chichilnisky</surname><given-names>E.J.</given-names></name><name><surname>Shadlen</surname><given-names>M.N.</given-names></name></person-group><article-title>fMRI of human visual cortex</article-title><source>Nature</source><volume>369</volume><year>1994</year><fpage>525</fpage><pub-id pub-id-type="pmid">8031403</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flandin</surname><given-names>G.</given-names></name><name><surname>Penny</surname><given-names>W.D.</given-names></name></person-group><article-title>Bayesian fMRI data analysis with sparse spatial basis function priors</article-title><source>NeuroImage</source><volume>34</volume><year>2007</year><fpage>1108</fpage><lpage>1125</lpage><pub-id pub-id-type="pmid">17157034</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Frankel</surname><given-names>T.</given-names></name></person-group><chapter-title>The geometry of physics</chapter-title><edition>2nd edn</edition><year>2004</year><publisher-name>Cambridge University Press</publisher-name><publisher-loc>Cambridge</publisher-loc><comment>England</comment></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friman</surname><given-names>O.</given-names></name><name><surname>Borga</surname><given-names>M.</given-names></name><name><surname>Lundberg</surname><given-names>P.</given-names></name><name><surname>Knutsson</surname><given-names>H.</given-names></name></person-group><article-title>Adaptive analysis of fMRI data</article-title><source>Neuroimage</source><volume>19</volume><year>2003</year><fpage>837</fpage><lpage>845</lpage><pub-id pub-id-type="pmid">12880812</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.</given-names></name></person-group><article-title>Functional integration and inference in the brain</article-title><source>Prog. Neurobiol.</source><volume>68</volume><year>2002</year><fpage>113</fpage><lpage>143</lpage><pub-id pub-id-type="pmid">12450490</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.J.</given-names></name><name><surname>Penny</surname><given-names>W.</given-names></name></person-group><article-title>Posterior probability maps and SPMs</article-title><source>NeuroImage</source><volume>19</volume><year>2003</year><fpage>1240</fpage><lpage>1249</lpage><pub-id pub-id-type="pmid">12880849</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.J.</given-names></name><name><surname>Glaser</surname><given-names>D.E.</given-names></name><name><surname>Henson</surname><given-names>R.N.</given-names></name><name><surname>Kiebel</surname><given-names>S.</given-names></name><name><surname>Phillips</surname><given-names>C.</given-names></name><name><surname>Ashburner</surname><given-names>J.</given-names></name></person-group><article-title>Classical and Bayesian inference in neuroimaging: applications</article-title><source>NeuroImage</source><volume>16</volume><year>2002</year><fpage>484</fpage><lpage>512</lpage><pub-id pub-id-type="pmid">12030833</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.J.</given-names></name><name><surname>Penny</surname><given-names>W.</given-names></name><name><surname>Phillips</surname><given-names>C.</given-names></name><name><surname>Kiebel</surname><given-names>S.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name><name><surname>Ashburner</surname><given-names>J.</given-names></name></person-group><article-title>Classical and Bayesian inference in neuroimaging: theory</article-title><source>NeuroImage</source><volume>16</volume><year>2002</year><fpage>465</fpage><lpage>483</lpage><pub-id pub-id-type="pmid">12030832</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.</given-names></name><name><surname>Ashburner</surname><given-names>J.</given-names></name><name><surname>Kiebel</surname><given-names>S.</given-names></name><name><surname>Nichols</surname><given-names>T.</given-names></name><name><surname>Penny</surname><given-names>W.</given-names></name></person-group><chapter-title>Statistical Parametric Mapping: The Analysis of Functional Brain Images</chapter-title><year>2006</year><publisher-name>Elsevier</publisher-name><publisher-loc>London</publisher-loc></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.</given-names></name><name><surname>Mattout</surname><given-names>J.</given-names></name><name><surname>Trujillo-Barreto</surname><given-names>N.</given-names></name><name><surname>Ashburner</surname><given-names>J.</given-names></name><name><surname>Penny</surname><given-names>W.</given-names></name></person-group><article-title>Variational free energy and the Laplace approximation</article-title><source>NeuroImage</source><volume>34</volume><year>2007</year><fpage>220</fpage><lpage>234</lpage><pub-id pub-id-type="pmid">17055746</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.J.</given-names></name><name><surname>Harrison</surname><given-names>L.M.</given-names></name><name><surname>Daunizeau</surname><given-names>J.</given-names></name><name><surname>Kiebel</surname><given-names>S.</given-names></name><name><surname>Phillips</surname><given-names>C.</given-names></name><name><surname>Trujillo-Barreto</surname><given-names>N.</given-names></name><name><surname>Henson</surname><given-names>R.</given-names></name><name><surname>Flandin</surname><given-names>G.</given-names></name><name><surname>Mattout</surname><given-names>J.</given-names></name></person-group><article-title>Multiple sparse priors for the M/EEG inverse problem</article-title><source>NeuroImage</source><volume>39</volume><issue>3</issue><year>2008</year><fpage>1104</fpage><lpage>1120</lpage><pub-id pub-id-type="pmid">17997111</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geman</surname><given-names>S.</given-names></name><name><surname>Geman</surname><given-names>D.</given-names></name></person-group><article-title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images</article-title><source>IEEE-PAMI</source><volume>6</volume><year>1984</year><fpage>721</fpage><lpage>741</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Grady</surname><given-names>L.</given-names></name><name><surname>Schwartz</surname><given-names>E.L.</given-names></name></person-group><chapter-title>The Graph Analysis Toolbox: Image Processing on Arbitrary Graphs</chapter-title><year>2003</year><publisher-name>Boston University</publisher-name><publisher-loc>Boston, MA</publisher-loc></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K.</given-names></name><name><surname>Sayres</surname><given-names>R.</given-names></name><name><surname>Ress</surname><given-names>D.</given-names></name></person-group><article-title>High-resolution imaging reveals highly selective nonface clusters in the fusiform face area</article-title><source>Nat. Neurosci.</source><volume>9</volume><year>2006</year><fpage>1177</fpage><lpage>1185</lpage><pub-id pub-id-type="pmid">16892057</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>A.K.</given-names></name><name><surname>Nagar</surname><given-names>D.K.</given-names></name></person-group><chapter-title>Matrix Variate Distributions</chapter-title><year>2000</year><publisher-name>Chapman &#x00026; Hall/CRC</publisher-name><publisher-loc>Boca Raton</publisher-loc></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>L.M.</given-names></name><name><surname>Penny</surname><given-names>W.</given-names></name><name><surname>Ashburner</surname><given-names>J.</given-names></name><name><surname>Trujillo-Barreto</surname><given-names>N.</given-names></name><name><surname>Friston</surname><given-names>K.J.</given-names></name></person-group><article-title>Diffusion-based spatial priors for imaging</article-title><source>NeuroImage</source><volume>38</volume><year>2007</year><fpage>677</fpage><lpage>695</lpage><pub-id pub-id-type="pmid">17869542</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Harville</surname><given-names>D.</given-names></name></person-group><chapter-title>Matrix Algebra From a Statistician's Perspective</chapter-title><year>1997</year><publisher-name>Springer Science+Business Media Inc</publisher-name><publisher-loc>New York</publisher-loc></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>J.D.</given-names></name><name><surname>Deichmann</surname><given-names>R.</given-names></name><name><surname>Rees</surname><given-names>G.</given-names></name></person-group><article-title>Eye-specific effects of binocular rivalry in the human lateral geniculate nucleus</article-title><source>Nature</source><volume>438</volume><year>2005</year><fpage>496</fpage><lpage>499</lpage><pub-id pub-id-type="pmid">16244649</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiebel</surname><given-names>S.J.</given-names></name><name><surname>Goebel</surname><given-names>R.</given-names></name><name><surname>Friston</surname><given-names>K.J.</given-names></name></person-group><article-title>Anatomically informed basis functions</article-title><source>NeuroImage</source><volume>11</volume><year>2000</year><fpage>656</fpage><lpage>667</lpage><pub-id pub-id-type="pmid">10860794</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="editor"><name><surname>MacKay</surname><given-names>D.J.C.</given-names></name></person-group><source>Introduction to Gaussian Processes, Neural Networks and Machine Learning edn</source><year>1998</year><publisher-name>Springer</publisher-name><publisher-loc>Berlin</publisher-loc></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>MacKay</surname><given-names>D.J.C.</given-names></name></person-group><chapter-title>Information Theory, Inference, and Learning Algorithms</chapter-title><year>2003</year><publisher-name>Cambridge University Press</publisher-name><publisher-loc>Cambridge</publisher-loc></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mobbs</surname><given-names>D.</given-names></name><name><surname>Petrovic</surname><given-names>P.</given-names></name><name><surname>Marchant</surname><given-names>J.L.</given-names></name><name><surname>Hassabis</surname><given-names>D.</given-names></name><name><surname>Weiskopf</surname><given-names>N.</given-names></name><name><surname>Seymour</surname><given-names>B.</given-names></name><name><surname>Dolan</surname><given-names>R.J.</given-names></name><name><surname>Frith</surname><given-names>C.D.</given-names></name></person-group><article-title>When fear is near: threat imminence elicits prefrontal-periaqueductal gray shifts in humans</article-title><source>Science</source><volume>317</volume><year>2007</year><fpage>1079</fpage><lpage>1083</lpage><pub-id pub-id-type="pmid">17717184</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moler</surname><given-names>C.</given-names></name><name><surname>Van Loan</surname><given-names>C.</given-names></name></person-group><article-title>Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later</article-title><source>Siam Rev.</source><volume>45</volume><year>2003</year><fpage>3</fpage><lpage>49</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Patterson</surname><given-names>H.D.</given-names></name><name><surname>Thompson</surname><given-names>R.</given-names></name></person-group><chapter-title>Maximum likelihood estimation of components of variance</chapter-title><source>Paper presented at: 8th International Biometrics Conference</source><year>1974</year><comment>Constanta, Romania</comment></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penny</surname><given-names>W.D.</given-names></name><name><surname>Trujillo-Barreto</surname><given-names>N.J.</given-names></name><name><surname>Friston</surname><given-names>K.J.</given-names></name></person-group><article-title>Bayesian fMRI time series analysis with spatial priors</article-title><source>NeuroImage</source><volume>24</volume><year>2005</year><fpage>350</fpage><lpage>362</lpage><pub-id pub-id-type="pmid">15627578</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penny</surname><given-names>W.</given-names></name><name><surname>Flandin</surname><given-names>G.</given-names></name><name><surname>Trujillo-Barreto</surname><given-names>N.</given-names></name></person-group><article-title>Bayesian comparison of spatially regularised general linear models</article-title><source>Hum. Brain Mapp.</source><volume>28</volume><year>2007</year><fpage>275</fpage><lpage>293</lpage><pub-id pub-id-type="pmid">17133400</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polzehl</surname><given-names>J.</given-names></name><name><surname>Spokoiny</surname><given-names>V.G.</given-names></name></person-group><article-title>Functional and dynamic magnetic resonance imaging using vector adaptive weights smoothing</article-title><source>J. R. Stat. Soc. Ser. C-Appl. Stat.</source><volume>50</volume><year>2001</year><fpage>485</fpage><lpage>501</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Qui</surname><given-names>H.</given-names></name><name><surname>Hancock</surname><given-names>E.R.</given-names></name></person-group><chapter-title>A robust graph partition method from the path-weighted matrix</chapter-title><source>Fifth workshop on graph-based representations in pattern recognition</source><year>2005</year><publisher-name>Springer Lectures Notes in Computer Science</publisher-name><fpage>262</fpage><lpage>272</lpage><comment>3424</comment></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>A.</given-names></name><name><surname>Bitouk</surname><given-names>D.</given-names></name><name><surname>Miller</surname><given-names>M.I.</given-names></name></person-group><article-title>Smooth functional and structural maps on the neocortex via orthonormal bases of the Laplace&#x02013;Beltrami operator</article-title><source>IEEE Trans. Med. Imag.</source><volume>25</volume><year>2006</year><fpage>1296</fpage><lpage>1306</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinonero-Candela</surname><given-names>J.Q.</given-names></name><name><surname>Rasmussen</surname><given-names>C.E.</given-names></name></person-group><article-title>A unifying view of sparse approximate Gaussian process regression</article-title><source>J. Mach. Learn. Res.</source><volume>6</volume><year>2005</year><fpage>1939</fpage><lpage>1959</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rasmussen</surname><given-names>C.</given-names></name><name><surname>Williams</surname><given-names>C.</given-names></name></person-group><chapter-title>Gaussian Processes for Machine Learning</chapter-title><year>2006</year><publisher-name>The MIT Press</publisher-name><publisher-loc>Cambridge</publisher-loc><comment>Massachusetts</comment></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>K.A.</given-names></name><name><surname>Kastner</surname><given-names>S.</given-names></name></person-group><article-title>Visual responses of the human superior colliculus: a high-resolution functional magnetic resonance imaging study</article-title><source>J. Neurophysiol.</source><volume>94</volume><year>2005</year><fpage>2491</fpage><lpage>2503</lpage><pub-id pub-id-type="pmid">15944234</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>M.I.</given-names></name><name><surname>McDonald</surname><given-names>C.T.</given-names></name><name><surname>Allman</surname><given-names>J.M.</given-names></name></person-group><article-title>Analysis of retinotopic maps in extrastriate cortex</article-title><source>Cereb. Cortex</source><volume>4</volume><year>1994</year><fpage>601</fpage><lpage>620</lpage><pub-id pub-id-type="pmid">7703687</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shafie</surname><given-names>K.</given-names></name><name><surname>Sigal</surname><given-names>B.</given-names></name><name><surname>Siegmund</surname><given-names>D.</given-names></name><name><surname>Worsley</surname><given-names>K.J.</given-names></name></person-group><article-title>Rotation space random fields with an application to fMRI data</article-title><source>Ann. Stat.</source><volume>31</volume><year>2003</year><fpage>1732</fpage><lpage>1771</lpage></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegmund</surname><given-names>D.O.</given-names></name><name><surname>Worsley</surname><given-names>K.J.</given-names></name></person-group><article-title>Testing for a signal with unknown location and scale in a stationary Gaussian random-field</article-title><source>Ann. Stat.</source><volume>23</volume><year>1995</year><fpage>608</fpage><lpage>639</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>W.K.</given-names></name><name><surname>Bellgowan</surname><given-names>P.S.</given-names></name><name><surname>Martin</surname><given-names>A.</given-names></name></person-group><article-title>Measuring selectivity in fMRI data</article-title><source>Nat. Neurosci.</source><volume>10</volume><year>2007</year><fpage>4</fpage><lpage>5</lpage><pub-id pub-id-type="pmid">17189941</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snelson</surname><given-names>E.</given-names></name><name><surname>Ghahramani</surname><given-names>Z.</given-names></name></person-group><article-title>Local and global sparse Gaussian process approximations</article-title><source>Artif. Intell. Stat.</source><volume>11</volume><year>2007</year></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sochen</surname><given-names>N.</given-names></name><name><surname>Kimmel</surname><given-names>R.</given-names></name><name><surname>Malladi</surname><given-names>R.</given-names></name></person-group><article-title>A general framework for low level vision</article-title><source>IEEE Trans. Image Process.</source><volume>7</volume><year>1998</year><fpage>310</fpage><lpage>318</lpage><pub-id pub-id-type="pmid">18276251</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Strang</surname><given-names>G.</given-names></name></person-group><chapter-title>Linear Algebra and its Applications</chapter-title><year>2004</year><publisher-name>Thomson Brookes/Cole</publisher-name><publisher-loc>Belmont, USA</publisher-loc></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sylvester</surname><given-names>R.</given-names></name><name><surname>Josephs</surname><given-names>O.</given-names></name><name><surname>Driver</surname><given-names>J.</given-names></name><name><surname>Rees</surname><given-names>G.</given-names></name></person-group><article-title>Visual FMRI responses in human superior colliculus show a temporal-nasal asymmetry that is absent in lateral geniculate and visual cortex</article-title><source>J. Neurophysiol.</source><volume>97</volume><year>2007</year><fpage>1495</fpage><lpage>1502</lpage><pub-id pub-id-type="pmid">17135475</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabelow</surname><given-names>K.</given-names></name><name><surname>Polzehl</surname><given-names>J.</given-names></name><name><surname>Voss</surname><given-names>H.U.</given-names></name><name><surname>Spokoiny</surname><given-names>V.</given-names></name></person-group><article-title>Analyzing fMRI experiments with structural adaptive smoothing procedures</article-title><source>Neuroimage</source><volume>33</volume><year>2006</year><fpage>55</fpage><lpage>62</lpage><pub-id pub-id-type="pmid">16891126</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>S.A.</given-names></name><name><surname>Miller</surname><given-names>D.</given-names></name><name><surname>Tanabe</surname><given-names>J.</given-names></name></person-group><article-title>Bilateral spatial filtering: refining methods for localizing brain activation in the presence of parenchymal abnormalities</article-title><source>Neuroimage</source><volume>33</volume><year>2006</year><fpage>564</fpage><lpage>569</lpage><pub-id pub-id-type="pmid">16942890</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wand</surname><given-names>M.P.</given-names></name></person-group><article-title>Vector differential calculus in statistics</article-title><source>Am. Stat.</source><volume>56</volume><year>2002</year><fpage>55</fpage><lpage>62</lpage></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J.M.</given-names></name><name><surname>Fleet</surname><given-names>D.J.</given-names></name><name><surname>Hertzmann</surname><given-names>A.</given-names></name></person-group><chapter-title>Gaussian Process Dynamical Models</chapter-title><source>Paper presented at: NIPS</source><year>2005</year></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warnking</surname><given-names>J.</given-names></name><name><surname>Dojat</surname><given-names>M.</given-names></name><name><surname>Guerin-Dugue</surname><given-names>A.</given-names></name><name><surname>Delon-Martin</surname><given-names>C.</given-names></name><name><surname>Olympieff</surname><given-names>S.</given-names></name><name><surname>Richard</surname><given-names>N.</given-names></name><name><surname>Chehikian</surname><given-names>A.</given-names></name><name><surname>Segebarth</surname><given-names>C.</given-names></name></person-group><article-title>fMRI retinotopic mapping&#x02014;step by step</article-title><source>Neuroimage</source><volume>17</volume><year>2002</year><fpage>1665</fpage><lpage>1683</lpage><pub-id pub-id-type="pmid">12498741</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname><given-names>M.W.</given-names></name><name><surname>Jenkinson</surname><given-names>M.</given-names></name><name><surname>Brady</surname><given-names>J.M.</given-names></name><name><surname>Smith</surname><given-names>S.M.</given-names></name></person-group><article-title>Fully Bayesian spatio-temporal modeling of FMRI data</article-title><source>IEEE Trans. Med. Imag.</source><volume>23</volume><year>2004</year><fpage>213</fpage><lpage>231</lpage></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Worsley</surname><given-names>K.J.</given-names></name><name><surname>Marrett</surname><given-names>S.</given-names></name><name><surname>Neelin</surname><given-names>P.</given-names></name><name><surname>Vandal</surname><given-names>A.C.</given-names></name><name><surname>Friston</surname><given-names>K.J.</given-names></name><name><surname>Evans</surname><given-names>A.C.</given-names></name></person-group><article-title>A unified statistical approach for determining significant voxels in images of cerebral activation</article-title><source>Hum. Brain Mapp.</source><volume>4</volume><year>1996</year><fpage>58</fpage><lpage>73</lpage><pub-id pub-id-type="pmid">20408186</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Worsley</surname><given-names>K.J.</given-names></name><name><surname>Andermann</surname><given-names>M.</given-names></name><name><surname>Koulis</surname><given-names>T.</given-names></name><name><surname>MacDonald</surname><given-names>D.</given-names></name><name><surname>Evans</surname><given-names>A.C.</given-names></name></person-group><article-title>Detecting changes in nonisotropic images</article-title><source>Hum. Brain Mapp.</source><volume>8</volume><year>1999</year><fpage>98</fpage><lpage>101</lpage><pub-id pub-id-type="pmid">10524599</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>F.</given-names></name><name><surname>Hancock</surname><given-names>E.R.</given-names></name></person-group><article-title>Image scale-space from the heat kernel</article-title><source>Prog. Pattern Recognit. Image Anal. Appl. Proc.</source><volume>3773</volume><year>2005</year><fpage>181</fpage><lpage>192</lpage></element-citation></ref></ref-list><sec><sec id="app1"><label>Appendix A</label><title>Linear algebra for the EM scheme</title><p>This appendix provides notes on the linear algebra used to compute the gradients and curvatures necessary for the EM scheme in the main text. They are not necessary to understand the results presented above but help optimize implementation.</p><p>We require the bound on the log-marginal likelihood, ln <italic>p</italic>(<italic>y</italic>|<italic>&#x003b1;</italic>) and its derivatives.<disp-formula id="fd20"><label>(A.1)</label><mml:math id="M31" altimg="si57.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mstyle scriptlevel="1"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ln</mml:mi><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>&#x003a3;</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>&#x003a3;</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>const</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>&#x003a3;</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>Z</mml:mi><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mtext>.</mml:mtext></mml:math></disp-formula></p><p>The first term of Eq. <xref rid="fd20" ref-type="disp-formula">(A.1)</xref> is<disp-formula id="fd21"><label>(A.2)</label><mml:math id="M32" altimg="si58.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>ln</mml:mi><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>&#x003a3;</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>&#x003a0;</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula><mml:math id="M33" altimg="si59.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>+</mml:mo><mml:mi>U</mml:mi><mml:mi>W</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>U</mml:mi></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, see appendix of <xref rid="bib40" ref-type="bibr">Rasmussen and Williams (2006)</xref>. This can be reduced further using <inline-formula><mml:math id="M34" altimg="si60.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The second term is<disp-formula id="fd22"><label>(A.3)</label><mml:math id="M35" altimg="si61.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>&#x003a3;</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi mathvariant="normal">Y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="normal">A</mml:mi><mml:mrow><mml:msub><mml:mi>&#x0025b;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">A</mml:mi><mml:mrow><mml:msub><mml:mi>&#x0025b;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>&#x0025b;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where we have used vec(A)<sup>T</sup>vec(B)&#x000a0;=&#x000a0;tr(A<sup><italic>T</italic></sup>B) and, where &#x003b5;&#x002c6;<sub>1</sub>&#x000a0;=&#x000a0;Y-X&#x003b2;&#x002c6; is the matrix of prediction errors, where &#x003bc;&#x000a0;=&#x000a0;vec(&#x003b2;&#x002c6;).</p><sec><label>A.1</label><title>Conditional moments of parameters (E-step)</title><p>The conditional precision is<disp-formula id="fd23"><label>(A.4)</label><mml:math id="M36" altimg="si62.gif" overflow="scroll"><mml:mrow><mml:mi>&#x003a0;</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mtext>.</mml:mtext></mml:math></disp-formula></p><p>The conditional covariance can be formulated in terms of eigenmodes of the second level prior covariance as follows: using the matrix inversion lemma, <inline-formula><mml:math id="M37" altimg="si63.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>Z</mml:mi><mml:mo>+</mml:mo><mml:mi>U</mml:mi><mml:mi>W</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow/><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow/><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>U</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow/><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow/><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>U</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow/><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, the data precision is<disp-formula id="fd24"><label>(A.5)</label><mml:math id="M38" altimg="si64.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mi>&#x003a3;</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:msup><mml:mi>&#x003a0;</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mtext>.</mml:mtext></mml:math></disp-formula></p><p>Using the eigenvalue decomposition; &#x003a3;<sub>2</sub>&#x000a0;=&#x000a0;&#x003a6;<sub>2</sub><italic>D</italic><sub>2</sub>&#x003a6;<sub>2</sub><sup><italic>T</italic></sup>, where <inline-formula><mml:math id="M39" altimg="si65.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M40" altimg="si66.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, i.e., <italic>&#x003a6;</italic> and <italic>D</italic> are eigenvectors and eigenvalues respectively, then<disp-formula id="fd25"><label>(A.6)</label><mml:math id="M41" altimg="si67.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>&#x003a3;</mml:mi><mml:mo>=</mml:mo><mml:mi>Z</mml:mi><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>&#x003a3;</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mtext>.</mml:mtext></mml:math></disp-formula></p><p>Comparing the last line of Eq. <xref rid="fd24" ref-type="disp-formula">(A.5)</xref> with <xref rid="fd25" ref-type="disp-formula">(A.6)</xref><disp-formula id="fd26"><label>(A.7)</label><mml:math id="M42" altimg="si68.gif" overflow="scroll"><mml:mtable><mml:mtr columnalign="left"><mml:mtd><mml:msup><mml:mi>&#x003a0;</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>E</mml:mi><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>D</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>D</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mtext>.</mml:mtext></mml:math></disp-formula></p><p>Note, for a diffusion-based prior, <inline-formula><mml:math id="M43" altimg="si69.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x0039b;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mo>-</mml:mo><mml:mi>&#x0039b;</mml:mi><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula>, however, we could use <inline-formula><mml:math id="M44" altimg="si70.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x0039b;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x0039b;</mml:mi><mml:mrow/><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> for a Laplacian prior (numerically stable expressions for each are given in the last and penultimate lines of Eq. (A.7) respectively).</p><p>The conditional mean is<disp-formula id="fd27"><label>(A.8)</label><mml:math id="M45" altimg="si71.gif" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>E</mml:mi><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>y</mml:mi></mml:mrow></mml:math></disp-formula></p></sec><sec><label>A.2</label><title>Conditional moments of hyperparameters (M-step)</title><p>To compute the derivatives<xref rid="fn6" ref-type="fn">6</xref> required for the M-step, we use standard results for Kronecker tensor products to show the score and expected information reduce to<disp-formula id="fd28"><label>(A.9)</label><mml:math id="M46" altimg="si72.gif" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>&#x0025b;</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>A</mml:mi><mml:mi>&#x0025b;</mml:mi></mml:msub><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:math></disp-formula>and<disp-formula id="fd29"><label>(A.10)</label><mml:math id="M47" altimg="si73.gif" overflow="scroll"><mml:mtable><mml:mtr columnalign="left"><mml:mtd><mml:mfrac><mml:mrow><mml:msup><mml:mo>&#x02202;</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mstyle scriptlevel="1"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mtext>tr</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo><mml:mtext>tr</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>B</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle scriptlevel="1"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>-</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mstyle scriptlevel="1"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mstyle scriptlevel="1"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>B</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where the superscript of matrices <italic>A</italic>, <italic>B</italic>, <italic>F</italic>, and <italic>G</italic> represents a hyperparameter index, i.e., <italic>k</italic>&#x000a0;&#x02282;&#x000a0;{1,2,3}, while the subscript represents a level index for error covariances, i.e., <italic>a</italic>&#x000a0;&#x02282;&#x000a0;{1,2}, which will simplify expressions later. Terms in Eqs. (A.9) and (A.10) are given by<disp-formula id="fd30"><label>(A.11)</label><mml:math id="M48" altimg="si74.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>X</mml:mi><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>D</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mi>D</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mtext>.</mml:mtext></mml:math></disp-formula></p><p>Supporting calculations for the score are<disp-formula id="fd31"><label>(A.12)</label><mml:math id="M49" altimg="si75.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mstyle scriptlevel="1"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>tr</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi>&#x003a3;</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>&#x003a3;</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msup><mml:mi>&#x003a3;</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>Z</mml:mi><mml:msup><mml:mi>&#x003a0;</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x02297;</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula><label>(A.13)</label><mml:math id="M50" altimg="si76.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>&#x0025b;</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>&#x0025b;</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>&#x0025b;</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>A</mml:mi><mml:mi>&#x0025b;</mml:mi></mml:msub><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mspace width="0.3em" height="0.3ex"/><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>&#x0025b;</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>A</mml:mi><mml:mi>&#x0025b;</mml:mi></mml:msub><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where we have used <italic>&#x003a0;</italic><sup>&#x02212;&#x000a0;1</sup>&#x000a0;=&#x000a0;<italic>&#x003a6;</italic><sub>2</sub><italic>E&#x003a6;</italic><sub>2</sub><sup><italic>T</italic></sup> and the notation in Eq. (A.11). The expression in Eq. (A.10) is derived from the expected Fisher information, I<sub><italic>kn</italic></sub>&#x000a0;=&#x000a0;&#x02212;&#x000a0;&#x02329;&#x02202;<sup>2</sup><italic>F</italic>&#x000a0;/&#x000a0;&#x02202;<italic>&#x003b1;</italic><sub><italic>k</italic></sub> &#x02202;<italic>&#x003b1;</italic><sub><italic>n</italic></sub>&#x0232a;, see <xref rid="fig3" ref-type="fig">Fig. 3</xref> last line, using Eq. <xref rid="fd31" ref-type="disp-formula">(A.12)</xref> and the cyclic property of trace. These expressions simplify further using tr(A&#x000a0;&#x02297;&#x000a0;B)&#x000a0;=&#x000a0;tr(A)tr(B). Note, if the data are transformed, i.e., <inline-formula><mml:math id="M51" altimg="si77.gif" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mi mathvariant="normal">Y</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, then all variables are transformed as shown in Eq. <xref rid="fd6" ref-type="disp-formula">(6)</xref>.</p><p>The formulation above is not a computationally efficient way to implement the algorithm. We want to make use of <inline-formula><mml:math id="M52" altimg="si78.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M53" altimg="si79.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, in particular, given<disp-formula id="fd32"><label>(A.14)</label><mml:math id="M54" altimg="si80.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi mathvariant="normal">L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mi>&#x0039b;</mml:mi><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>exp</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="bold">L</mml:mi><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>&#x0039b;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mo>-</mml:mo><mml:mi>&#x0039b;</mml:mi><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Computationally efficient expressions are obtained using tr(AB<sup><italic>T</italic></sup>)&#x000a0;=&#x000a0;1<sup><italic>T</italic></sup>(A&#x025cb;B)1 (where &#x025cb; is the Hadamard product, 1 is a column of ones) and the following<disp-formula id="fd33"><label>(A.15)</label><mml:math id="M55" altimg="si81.gif" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mtext>tr</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x02015;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>tr</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>B</mml:mi><mml:mo>&#x02015;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>&#x02015;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo>&#x02015;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>&#x02015;</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo>&#x02015;</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>B</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>together with the expressions in <xref rid="tbl3 tbl4 tbl5" ref-type="table">Tables 3&#x02212;5</xref>. Here we have used the notation <inline-formula><mml:math id="M56" altimg="si82.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>&#x02015;</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mrow><mml:mrow/></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mi>a</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mrow/></mml:msubsup><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to represent left and right multiplication of <italic>X</italic><sub>1</sub> by bases <italic>&#x00424;</italic><sub><italic>a</italic></sub> and <italic>&#x00424;</italic><sub><italic>b</italic></sub> respectively. This is important when using a reduced eigen system, e.g., <italic>n</italic><sub><italic>a</italic></sub>,<italic>n</italic><sub><italic>b</italic></sub>&#x000a0;&#x0003c;&#x000a0;<italic>N</italic>, as the dimension of <italic>X</italic><sub>1</sub> is reduced from <italic>N</italic>&#x000a0;&#x000d7;&#x000a0;<italic>N</italic> to <italic>n</italic><sub><italic>a</italic></sub>&#x000a0;&#x000d7;&#x000a0;<italic>n</italic><sub><italic>b</italic></sub>. Components of Eqs. (A.9) and (A.10) then can be written<disp-formula id="fd34"><label>(A.16)</label><mml:math id="M57" altimg="si83.gif" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#x02218;</mml:mo><mml:mi>d</mml:mi><mml:msubsup><mml:mi>D</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#x02218;</mml:mo><mml:mi>d</mml:mi><mml:msubsup><mml:mi>D</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02218;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:msubsup><mml:mi>D</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02218;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:msubsup><mml:mi>D</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>B</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:msubsup><mml:mi>D</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02218;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:msubsup><mml:mi>D</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mi>E</mml:mi><mml:mo>&#x02218;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>C</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>B</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>G</mml:mi><mml:mo stretchy="true">&#x02015;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02218;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The expressions for tr(<italic>A</italic><sub><italic>a</italic></sub><sup>(<italic>k</italic>)</sup>) and tr(<italic>B</italic><sub><italic>a</italic></sub><sup>(<italic>k</italic>)</sup>) are sparse because <italic>dD</italic><sub><italic>a</italic></sub><sup>(<italic>k</italic>)</sup> is diagonal, even if <italic>K</italic>&#x000af;<sub>a1a</sub><sup>&#x02013;1</sup> or <italic>S</italic>&#x000af;<sub>a1a</sub><sup>&#x02013;1</sup> are not.</p></sec></sec><sec><label>Appendix B</label><title>Embedding space metric of graph Laplacian</title><p>In this paper the embedding metric (Eq. <xref rid="fd13" ref-type="disp-formula">(13)</xref>) is fixed, where <inline-formula><mml:math id="M58" altimg="si84.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and<disp-formula id="fd35"><label>(A.17)</label><mml:math id="M59" altimg="si85.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>H</mml:mi><mml:mtext>f</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>vec</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:msub><mml:msubsup><mml:mn>1</mml:mn><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>&#x003b8;</italic><sub><italic>ols</italic></sub> is the <italic>P&#x000a0;</italic>&#x000d7;&#x000a0;<italic>N</italic> matrix of OLS estimates and 1<sub><italic>N</italic></sub> is a column vector of ones length <italic>N</italic>.</p></sec></sec><ack><title>Acknowledgments</title><p>The Wellcome Trust funded this work and JD is supported by a Marie Curie Fellowship from the EU.</p></ack><fn-group><fn id="fn1"><label>1</label><p>A univariate random variable (rv), <italic>x</italic>, has probability density function (pdf) (2<italic>&#x003c0;&#x003c3;</italic><sup>2</sup>)<sup>&#x02212;</sup>&#x000a0;<sup>1/2</sup>exp(&#x02212;&#x000a0;(x&#x000a0;&#x02212;&#x000a0;<italic>&#x003bc;</italic>)<sup>2</sup>&#x000a0;/&#x000a0;2<italic>&#x003c3;</italic><sup>2</sup>), <italic>x&#x000a0;</italic>&#x02208;&#x000a0;<inline-formula><mml:math id="M60" altimg="si20.gif" overflow="scroll"><mml:mi mathvariant="fraktur">R</mml:mi></mml:math></inline-formula>, <italic>&#x003bc;&#x000a0;</italic>&#x02208;&#x000a0;<inline-formula><mml:math id="M61" altimg="si21.gif" overflow="scroll"><mml:mi mathvariant="fraktur">R</mml:mi></mml:math></inline-formula>, a multivariate rv, <italic>x&#x000a0;</italic>=&#x000a0;(<italic>x</italic><sub>1</sub>,...,<italic>x</italic><sub><italic>r</italic></sub>)<sup><italic>T</italic></sup>, has pdf (2<italic>&#x003c0;</italic>)<sup>&#x02212;</sup>&#x000a0;<sup>1/2</sup>|<italic>S</italic>|<sup>&#x02212;</sup>&#x000a0;<sup><italic>r</italic>/2</sup>exp(&#x02212;&#x000a0;<italic>tr</italic>(<italic>S</italic><sup>&#x02212;</sup>&#x000a0;<sup>1</sup>(<italic>x</italic>&#x000a0;&#x02212;&#x000a0;<italic>&#x003bc;</italic>)(<italic>x</italic>&#x000a0;&#x02212;&#x000a0;<italic>&#x003bc;</italic>)<sup><italic>T</italic></sup>&#x000a0;/&#x000a0;2), <italic>x&#x000a0;</italic>&#x02208;&#x000a0;<inline-formula><mml:math id="M62" altimg="si22.gif" overflow="scroll"><mml:mi mathvariant="fraktur">R</mml:mi></mml:math></inline-formula><sup><italic>r</italic></sup><sup>&#x000a0;&#x000d7;&#x000a0;1</sup>, <italic>&#x003bc;&#x000a0;</italic>&#x02208;&#x000a0;<inline-formula><mml:math id="M63" altimg="si23.gif" overflow="scroll"><mml:mi mathvariant="fraktur">R</mml:mi></mml:math></inline-formula><sup><italic>r</italic></sup><sup>&#x000a0;&#x000d7;&#x000a0;1</sup> and is represented by <italic>x</italic>&#x000a0;~&#x000a0;<italic>N</italic><sub><italic>r</italic></sub>(<italic>&#x003bc;</italic>,<italic>S</italic>) and a matrix-variate normal rv, <italic>X</italic>, has pdf (2<italic>&#x003c0;</italic>)<sup>&#x02212;</sup>&#x000a0;<sup><italic>rc</italic>/2</sup>|<italic>S</italic>|<sup>&#x02212;</sup>&#x000a0;<sup>c&#x000a0;/&#x000a0;2</sup>|<italic>K</italic>|<sup>&#x02212;</sup>&#x000a0;<sup><italic>r</italic></sup><sup>&#x000a0;/&#x000a0;2</sup>exp(&#x02212;&#x000a0;<italic>tr(S</italic><sup>&#x02212;</sup>&#x000a0;<sup>1</sup>(<italic>X</italic>&#x000a0;&#x02212;&#x000a0;<italic>M</italic>)<italic>K</italic><sup>&#x02212;</sup>&#x000a0;<sup>1</sup>(<italic>X</italic>&#x000a0;&#x02212;&#x000a0;<italic>M</italic>)<sup><italic>T</italic></sup>)&#x000a0;/&#x000a0;2), <italic>X&#x000a0;</italic>&#x02208;&#x000a0;<inline-formula><mml:math id="M64" altimg="si24.gif" overflow="scroll"><mml:mi mathvariant="fraktur">R</mml:mi></mml:math></inline-formula><sup><italic>r</italic></sup><sup>&#x000a0;&#x000d7;&#x000a0;c</sup>, <italic>M</italic>&#x000a0;&#x003f5;&#x000a0;<inline-formula><mml:math id="M65" altimg="si25.gif" overflow="scroll"><mml:mi mathvariant="fraktur">R</mml:mi></mml:math></inline-formula><sup><italic>r</italic></sup><sup>&#x000a0;&#x000d7;&#x000a0;</sup><sup><italic>c</italic></sup> represented by <italic>X&#x000a0;</italic>~&#x000a0;<italic>N</italic><sub><italic>r,c</italic></sub>(<italic>M</italic>,<italic>S</italic>&#x000a0;&#x02297;&#x000a0;<italic>K</italic>) with multivariate densities over <italic>&#x00058;&#x020d7;<sup>T</sup></italic> ~ <italic>N</italic><sub>rc</sub>(<italic>&#x0004d;&#x020d7;<sup>T</sup></italic>, <italic>S</italic> &#x02297; <italic>K</italic>) and <italic>&#x00058;&#x020d7; ~ N</italic><sub>cr</sub>(<italic>&#x0004d;&#x020d7;</italic>, <italic>S</italic> &#x02297; <italic>K</italic>).</p></fn><fn id="fn2"><label>2</label><p>Technically the Laplacian matrix is a function of the random variable, <italic>b</italic>, i.e., <bold>L</bold>(<italic>b</italic>,<italic>H</italic>), which renders it the generator of a nonlinear dynamic system. This can be approximated by substituting the conditional expectation, <italic>&#x003bc;</italic>&#x000a0;=&#x000a0;&#x0003c;<italic>b</italic>&#x0003e;, i.e., <bold>L</bold>(<italic>&#x003bc;</italic>,<italic>H</italic>), which is used in the main text.</p></fn><fn id="fn3"><label>3</label><p>This is equivalent to a Newton step, but using the expected curvature as opposed to the local curvature of the objective function.</p></fn><fn id="fn4"><label>4</label><p>A posterior probability map has two thresholds <italic>t</italic><sub>1&#x000a0;</sub>&#x003f5;&#x000a0;<inline-formula><mml:math id="M66" altimg="si55.gif" overflow="scroll"><mml:mi mathvariant="fraktur">R</mml:mi></mml:math></inline-formula> and <italic>t</italic><sub>2&#x000a0;</sub>&#x003f5;&#x000a0;[0,1] that are used to show voxels where the model is at least 100&#x000a0;&#x000d7;&#x000a0;<italic>t</italic><sub>2</sub>% sure that the effect size is greater than <italic>t</italic><sub>1</sub> and is represented by the expression <italic>p</italic>(<bold>&#x003b2;</bold>&#x000a0;&#x0003e;&#x000a0;<italic>t</italic><sub>1</sub>)&#x000a0;&#x0003e;&#x000a0;<italic>t</italic><sub>2</sub>.</p></fn><fn id="fn5"><label>5</label><p>The <italic>i</italic>th local kernel is centred at the <italic>i</italic>th voxel and is given by (an image format of) the same row of the spatial covariance matrix.</p></fn><fn id="fn6"><label>6</label><p>All derivatives are with respect to &#x003b3; = ln &#x003b1;.</p></fn></fn-group></back><floats-group><fig id="fig1"><label>Fig. 1</label><caption><p>Three-stage procedure in SPM. The statistical model (central panel) models each voxel separately. Several consequences follow; (i) this statistical model is unable to explain correlations in measurements over anatomical space and (ii) inferences over many voxels have to deal with spatial dependencies when adjusting for multiple comparisons. These are dealt with in SPM by smoothing data with a user specified fixed Gaussian kernel (left panel) and using RFT to adjust classical <italic>p</italic>-values <italic>post hoc</italic> (right panel).</p></caption><graphic xlink:href="gr1"/></fig><fig id="fig2"><label>Fig. 2</label><caption><p>Graphical representation of a generative and recognition model (upper and lower panels respectively). Each node represents a random variable (rv). The observed rv, i.e., data, is shaded and arrows indicate conditional dependence.</p></caption><graphic xlink:href="gr2"/></fig><fig id="fig3"><label>Fig. 3</label><caption><p>Pseudo-code. Prior densities are specified e.g., diffusion-based prior, and the posterior density optimized, given data, by iterating E and M-steps. The dimension of posterior multivariate density is <italic>n</italic><sub>2</sub>&#x000a0;=&#x000a0;<italic>P</italic>&#x000a0;&#x000d7;&#x000a0;<italic>N</italic>.</p></caption><graphic xlink:href="gr3"/></fig><fig id="fig4"><label>Fig. 4</label><caption><p>Synthetic data. Data were simulated using a generative model with non-stationary spatial kernel, producing two distinct regions. (a) design matrix (top), true spatial signal (left), example time-series (lower) and OLS estimate of first column GLM parameters (right), (b) posterior mean estimates of GSP, EGL and GGL-based priors on top row and PPMs, threshold at <italic>p</italic>(<italic>b</italic>&#x000a0;&#x0003e;&#x000a0;0.33)&#x000a0;&#x0003e;&#x000a0;0.95, below, (c) local kernels of EGL and GGL on top row along with plot of edge weights below, (d, e) predictions against data at two locations (same as local kernels in panel, c) inside [outside] the edge of the central region, (f, g) 2nd&#x02013;5th eigenmodes of EGL and GGL (1st eigenmode is not included as it is constant over the graph), (h) outer product of 4th eigenmode (covariance component) on top row and full diffusion kernel (sum of all covariance components weighted by their eigenvalues) below, and (i) spectra of EGL at two values of <italic>&#x003c4;</italic>.</p></caption><graphic xlink:href="gr4ae"/><graphic xlink:href="gr4fi"/></fig><fig id="fig5"><label>Fig. 5</label><caption><p>Real fMRI data. Mean parameter estimates (two slices through auditory cortex) of standard resolution fMRI (3&#x000a0;mm<sup>3</sup>) data of one subject's response to an auditory stimulus. (a&#x02013;c) Posterior mean estimates using GSP, EGL and GGL respectively, (d) data smoothed with a 6&#x000a0;mm Gaussian kernel (conventional practice), (e) local kernels and PPMs, threshold at <italic>p</italic>(<italic>b</italic>&#x000a0;&#x0003e;&#x000a0;2)&#x000a0;&#x0003e;&#x000a0;0.95, (f) PPM for GGL overlaid on anatomical image (same resolution as functional data), (g) comparison of predictions from EGL and GGL models and (h, i) 2nd&#x02013;5th eigenmodes (in image format) from EGL and GGL.</p></caption><graphic xlink:href="gr5ad"/><graphic xlink:href="gr5ei"/></fig><table-wrap id="tbl1" position="float"><label>Table 1</label><caption><p>Derivatives of data covariance matrix (using <italic>&#x003b3;</italic>&#x000a0;=&#x000a0;ln<italic>&#x003b1;</italic>)</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"><italic>K</italic><hr/></th><th valign="top">1<hr/></th><th valign="top">2<hr/></th><th valign="top">3<hr/></th></tr><tr><th valign="top">Hyperparameter</th><th valign="top"><italic>&#x003c5;</italic></th><th valign="top"><italic>&#x003c4;</italic></th><th valign="top"><italic>&#x003b7;</italic></th></tr></thead><tbody><tr><td valign="top"><disp-formula id="fd36"><mml:math id="M67" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></td><td valign="top"><disp-formula id="fd37"><mml:math id="M68" altimg="si2.gif" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></disp-formula></td><td valign="top"><disp-formula id="fd38"><mml:math id="M69" altimg="si3.gif" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x02297;</mml:mo><mml:mi>X</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula></td><td valign="top"><disp-formula id="fd39"><mml:math id="M70" altimg="si4.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02297;</mml:mo><mml:mi>X</mml:mi><mml:mfrac><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mo>&#x02202;</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula></td></tr><tr><td valign="top"><inline-formula><mml:math id="M71" altimg="si5.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover><mml:mi>A</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td valign="top"><italic>K</italic><sub>1</sub></td><td valign="top">&#x02212;&#x000a0;L<italic>K</italic><sub>2</sub><italic>&#x003c4;</italic></td><td valign="top"><italic>K</italic><sub>2</sub></td></tr><tr><td valign="top"><inline-formula><mml:math id="M72" altimg="si6.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover><mml:mi>B</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td valign="top"><italic>S</italic><sub>1</sub></td><td valign="top"><italic>XS</italic><sub>2</sub><italic>X</italic><sup><italic>T</italic></sup></td><td valign="top"><italic>XS</italic><sub>2</sub><italic>X</italic><sup><italic>T</italic></sup></td></tr></tbody></table></table-wrap><table-wrap id="tbl2" position="float"><label>Table 2</label><caption><p>Model comparison for synthetic (<xref rid="fig4" ref-type="fig">Fig. 4</xref>) and real data (<xref rid="fig5" ref-type="fig">Fig. 5</xref>)</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Covariance</th><th valign="top">Synthetic data</th><th valign="top">Real data</th></tr></thead><tbody><tr><td valign="top">GSP</td><td align="left" valign="top">&#x02212;&#x000a0;46,891</td><td align="left" valign="top">&#x02212;&#x000a0;36,891</td></tr><tr><td valign="top">EGL</td><td align="left" valign="top">&#x02212;&#x000a0;46,629</td><td align="left" valign="top">&#x02212;&#x000a0;36,292</td></tr><tr><td valign="top">GGL</td><td align="left" valign="top">&#x02212;&#x000a0;46,488<sup>&#x0204e;</sup></td><td align="left" valign="top">&#x02212;&#x000a0;35,150<sup>&#x0204e;</sup></td></tr></tbody></table><table-wrap-foot><fn><p>Log-evidence for GSP, EGL and GGL. Greatest evidence indicated by <sup>&#x0204e;</sup>.</p></fn></table-wrap-foot></table-wrap><table-wrap id="tbl3" position="float"><label>Table 3</label><caption><p>Column precisions <inline-formula><mml:math id="M73" altimg="si7.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>&#x02015;</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <italic>a</italic>,<italic>b&#x000a0;</italic>&#x02282;&#x000a0;{1,2}</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top">1</th><th valign="top">2</th></tr></thead><tbody><tr><td valign="top">1</td><td valign="top"><disp-formula id="fd41"><mml:math id="M74" altimg="si8.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></td><td valign="top"><disp-formula id="fd42"><mml:math id="M75" altimg="si9.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></td></tr><tr><td valign="top">2</td><td valign="top"><disp-formula id="fd44"><mml:math id="M76" altimg="si10.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></td><td valign="top"><disp-formula id="fd45"><mml:math id="M77" altimg="si11.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></td></tr></tbody></table></table-wrap><table-wrap id="tbl4" position="float"><label>Table 4</label><caption><p>Row precisions <inline-formula><mml:math id="M78" altimg="si12.gif" overflow="scroll"><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo>&#x02015;</mml:mo></mml:mover></mml:math></inline-formula><sub>a1b</sub><sup>&#x02013;1</sup>, where <italic>a</italic>,<italic>b&#x000a0;</italic>&#x02282;&#x000a0;{1,2}</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top">1</th><th valign="top">2</th></tr></thead><tbody><tr><td valign="top">1</td><td valign="top"><disp-formula id="fd47"><mml:math id="M79" altimg="si13.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></td><td valign="top"><disp-formula id="fd48"><mml:math id="M80" altimg="si14.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>X</mml:mi><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></td></tr><tr><td valign="top">2</td><td valign="top"><disp-formula id="fd50"><mml:math id="M81" altimg="si15.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></td><td valign="top"><disp-formula id="fd51"><mml:math id="M82" altimg="si16.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>X</mml:mi><mml:msub><mml:mi>&#x003a6;</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></td></tr></tbody></table></table-wrap><table-wrap id="tbl5" position="float"><label>Table 5</label><caption><p>Eigenvalues of derivatives (with respect to &#x003b3;&#x000a0;=&#x000a0;ln &#x003b1;)</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top">1</th><th valign="top">2</th><th valign="top">3</th></tr></thead><tbody><tr><td valign="top"><italic>dD<sub>a</sub></italic><sup>(<italic>k</italic>)</sup></td><td valign="top"><disp-formula id="fd52"><mml:math id="M83" altimg="si17.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow/></mml:msubsup></mml:mrow></mml:math></disp-formula></td><td valign="top"><disp-formula id="fd53"><mml:math id="M84" altimg="si18.gif" overflow="scroll"><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>&#x0039b;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></td><td valign="top"><disp-formula id="fd54"><mml:math id="M85" altimg="si19.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow/></mml:msubsup><mml:mo>&#x02297;</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow/></mml:msubsup></mml:mrow></mml:math></disp-formula></td></tr></tbody></table></table-wrap></floats-group></article>