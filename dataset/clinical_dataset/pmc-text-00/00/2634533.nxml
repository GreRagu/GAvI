<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article" xml:lang="EN"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Neuroinformatics</journal-id><journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id><journal-title>Frontiers in Neuroinformatics</journal-title><issn pub-type="epub">1662-5196</issn><publisher><publisher-name>Frontiers Research Foundation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19194529</article-id><article-id pub-id-type="pmc">2634533</article-id><article-id pub-id-type="doi">10.3389/neuro.11.011.2008</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>PyNN: A Common Interface for Neuronal Network Simulators</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Davison</surname><given-names>Andrew P.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn001">*</xref></contrib><contrib contrib-type="author"><name><surname>Br&#x000fc;derle</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Eppler</surname><given-names>Jochen</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib><contrib contrib-type="author"><name><surname>Kremkow</surname><given-names>Jens</given-names></name><xref ref-type="aff" rid="aff5"><sup>5</sup></xref><xref ref-type="aff" rid="aff6"><sup>6</sup></xref></contrib><contrib contrib-type="author"><name><surname>Muller</surname><given-names>Eilif</given-names></name><xref ref-type="aff" rid="aff7"><sup>7</sup></xref></contrib><contrib contrib-type="author"><name><surname>Pecevski</surname><given-names>Dejan</given-names></name><xref ref-type="aff" rid="aff8"><sup>8</sup></xref></contrib><contrib contrib-type="author"><name><surname>Perrinet</surname><given-names>Laurent</given-names></name><xref ref-type="aff" rid="aff6"><sup>6</sup></xref></contrib><contrib contrib-type="author"><name><surname>Yger</surname><given-names>Pierre</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Unit&#x000e9; de Neurosciences Int&#x000e9;gratives et Computationelles, CNRS</institution><country>Gif sur Yvette, France</country></aff><aff id="aff2"><sup>2</sup><institution>Kirchhoff Institute for Physics, University of Heidelberg</institution><country>Heidelberg, Germany</country></aff><aff id="aff3"><sup>3</sup><institution>Honda Research Institute Europe GmbH, Offenbach</institution><country>Germany</country></aff><aff id="aff4"><sup>4</sup><institution>Berstein Center for Computational Neuroscience, Albert-Ludwigs-University</institution><country>Freiburg, Germany</country></aff><aff id="aff5"><sup>5</sup><institution>Neurobiology and Biophysics, Institute of Biology III, Albert-Ludwigs-University</institution><country>Freiburg, Germany</country></aff><aff id="aff6"><sup>6</sup><institution>Institut de Neurosciences Cognitives de la M&#x000e9;diterran&#x000e9;e, CNRS</institution><country>Marseille, France</country></aff><aff id="aff7"><sup>7</sup><institution>Laboratory of Computational Neuroscience, Ecole Polytechnique F&#x000e9;d&#x000e9;rale de Lausanne</institution><country>Lausanne, Switzerland</country></aff><aff id="aff8"><sup>8</sup><institution>Institute for Theoretical Computer Science, Graz University of Technology</institution><country>Graz, Austria</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Rolf K&#x000f6;tter, Radboud University Nijmegen, The Netherlands</p></fn><fn fn-type="edited-by"><p>Reviewed by: Graham Cummins, Montana State University, USA; Fred Howell, Textensor Limited, UK</p></fn><corresp id="fn001">*Correspondence: Andrew Davison, UNIC, B&#x000e2;t. 32/33, CNRS, 1 Avenue de la Terrasse, 91198 Gif sur Yvette, France. e-mail: <email>andrew.davison@unic.cnrs-gif.fr</email></corresp></author-notes><pub-date pub-type="epreprint"><day>21</day><month>10</month><year>2008</year></pub-date><pub-date pub-type="epub"><day>27</day><month>1</month><year>2009</year></pub-date><pub-date pub-type="collection"><year>2008</year></pub-date><volume>2</volume><elocation-id>11</elocation-id><history><date date-type="received"><day>21</day><month>9</month><year>2008</year></date><date date-type="accepted"><day>22</day><month>12</month><year>2008</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2009 Davison, Br&#x000fc;derle, Eppler, Kremkow, Muller, Pecevski, Perrinet and Yger.</copyright-statement><copyright-year>2009</copyright-year><license license-type="open-access" xlink:href="http://www.frontiersin.org/licenseagreement"><p>This is an open-access article subject to an exclusive license agreement between the authors and the Frontiers Research Foundation, which permits unrestricted use, distribution, and reproduction in any medium, provided the original authors and source are credited.</p></license></permissions><abstract><p>Computational neuroscience has produced a diversity of software for simulations of networks of spiking neurons, with both negative and positive consequences. On the one hand, each simulator uses its own programming or configuration language, leading to considerable difficulty in porting models from one simulator to another. This impedes communication between investigators and makes it harder to reproduce and build on the work of others. On the other hand, simulation results can be cross-checked between different simulators, giving greater confidence in their correctness, and each simulator has different optimizations, so the most appropriate simulator can be chosen for a given modelling task. A common programming interface to multiple simulators would reduce or eliminate the problems of simulator diversity while retaining the benefits. PyNN is such an interface, making it possible to write a simulation script once, using the Python programming language, and run it without modification on any supported simulator (currently NEURON, NEST, PCSIM, Brian and the Heidelberg VLSI neuromorphic hardware). PyNN increases the productivity of neuronal network modelling by providing high-level abstraction, by promoting code sharing and reuse, and by providing a foundation for simulator-agnostic analysis, visualization and data-management tools. PyNN increases the reliability of modelling studies by making it much easier to check results on multiple simulators. PyNN is open-source software and is available from <uri xlink:type="simple" xlink:href="http://neuralensemble.org/PyNN">http://neuralensemble.org/PyNN</uri>.</p></abstract><kwd-group><kwd>Python</kwd><kwd>interoperability</kwd><kwd>large-scale models</kwd><kwd>simulation</kwd><kwd>parallel computing</kwd><kwd>reproducibility</kwd><kwd>computational neuroscience</kwd><kwd>translation</kwd></kwd-group><counts><fig-count count="3"/><table-count count="1"/><equation-count count="0"/><ref-count count="19"/><page-count count="10"/><word-count count="7917"/></counts></article-meta></front><body><sec sec-type="introduction"><title>Introduction</title><p>Science rests upon the three pillars of open communication, reproducibility of results and building upon what has gone before. In these respects, computational neuroscience ought to be in a good position, since computers by design excel at repeating the same task without variation, as many times as desired: reproducibility of computational results ought, then, to be a trivial task. Similarly, the Internet enables almost instantaneous transmission of research materials, i.e. source code, between labs.</p><p>However, in practice this theoretical ease of reproducibility and communication is seldom achieved outside of a single lab and a time frame of a few months or years. While a given scientist may easily be able to reproduce a result obtained a few months ago, precisely reproducing a result obtained several years ago is likely to be rather more difficult, and the general experience seems to be that reproducing the results of others is both difficult and time consuming: very many published papers lack sufficient detail to rebuild a model from scratch, and typographic errors are common.</p><p>Having available the source code of the model greatly improves the situation, but here still there are numerous barriers to reproducibility and to building upon previously published models. One is that source code can rapidly go out of date as computer architectures, compiler standards and simulators develop. Another is that model source code is often not written with reuse and extension in mind, and so considerable rewriting to modularize the code is necessary. Probably the most important barrier is that code written for one simulator is not compatible with any other simulator.</p><p>Although many computational models in neuroscience are written from the ground up in a general purpose programming language such as C++ or Fortran, probably the majority use a special purpose simulator that allows models to be expressed in terms of neuroscience-specific concepts such as neurons, ion channels, synapses; the simulator takes care of translating these concepts into a system of equations and of numerically solving the equations. A large number of such simulators are available (reviewed in Brette et al., <xref ref-type="bibr" rid="B2">2007</xref>), mostly as open-source software, and each has its own programming language, configuration syntax and/or graphical interface, which creates considerable difficulty in translating models from one simulator to another, or even in understanding someone else's code, with obvious negative consequences for communication between investigators, reproducibility of others' models and building on existing models.</p><p>However, the diversity of simulators also has a number of positive consequences: (i) it allows cross-checking &#x02013; the probability of two different simulators having the same bugs or hidden assumptions is very small; (ii) each simulator has a different balance between efficiency (how fast the simulations run), flexibility (how easy it is to add new functionality; the range of models that can be simulated), scalability (for parallel, distributed computation on clusters or supercomputers), and ease of use, so the most appropriate can be chosen for a given task.</p><p>Addressing the problems associated with an ecosystem of multiple simulators while retaining the benefits would greatly increase the ease of reproducibility of computational models in neuroscience and hence make it easier to verify the validity of published models and to build upon previous work.</p><p>There are at least two possible (and complementary) approaches to this. One is to enable direct, efficient communication between different simulators at run-time, allowing different components of a model to be simulated on different simulators (Ekeberg and Djurfeldt, <xref ref-type="bibr" rid="B6">2008</xref>). This approach addresses the problem of building a model from diverse components, but still leaves the problem of having to use different programming languages, and does not enable straightforward cross-checking. The other approach is to develop a system for model specification that is simulator-independent. Translation then only has to be done once for each simulator and not once for each model.</p><p>Here we can take advantage of the recent, rapid emergence of the Python programming language as an alternative interface to several of the more widely-used simulators. Thus, for example, both NEURON and NEST may be controlled either via their original, native interpreter (Hoc and SLI, respectively) or via Python. More recent simulators (e.g. PCSIM, Brian) have Python as the only available scripting language. This widespread adoption of Python is probably due to a number of factors, including the powerful data structures, clean and expressive syntax, extensive library, maturity of tools for numerical analysis and visualization (allowing use of a single language for the entire modelling workflow from simulation to analysis to graphing), and the ease-of-use of Python as a glue language which allows computation-intensive code written in a low-level language such as C to be transparently accessed within high-level Python code.</p><p>Python alone does not address the translation problem (although it does make the translation process easier, since at least simple data structures such as lists and arrays are the same for each simulator), since neuroscience-specific concepts are still expressed differently. However, it is now possible to define a simulator-independent Python interface for neuronal network simulators and to implement automatic translation to any Python-enabled simulator. We have designed and implemented such an interface, PyNN (pronounced &#x0201c;pine&#x0201d;). In this paper we describe its design, concepts, implementation and use. We do not attempt here to provide a complete user guide &#x02013; this may be found online at <uri xlink:type="simple" xlink:href="http://neuralensemble.org/PyNN">http://neuralensemble.org/PyNN</uri>.</p></sec><sec><title>Design Goals</title><p>When designing and implementing a common simulator interface, the following goals should be taken into account. These are the goals we have kept in mind when designing and implementing the PyNN interface, but they are equally applicable to any other such interface.</p><p><bold>Write the code for a model once, run it on any supported simulator or hardware device <italic>without modification</italic></bold>. This is the primary design goal for PyNN.</p><p><bold>Support a high-level of abstraction</bold>. For example, it is often preferable to deal with a single object representing a population of neurons than to deal with all the individual neurons directly. Each single neuron can be accessed when necessary, but in many cases the population is the more useful abstraction. The advantages of this approach are that (i) it is easier to maintain a conceptual idea of the model, without being distracted by implementation details, and (ii) the internal implementation of an object can be optimized for speed, parallelization or memory requirements without changing the interface presented to the user.</p><p><bold>Support any feature provided by at least two supported simulators</bold>. The aim is to strike a balance between supporting all features of all simulators (unfeasible) and supporting only the subset of features common to all simulators (overly restrictive).</p><p><bold>Allow mixing of PyNN and native simulator code</bold>. PyNN should not limit the range of models that can be implemented. Following the two-simulator rule, above, there will be things that are possible in one simulator and not in any other. Although a model implementation consisting of 100% PyNN is the best scenario for running on multiple simulators, an implementation with 50% PyNN code will be easier to convert between simulators than one with no PyNN code.</p><p><bold>Facilitate porting of models between simulators</bold>. PyNN changes the process of porting a model between simulators from all-or-nothing, in which the validity of the translated model cannot be tested until the entire translation is complete, to an incremental approach, in which the native code is gradually replaced by simulator-independent code. At each stage, the hybrid code remains runnable, and so it is straightforward to verify that the model behaviour has not been changed.</p><p><bold>Minimize dependencies</bold>, to make installation as simple as possible and maximize flexibility. There are no visualization and few data analysis tools built-in to PyNN, which means the user can use any such tools they wish.</p><p><bold>Present a consistent interface on output as well as on input</bold>. The formats used for simulation outputs are consistent across simulator back-ends, making it a stable base upon which to build more complex systems of simulation control, data-analysis and visualization.</p><p><bold>Prioritize compatibility over optimizations</bold>, but allow compatibility-breaking optimizations to be selected by a deliberate choice of the user (e.g. the <monospace>compatible_output</monospace> flag of the various <monospace>print()</monospace> methods is <monospace>True</monospace> by default, but can be set to <monospace>False</monospace> to get potentially-faster writing of data to file).</p><p><bold>API Versioning</bold>. The PyNN API will inevitably evolve over time, as more simulators are supported and to take account of the preferences of the community of users. To ensure backwards compatibility, the API should be versioned so that the user can indicate which version was used for a particular implementation. Note that the examples given in this paper use version 0.4 of the API.</p><p><bold>Transparent parallelization</bold>. Code that runs on a single processor should run on multiple processors (using MPI) without changes.</p><p>Some of these goals are somewhat contradictory: for example, having a high level of abstraction and making porting easy. Reconciling this particular pair of goals has led to the presence in PyNN of both a high-level, object-oriented interface and a low-level, procedural interface that is more similar to the interface of many existing simulators. These will be discussed further below.</p></sec><sec><title>Usage Examples</title><p>Before describing in detail the concepts underlying the PyNN interface, we will work through some examples of how it is used in practice: first a simple example using the low-level, procedural interface and then a more complex example using the high-level, object-oriented interface.</p><p>For the simple example, we will build a network consisting of a single integrate-and-fire (IF) cell receiving spiking input from a Poisson process.</p><p>First, we choose which simulator to use by importing the relevant module from PyNN:<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; from pyNN.neuron import *</monospace></preformat></p><p>If we wanted to use PCSIM, we would just import <monospace>pyNN.pcsim</monospace>, etc. Whichever simulator back-end we use, none of the code below would change.</p><p>Next we set global parameters of the simulator:<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; setup(timestep=0.1, min_delay=2.0)</monospace></preformat></p><p>Now we create two cells: an IF neuron with synapses that respond to a spike with a step increase in synaptic conductance, which then decays exponentially, and a &#x0201c;spike source&#x0201d;, a simple cell that emits spikes at predetermined times but cannot receive input spikes.<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; ifcell = create(IF_cond_exp,&#x02026;                  {'i_offset': 0.11,&#x02026;                   'tau_refrac': 3.0,&#x02026;                   'v_thresh' : -51.0})&#x0003e;&#x0003e;&#x0003e; times = map(float, range(5,105,10))&#x0003e;&#x0003e;&#x0003e; source = create(SpikeSourceArray,&#x02026;                   {'spike_times': times})</monospace></preformat></p><p>Behind the scenes, the <monospace>create()</monospace> function translates the standard PyNN model name, <monospace>IF_cond_exp</monospace> in this case, into the model name used by the simulator, <monospace>Standard_IF</monospace> for NEURON, <monospace>iaf_cond_exp</monospace> for NEST, for example and also translates parameter names and units into simulator-specific names and units. To take one example, the <monospace>i_offset</monospace> parameter represents the amplitude of a constant current injected into the cell, and is given in nanoamps. The equivalent parameter of the NEST <monospace>iaf_cond_exp</monospace> model has the name <monospace>I_e</monospace> and units of picoamps, so PyNN both converts the name and multiplies the numerical value by 1000 when running with NEST. Standard cell models and automatic translation are discussed in more detail in the next section.</p><p>The <monospace>create()</monospace> function returns an <monospace>ID</monospace> object, which provides access to the parameters of the cell models, e.g.:<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; ifcell.tau_refrac3.0&#x0003e;&#x0003e;&#x0003e; ifcell.tau_m = 12.5&#x0003e;&#x0003e;&#x0003e; ifcell.get_parameters(){'tau_refrac': 3.0, 'tau_m': 12.5, 'e_rev_E': 0.0, 'i_offset': 0.11, 'cm': 1.0, 'e_rev_I': -70.0, 'v_init': -65.0, 'v_thresh': -51.0, 'tau_syn_E': 5.0, 'v_rest': -65.0, 'tau_syn_I': 5.0, 'v_reset': -65.0}</monospace></preformat></p><p>Having created the cells, we connect them with the <monospace>connect()</monospace> function:<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; connect(source, ifcell, weight=0.006,&#x02026;           synapse_type='excitatory', delay=2.0)</monospace></preformat></p><p>Now we tell the system what variable or variables to record, run the simulation and finish.<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; record_v(ifcell, 'ifcell.dat')&#x0003e;&#x0003e;&#x0003e; run(200.0)&#x0003e;&#x0003e;&#x0003e; end()</monospace></preformat></p><p>The result of running the above model is shown in Figure <xref ref-type="fig" rid="F1">1</xref>, which also shows the degree of reproducibility obtainable between different simulators for such a simple network.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Results of running first example given in the text, with NEURON, NEST and PCSIM as back-end simulators</bold>. <bold>(A)</bold> Entire membrane potential trace with integration time-step 0.1&#x02009;ms. <bold>(B)</bold> Zoom into a smaller region of the trace, showing small numerical differences between the results of the different simulators. <bold>(C)</bold> Results of a simulation with integration time-step 0.01&#x02009;ms, showing greatly reduced numerical differences.</p></caption><graphic xlink:href="fninf-02-011-g001"/></fig><p>The low-level, procedural interface, using the <monospace>create()</monospace>, <monospace>connect()</monospace> and <monospace>record()</monospace> functions, is useful for simple models or when porting an existing model written in a different language that uses the create/connect idiom. For larger, more complex networks we have found that an object-oriented approach, with a higher-level of abstraction, is more effective, since it both clarifies the conceptual structure of the model, by hiding implementation details, and allows behind-the-scenes optimizations.</p><p>To illustrate the high-level, object-oriented interface we turn now from the simple example of a few neurons to a more complex example: a network of several thousand excitatory and inhibitory neurons that displays self-sustained activity (based on the &#x0201c;CUBA&#x0201d; model of Vogels and Abbott (<xref ref-type="bibr" rid="B18">2005</xref>), and reproducing the benchmark model used in Brette et al. (<xref ref-type="bibr" rid="B2">2007</xref>)). This still is not a particularly complicated network, since it has only two cell types, no spatial structure and no heterogeneity of neuronal or connection properties, but in demonstrating how building such a network becomes trivial using PyNN we hope to convince the reader that building genuinely complex, structured and heterogeneous networks becomes manageable.</p><p>Again, we begin by choosing which simulator to use. We also import some classes from PyNN's <monospace>random</monospace> module.<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; from pyNN.nest2 import *&#x0003e;&#x0003e;&#x0003e; from pyNN.random import (NumpyRNG,&#x02026;                            RandomDistribution)</monospace></preformat></p><p>We next specify the parameters of the neuron model (the same model and same parameters are used for both excitatory and inhibitory neurons).<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; cell_params = {&#x02026;     'tau_m':     20.0,  'tau_syn_E':   5.0,&#x02026;     'cm':         0.2,  'tau_syn_I':  10.0,&#x02026;     'v_rest':   -49.0,  'v_reset':   -60.0,&#x02026;     'v_thresh': -50.0,  'tau_refrac':  5.0&#x02026;     }</monospace></preformat></p><p>Parameters with dimensions of voltage are in millivolts, time in milliseconds and capacitance in nanofarads. The units convention is discussed further in the next section.</p><p>We now initialize the simulation, this time accepting the default values for the global parameters.<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; setup()</monospace></preformat></p><p>Now, rather than creating each cell separately, we just create a <monospace>Population</monospace> object for each different type of cell:<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; pE = Population(4000, IF_cond_exp,&#x02026;                   cell_params,&#x02026;                   label="Excitatory")&#x0003e;&#x0003e;&#x0003e; pI = Population(1000, IF_cond_exp,&#x02026;                   cell_params,&#x02026;                   label="Inhibitory")</monospace></preformat></p><p>By default, all cells of a given Population are created with identical parameters, but these can be changed afterwards. Here we wish to randomize the value of the membrane potential at the start of the simulation to values between &#x02212;50 and &#x02212;70&#x02009;mV.<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; unif_distr = RandomDistribution('uniform',&#x02026;                                   [-50,-70])&#x0003e;&#x0003e;&#x0003e; pE.randomInit(unif_distr)&#x0003e;&#x0003e;&#x0003e; pI.randomInit(unif_distr)</monospace></preformat></p><p><monospace>randomInit()</monospace> is a convenience method for randomizing the initial membrane potential. For the more general case of randomizing any cell parameter use <monospace>rset()</monospace>.</p><p>Just as individual neurons are encapsulated within <monospace>Populations</monospace>, connections between neurons are encapsulated within <monospace>Projections</monospace>. To create a <monospace>Projection</monospace> object, we need to specify how the neurons will be connected, either via an algorithm or via an explicit list. Different algorithms are encapsulated in different <monospace>Connector</monospace> classes, e.g. <monospace>FixedProbabilityConnector</monospace>, <monospace>AllToAllConnector</monospace>. An explicit list of connections can be provided via a <monospace>FromListConnector</monospace> or a <monospace>FromFileConnector</monospace>.<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; FPC = FixedProbabilityConnector&#x0003e;&#x0003e;&#x0003e; exc_conn = FPC(0.02, weights=0.004,&#x02026;                  delays=0.1)&#x0003e;&#x0003e;&#x0003e; inh_conn = FPC(0.02, weights=0.051,&#x02026;                  delays=0.1)</monospace></preformat></p><p>Note that weights are in microsiemens and delays in milliseconds. Where the delay is not specified, the global minimum delay specified in the <monospace>setup()</monospace> function is used. Here we set all weights and delays of a <monospace>Projection</monospace> to the same value, but it is equally possible to pass the constructor a <monospace>RandomDistribution</monospace> object, as we did above for the initial membrane potential, or an explicit list of values.</p><p>To create a <monospace>Projection</monospace>, we need to specify the pre- and post-synaptic <monospace>Populations</monospace>, a <monospace>Connector</monospace> object, and a synapse type. The standard IF cells each have two synapse types, &#x0201c;<monospace>excitatory</monospace>&#x0201d; and &#x0201c;<monospace>inhibitory</monospace>&#x0201d;. User-defined models can use arbitrary names, e.g. &#x0201c;<monospace>AMPA</monospace>&#x0201d;, &#x0201c;<monospace>NMDA</monospace>&#x0201d;.<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; e2e = Projection(pE, pE, exc_conn,&#x02026;                    target='excitatory')&#x0003e;&#x0003e;&#x0003e; e2i = Projection(pE, pI, exc_conn,&#x02026;                    target='excitatory')&#x0003e;&#x0003e;&#x0003e; i2e = Projection(pI, pE, inh_conn,&#x02026;                    target='inhibitory')&#x0003e;&#x0003e;&#x0003e; i2i = Projection(pI, pI, inh_conn,&#x02026;                    target='inhibitory')</monospace></preformat></p><p>Having constructed the network, we now need to instrument it, using the <monospace>record()</monospace> (for recording spikes) and <monospace>record_v()</monospace> (membrane potential) methods of the <monospace>Population</monospace> objects. Here we choose to record spikes from 1000 of the excitatory neurons (chosen at random) and all of the inhibitory neurons, and to record the membrane potential of two specific excitatory neurons. We then run the simulation for 1000&#x02009;ms.<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; pE.record(1000)&#x0003e;&#x0003e;&#x0003e; pI.record()&#x0003e;&#x0003e;&#x0003e; pE.record_v([pE[0], pE[1]])&#x0003e;&#x0003e;&#x0003e; run(1000.0)</monospace></preformat></p><p>After running the simulation, we can access the results or write them to file.<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; pI.getSpikes()[:5]array([[ 715. ,     1.5],       [ 609. ,     1.6],       [ 708. ,     1.7],       [ 796. ,     1.7],       [  34. ,     1.8]])&#x0003e;&#x0003e;&#x0003e; pE.get_v()[:5]array([[  0.   ,     0.1  ,  -55.073],       [  1.   ,     0.1  ,  -50.163],       [  0.   ,     0.2  ,  -55.098],       [  1.   ,     0.2  ,  -50.212],       [  0.   ,     0.3  ,  -55.122]])&#x0003e;&#x0003e;&#x0003e; end()</monospace></preformat></p><p>The results of running simulations of the above network with two different simulator back-ends are shown in Figure <xref ref-type="fig" rid="F2">2</xref>.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Results of running the second example given in the text, with NEURON and NEST as back-end simulators</bold>. Note that the network connectivity and initial conditions were identical in the two cases. <bold>(A)</bold> Membrane potential traces for two excitatory neurons. Note that the NEST and NEURON traces are very similar for the first 50&#x02009;ms, but after that diverge rapidly due to the effects of network activity, which amplifies the small numerical integration differences. <bold>(B)</bold> Spiking activity of excitatory (black) and inhibitory (green) neurons. Each dot represents a spike and each row of dots a different neuron. All 5000 neurons are shown. <bold>(C)</bold> Distribution of pooled inter-spike intervals (ISIs) for excitatory and inhibitory neurons. <bold>(D)</bold> Distribution over neurons of the coefficient of variation of the ISI [CV(ISI)].</p></caption><graphic xlink:href="fninf-02-011-g002"/></fig></sec><sec><title>Principal Concepts</title><p>To achieve the goal of &#x0201c;<italic>write the code for a model once, run it on any supported simulator without modification</italic>&#x0201d; requires (i) a common interface, (ii) neuron and synapse models that are standardized across simulators, (iii) consistent handling of physical units, (iv) consistent handling of (pseudo-)random numbers. To achieve the twin goals of supporting a high-level of abstraction and facilitating porting of models between simulators requires both an object-oriented and a procedural interface. The implementation of all these requirements is described in more depth in the following. We also illustrate the mixing of PyNN and native simulator code, and how PyNN can support features that are found in only a single simulator back-end, by describing support for multi-compartmental models.</p><sec><title>Standard cell models</title><p>A fundamental concept in PyNN is the cell type &#x02013; a given model of a neuron, representable by a set of equations, and comprising sub-threshold behaviour, spiking mechanism and post-synaptic response. The public interface of a cell type is mainly defined by its parameters. Different neurons of the same cell type may have very different behaviour if they have different values of the parameters. For example, the Izhikevich model (Izhikevich, <xref ref-type="bibr" rid="B13">2003</xref>), can reproduce a wide range of spiking patterns, from fast-spiking through regular spiking to multiple types of bursting, depending on the parameter values chosen. A cell type is therefore a model type rather than a biologically defined cell type (such as &#x0201c;Layer V pyramidal neuron&#x0201d;, for example).</p><p>When using a given simulator back-end, PyNN can work with any cell type that is supported by that simulator. In this case, the cell type is generally represented by a string, holding a model name that is meaningful for that simulator, e.g. &#x0201c;<monospace>iaf_neuron</monospace>&#x0201d; in NEST.</p><p>Of course, such a cell type will only work with one simulator. To create a model that will run on different simulators requires you to use one of PyNN's built-in, standard cell models, each represented by a sub-class of the <monospace>StandardCell</monospace> class. The models provided by PyNN include various simple IF models, the Izhikevich-like adaptive exponential IF model (Brette and Gerstner, <xref ref-type="bibr" rid="B1">2005</xref>), a single-compartment neuron with Hodgkin&#x02013;Huxley sodium and potassium channels, and various models that emit spikes (e.g. according to a Poisson process) but cannot receive them.</p><p>The <monospace>StandardCell</monospace> class contains machinery for translating model names, parameter names and parameter units between PyNN standardized values and simulator-specific values. This is particularly useful when the underlying simulators use different unit systems or different parameterizations of the same set of equations, e.g. when one simulator expects the membrane time constant and another the membrane leak conductance. An example of the translations performed by PyNN is given in Table <xref ref-type="table" rid="T1">1</xref>.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p><bold>Comparison of parameter names and units for different implementations of a leaky integrate-and-fire model with a fixed firing threshold and current-based, alpha-function synapses</bold>. This model is called <monospace>IF_curr_alpha</monospace> in PyNN, <monospace>iaf_psc_alpha</monospace> in NEST, <monospace>LIFCurrAlphaNeuron</monospace> in PCSIM and <monospace>StandardIF</monospace> in NEURON (this is a model template distributed with PyNN and is not in the standard NEURON distribution). Manual conversion of names and units is straightforward but error-prone and time-consuming. PyNN takes care of such conversions transparently.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Parameter</th><th align="center" colspan="2" rowspan="1">PyNN</th><th align="center" colspan="2" rowspan="1">NEST</th><th align="center" colspan="2" rowspan="1">NEURON</th><th align="center" colspan="2" rowspan="1">PCSIM</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Resting membrane potential</td><td align="left" rowspan="1" colspan="1"><monospace>v_rest</monospace></td><td align="left" rowspan="1" colspan="1">mV</td><td align="left" rowspan="1" colspan="1"><monospace>E_L</monospace></td><td align="left" rowspan="1" colspan="1">mV</td><td align="left" rowspan="1" colspan="1"><monospace>v_rest</monospace></td><td align="left" rowspan="1" colspan="1">mV</td><td align="left" rowspan="1" colspan="1"><monospace>Vresting</monospace></td><td align="left" rowspan="1" colspan="1">V</td></tr><tr><td align="left" rowspan="1" colspan="1">Reset membrane potential</td><td align="left" rowspan="1" colspan="1"><monospace>v_reset</monospace></td><td align="left" rowspan="1" colspan="1">mV</td><td align="left" rowspan="1" colspan="1"><monospace>V_reset</monospace></td><td align="left" rowspan="1" colspan="1">mV</td><td align="left" rowspan="1" colspan="1"><monospace>v_reset</monospace></td><td align="left" rowspan="1" colspan="1">mV</td><td align="left" rowspan="1" colspan="1"><monospace>Vreset</monospace></td><td align="left" rowspan="1" colspan="1">V</td></tr><tr><td align="left" rowspan="1" colspan="1">Membrane capacitance</td><td align="left" rowspan="1" colspan="1"><monospace>cm</monospace></td><td align="left" rowspan="1" colspan="1">nF</td><td align="left" rowspan="1" colspan="1"><monospace>C_m</monospace></td><td align="left" rowspan="1" colspan="1">pF</td><td align="left" rowspan="1" colspan="1"><monospace>CM</monospace></td><td align="left" rowspan="1" colspan="1">nF</td><td align="left" rowspan="1" colspan="1"><monospace>Cm</monospace></td><td align="left" rowspan="1" colspan="1">F</td></tr><tr><td align="left" rowspan="1" colspan="1">Membrane time constant</td><td align="left" rowspan="1" colspan="1"><monospace>tau_m</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>tau_m</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>tau_m</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>taum</monospace></td><td align="left" rowspan="1" colspan="1">s</td></tr><tr><td align="left" rowspan="1" colspan="1">Refractory period</td><td align="left" rowspan="1" colspan="1"><monospace>tau_refrac</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>t_ref</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>t_refrac</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>Trefrac</monospace></td><td align="left" rowspan="1" colspan="1">s</td></tr><tr><td align="left" rowspan="1" colspan="1">Excitatory synaptic time constant</td><td align="left" rowspan="1" colspan="1"><monospace>tau_syn_E</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>tau_syn_ex</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>tau_e</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>TauSynExc</monospace></td><td align="left" rowspan="1" colspan="1">s</td></tr><tr><td align="left" rowspan="1" colspan="1">Inhibitory synaptic time constant</td><td align="left" rowspan="1" colspan="1"><monospace>tau_syn_I</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>tau_syn_in</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>tau_i</monospace></td><td align="left" rowspan="1" colspan="1">ms</td><td align="left" rowspan="1" colspan="1"><monospace>TauSynInh</monospace></td><td align="left" rowspan="1" colspan="1">s</td></tr><tr><td align="left" rowspan="1" colspan="1">Spike threshold</td><td align="left" rowspan="1" colspan="1"><monospace>v_thresh</monospace></td><td align="left" rowspan="1" colspan="1">mV</td><td align="left" rowspan="1" colspan="1"><monospace>V_th</monospace></td><td align="left" rowspan="1" colspan="1">mV</td><td align="left" rowspan="1" colspan="1"><monospace>v_thresh</monospace></td><td align="left" rowspan="1" colspan="1">mV</td><td align="left" rowspan="1" colspan="1"><monospace>Vthresh</monospace></td><td align="left" rowspan="1" colspan="1">V</td></tr><tr><td align="left" rowspan="1" colspan="1">Injected current amplitude</td><td align="left" rowspan="1" colspan="1"><monospace>i_offset</monospace></td><td align="left" rowspan="1" colspan="1">nA</td><td align="left" rowspan="1" colspan="1"><monospace>I_e</monospace></td><td align="left" rowspan="1" colspan="1">pA</td><td align="left" rowspan="1" colspan="1"><monospace>i_offset</monospace></td><td align="left" rowspan="1" colspan="1">nA</td><td align="left" rowspan="1" colspan="1"><monospace>Iinject</monospace></td><td align="left" rowspan="1" colspan="1">A</td></tr></tbody></table></table-wrap><p>Currently, all the standard cell types are single-compartment or point neuron models, since PyNN currently supports only one simulator for multi-compartmental models (NEURON). Further details on using multi-compartmental models with PyNN's NEURON back-end are given below. We plan in future to allow specifying multi-compartmental cell types using a NeuroML description (Crook et al., <xref ref-type="bibr" rid="B5">2005</xref>).</p></sec><sec><title>Units</title><p>As is clear from the previous section, each simulator back-end has its own convention for which units to use for which physical quantities. The exception to this is Brian, which has a system for explicitly specifying units and for checking that equations are dimensionally consistent. In the future, we plan to adopt Brian's system for PyNN, but for now we have chosen to use a convention, which is similar to that of NEURON and NEST in that the units are those that tend to be used by experimental physiologists. An alternative would have been the convention used by PCSIM (and also by the GENESIS simulator) of using pure SI units with no prefixes. The advantage of the latter convention is that there is no need for checking equations for dimensional consistency. The disadvantage is that numerical values in such a system are often very large or very small, and hence the human intuition for reasonable and unreasonable parameter values is mostly lost.</p><p>Irrespective of the relative merits of different conventions, the most important thing is that PyNN now provides a single convention which is valid across simulators. In detail, the convention is as follows: voltage &#x02013; mV, current &#x02013; nA, conductance &#x02013; &#x003bc;S, time &#x02013; ms, capacitance &#x02013; nF.</p></sec><sec><title>Standard synapse models</title><p>In PyNN, the shape and time-course of the elementary post-synaptic current or conductance change in response to a pre-synaptic spike are considered to be a part of the post-synaptic neuron model, while all other properties of a synaptic connection, notably its weight (the peak current or conductance of the synaptic response), delay (for point models, this implicitly includes axonal propagation, chemical transmission and dendritic propagation; more morphologically and/or biophysically detailed models may model explicitly some or all of these sources of delay), and short- and long-term plasticity, are considered to depend on both pre- and post-synaptic neurons, and so are encapsulated in the concept of &#x0201c;synapse type&#x0201d; that mirrors the &#x0201c;cell type&#x0201d; discussed above.</p><p>The default type of synaptic connection in PyNN is static, with fixed synaptic weights. To model dynamic synapses, for which the synaptic weight (and possibly other properties, such as rise-time) varies depending on the recent history of post- and/or pre-synaptic activity, we use the same idea as for neurons, of standardized, named models that have the same interface and behaviour across simulators, even if the underlying implementation may be very different.</p><p>Where the approach for dynamic synapses differs from that for neurons is that we attempt a greater degree of compositionality, i.e. we decompose models into a number of components, for example for short-term and long-term dynamics, or for the timing-dependence and the weight-dependence of STDP rules, that can then be composed in different ways.</p><p>The advantage of this is that if we have <italic>n</italic> different models for component A and <italic>m</italic> models for component B, then we require only <italic>n</italic> + <italic>m</italic> models rather than <italic>n</italic> &#x000d7; <italic>m</italic>, which had advantages in terms of code-simplicity and in shorter model names. The disadvantage is that not all combinations may exist, if the underlying simulator implements composite models rather than using components itself: in this situation, PyNN checks whether a given composite model AB exists for a given simulator and raises an Exception if it does not. The composite approach may be extended to neuron models in future versions of the PyNN interface depending on the experience with composite synapse models.</p><p>Currently only a single model exists in PyNN for the short-term plasticity component, the Tsodyks&#x02013;Markram model (Markram et al., <xref ref-type="bibr" rid="B14">1998</xref>). For long-term plasticity there is a spike-timing-dependent plasticity STDP component, which itself is composed of separate timing-dependence and weight-dependence components.</p></sec><sec><title>Low-level, procedural interface</title><p>We refer to the procedural interface as &#x0201c;low-level&#x0201d; because it deals with a lower level of abstraction &#x02013; individual neurons and individual synapses &#x02013; than the object-oriented interface. The procedural interface consists of the functions <monospace>create()</monospace>, <monospace>connect()</monospace>, <monospace>set()</monospace>, <monospace>record()</monospace> (for recording spikes) and <monospace>record_v()</monospace> (for recording membrane potential). Each of these functions operates on, or returns, either individual cell <monospace>ID</monospace> objects or lists of such objects. As was described in the Usage Examples section, as well as being passed around as arguments, the <monospace>ID</monospace> object may be used for accessing/modifying the parameters of individual neurons, and takes care of parameter translation using the <monospace>StandardCell</monospace> mechanisms described above.</p><p>It is possible to some extent to mix the low-level and high-level interfaces. For example, it is possible to access individual neurons within a <monospace>Population</monospace> as <monospace>ID</monospace> objects and then use the <monospace>connect()</monospace> function to connect them, instead of using a <monospace>Projection</monospace> object.</p><p>Why have both a low-level and high-level interface? Having both is a potential source of confusion for users and is definitely a maintenance burden for developers. The main reason is to support the use of PyNN as a porting tool. The majority of neuronal network models using existing simulators use a procedural approach, and so conversion to PyNN is easier if PyNN supports the same approach. In addition, when developing a PyNN interface for a simulator, or for neuromorphic hardware, that deals primarily with individual cells and synaptic connections, it is easier to implement only the low-level interface, since the high-level interface can be built upon it.</p></sec><sec><title>High-level, object-oriented interface</title><p>Object-oriented programming has been used for many years in computer science as a method for reducing program complexity. As the ambition and scope of large-scale, biologically detailed neuronal network modelling increases, reducing program complexity will become more and more critical, as the limiting factor in computational neuroscience becomes the productivity of the programmer and not the capacity of the computer (Wilson, <xref ref-type="bibr" rid="B19">2006</xref>). It is for this reason that the preferred interface in PyNN for developing new models is an object-oriented one.</p><p>The object-oriented interface is built around three main classes:</p><p><bold>Population</bold> &#x02013; a group of cells all with the same cell type (model type). It is generally considered that the cells in a <monospace>Population</monospace> should all represent the same biological cell type, i.e. although parameter values may vary between cells in the group, all cells should have qualitatively the same firing response. This is not enforced, but is a good guideline to follow for producing understandable code. The <monospace>Population</monospace> class eliminates tedious iteration over lists of neurons and enables more efficient, array-based management of neuron properties.</p><p><bold>Projection</bold> &#x02013; the set of connections of a given synapse type between two <monospace>Populations</monospace>. Creating a Projection requires specifying the pre- and post-synaptic <monospace>Populations</monospace>, the synapse type, and the algorithm used to determine which neurons connect to which.</p><p><bold>Connector</bold> &#x02013; an encapsulation of the connection algorithm used in creating a Projection. Simple examples of such algorithms are &#x0201c;all-to-all&#x0201d;, &#x0201c;one-to-one&#x0201d; and &#x0201c;connect-each-pre-and-post-synaptic-cell-with-a-fixed-probability&#x0201d;. It is also possible to provide an explicit list of which cells are to be connected to which others. Each algorithm is defined within a subclass of the <monospace>Connector</monospace> class. PyNN contains a number of such classes, but it is fairly straightforward for a user to define their own algorithms.</p><p>In future development of PyNN, we plan to extend the interface to still higher-level abstractions, such as layers, cortical columns, brain areas and inter-areal projections. We also aim to use the high-level interface as a link between spiking network models and more abstract models that do not represent individual neurons, such as mean-field models.</p></sec><sec><title>Random numbers</title><p>The central nervous system contains many sources of noise, and activity patterns are often sufficiently complex, and possibly chaotic, to make a stochastic representation a reasonable model.</p><p>This can become a problem when comparing the behaviour of a given model run on different simulators, since random differences might obscure real inconsistencies between implementations of the model. Similarly, when performing distributed computations on parallel machines, the model behaviour should not depend on the number of processors used (Morrison et al., <xref ref-type="bibr" rid="B15">2005</xref>), and random differences can conceal real differences between the parallel and serial implementations.</p><p>For these reasons, it is important to be able to use identical sequences of random numbers in different simulators, and to have the random number used at a particular point in the program execution be independent of which processor it is running on.</p><p>Another consideration is that simulations in most cases use only pseudo-random sequences, and low-quality random number generators (RNGs) may have correlations between different elements of the sequence that can significantly affect the qualitative behaviour of a network. Hence it is necessary to be able to test the simulation with different RNGs.</p><p>PyNN supports simulator-independent RNGs and use of different generators &#x02013; currently any of the generators provided by the <monospace>numpy</monospace> package or by the GNU Scientific Library (GSL) can be used.</p><p>This is done by wrapping the <monospace>numpy</monospace> and GSL RNGs in classes with a common interface. PyNN's <monospace>random</monospace> module contains the classes <monospace>NumpyRNG</monospace> and <monospace>GSLRNG</monospace>, which both have a single method, <monospace>next(n, distribution, parameters)</monospace>, which returns <monospace>n</monospace> random numbers from a distribution of type <monospace>distribution</monospace> with parameters <monospace>parameters</monospace>, e.g.<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; from pyNN.random import NumpyRNG, GSLRNG&#x0003e;&#x0003e;&#x0003e; rngN = NumpyRNG(seed=76847376)&#x0003e;&#x0003e;&#x0003e; rngG = GSLRNG(seed=87548753)&#x0003e;&#x0003e;&#x0003e; rngN.next()0.91457981651574294&#x0003e;&#x0003e;&#x0003e; rngG.next(5)array([ 0.02518011, 0.79118205, 0.16679516,&#x02026;       0.1902914, 0.66204769])&#x0003e;&#x0003e;&#x0003e; rngN.next(3, 'gamma', [2.0, 0.5])array([ 0.48903019, 0.63129009, 0.70428452])&#x0003e;&#x0003e;&#x0003e; rngG.next(distribution='uniform')0.93618978746235371</monospace></preformat></p><p>Since all PyNN code that uses random numbers accesses the RNG classes only through this <monospace>next()</monospace> method, a user can substitute their own RNG simply by defining a wrapper class with such a method.</p><p>Since very often one wishes to use the same random distribution repeatedly, rather than changing distribution each time, the <monospace>random</monospace> module also provides the <monospace>RandomDistribution</monospace> class, which is initialized with the distribution name and parameters, and thereafter the <monospace>next()</monospace> method is simplified to take a single argument, the number of values to draw from the distribution, e.g.<preformat position="float" xml:space="preserve"><monospace>&#x0003e;&#x0003e;&#x0003e; from pyNN.random import (NumpyRNG,&#x02026;                            RandomDistribution)&#x0003e;&#x0003e;&#x0003e; rng = NumpyRNG(seed=8745753)&#x0003e;&#x0003e;&#x0003e; gamma_distr = RandomDistribution('gamma',&#x02026;                                    [2.0, 0.5],&#x02026;                                    rng=rng)&#x0003e;&#x0003e;&#x0003e; gamma_distr.next(3)array([ 0.72682412, 0.82490159, 1.03882654])</monospace></preformat></p><p>Note that NumpyRNG and GSLRNG distributions may not have the same names, e.g. &#x0201c;normal&#x0201d; for <monospace>NumpyRNG</monospace> and &#x0201c;gaussian&#x0201d; for <monospace>GSLRNG</monospace>, and the arguments may also differ. One of our future plans is to extend the <monospace>random</monospace> module in order to harmonize names across RNGs.</p></sec><sec><title>Multi-compartmental models</title><p>PyNN currently supports only a single simulator, NEURON, that is suitable for many-compartment models. Given the principle of supporting simulator-independence only for features that are shared by at least two of the supported simulators, and given PyNN's focus on network modelling, PyNN does not provide an API for specifying simulator-independent multi-compartmental models. This is a possible future development &#x02013; preliminary work has been done on a PyNN interface to the MOOSE simulator (Ray and Bhalla, <xref ref-type="bibr" rid="B16">2008</xref>) &#x02013; but a more likely path would be to make use of the NeuroML standards for specifying multi-compartmental models. In this scenario, the filename of a NeuroML level 2 file, specifying a single cell type, would be passed as the <monospace>cellclass</monospace> argument to the PyNN <monospace>create()</monospace> function or <monospace>Population</monospace> constructor.</p><p>However, since native and PyNN code can be mixed, the <monospace>pyNN.neuron</monospace> module already supports simulations with multi-compartmental models. The pre-synaptic compartment whose voltage is watched to trigger synaptic transmission (e.g. axon terminal) can be specified using the <monospace>source</monospace> argument to the <monospace>Projection</monospace> constructor, and the post-synaptic mechanism specified with the <monospace>target</monospace> argument.</p></sec><sec><title>Debugging</title><p>Should an error occur in a PyNN simulation, a good first step is to re-run it on another simulator back-end and so narrow down the source of the problem to one back-end in particular. Nevertheless, it has proven to be the case that the additional layers of abstraction provided by PyNN sometimes make it harder to track down sources of errors. To counterbalance this, PyNN traps errors coming from the simulator core and employs Python's introspection capabilities to provide additional information about the error context. For example, if an invalid parameter name is provided to a neuron model, the error message lists all the valid parameter names for that model. Furthermore, logging can be switched on via the <monospace>init_logging()</monospace> function in the <monospace>pyNN.utility</monospace> module, causing detailed information about what the system is doing to be written to file, a valuable resource for tracking down bugs.</p></sec></sec><sec><title>Implementation</title><p>PyNN is both a definition of a common simulator interface and an implementation of this interface for each supported simulator. PyNN is implemented as a Python package containing a <monospace>common</monospace> module, which defines the API and contains functionality common to all simulator back-ends, a <monospace>random</monospace> module (described above), and a module for each simulator back-end, as shown in Figure <xref ref-type="fig" rid="F3">3</xref>. Each simulator module separately implements the API, although it can make use of much shared code in <monospace>common</monospace>. In most cases, the simulator modules have been implemented by, or in close collaboration with, the simulator developers.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>The architecture of PyNN</bold>.</p></caption><graphic xlink:href="fninf-02-011-g003"/></fig><p>PyNN currently fully supports the following simulators: NEURON (Carnevale and Hines, <xref ref-type="bibr" rid="B4">2006</xref>; Hines and Carnevale, <xref ref-type="bibr" rid="B11">1997</xref>; Hines et al., <xref ref-type="bibr" rid="B12">2008</xref>), NEST (Eppler et al., <xref ref-type="bibr" rid="B7">2008</xref>; Gewaltig and Diesmann, <xref ref-type="bibr" rid="B8">2007</xref>), PCSIM (<uri xlink:type="simple" xlink:href="http://www.lsm.tugraz.at/pcsim/">http://www.lsm.tugraz.at/pcsim/</uri>) and Brian (Goodman and Brette, <xref ref-type="bibr" rid="B10">2008</xref>). Support for MOOSE (Ray and Bhalla, <xref ref-type="bibr" rid="B16">2008</xref>) and for export in NeuroML format (Crook et al., <xref ref-type="bibr" rid="B5">2005</xref>) is under development.</p><p>PyNN also supports the Heidelberg neuromorphic hardware system (Schemmel et al., <xref ref-type="bibr" rid="B17">2007</xref>). This illustrates a major benefit of the existence of a common neuronal simulation interface: novel simulation or emulation systems do not need to develop their own programming interface, but can benefit from an existing one that guarantees interoperability with existing tools. Using PyNN as the interface to neuromorphic hardware systems provides the possibility of closing the gap between the two domains of numerical simulation and physical emulation, which have so far coexisted rather separately.</p></sec><sec><title>Limitations on Reproducibility</title><p>For a given model with a given parameter set run on a given version of a given simulator, it should be possible to exactly reproduce a simulation result, independent of computer architecture (except where this affects the precision of the floating-point representation) or operating system. For parallel systems, results should also be independent of how many threads or processes are used in the computation, although here exact quantitative reproduction is harder to achieve. Reproducibility across different versions of a given simulator is not essential provided the precise version used to generate a given result is specified, but it is of course highly desirable. When running a model on different simulators, exact reproduction is impossible to achieve, except in simple cases, due to round-off errors in floating point calculations. When validating a model implementation by running it on two or more simulators, therefore, what level of reproducibility is achievable, and how can we tell whether any differences are due to round-off error or to implementation errors?</p><p>To get a preliminary handle on this problem, we have compared the difference in model activity between two simulators to the difference due to two different initial conditions with the same simulator.</p><p>Our test case is the balanced random network, based on Vogels and Abbott (<xref ref-type="bibr" rid="B18">2005</xref>), whose implementation was shown above. The activity pattern of this network is very sensitive to initial conditions (chaotic or near-chaotic), and so we cannot use differences in the precise spike pattern to measure reproducibility: we are more interested in the statistical properties of the activity, and so we have chosen to take the distribution of inter-spike intervals (ISIs) of excitatory neurons (see Figure <xref ref-type="fig" rid="F2">2</xref>C) as a measure of network activity.</p><p>To measure the difference between the distributions from two different runs we use the Kolmogorov&#x02013;Smirnov two-sample test. We ran the simulation ten times, each time with a different seed for the RNG used to generate the initial membrane potential distribution, with both NEURON and NEST back-ends. This gave values for the Kolmogorov&#x02013;Smirnov D-statistic between 0.008 and 0.026 (<italic>n</italic>&#x02009;&#x02243;&#x02009;19000) with a mean of 0.015, with associated <italic>p</italic>-values (probability that the two distributions are the same) between 6.3&#x02009;&#x000d7;&#x02009;10<sup>&#x02212;5</sup> and 0.68 with mean 0.15.</p><p>We then ran the simulation twenty times just on NEURON, each time with a different RNG seed, to give 10 pairs of distributions. In this case the <italic>D</italic>-values were in the range 0.007&#x02013;0.026, mean 0.015, and the <italic>p</italic>-values in the range 2.8&#x02009;&#x000d7;&#x02009;10<sup>&#x02212;5</sup> to 0.77, mean 0.20.</p><p>In summary, the differences due to different simulators are in almost exactly the same range as those due to different initial conditions, suggesting that the differences between the simulators are indeed due to round-off errors and that there are not, therefore, any implementation errors in this case.</p><p>It is also interesting to note that in most cases the null hypothesis is supported, i.e. the distributions are the same, but that for some initial conditions there are highly significant differences between the ISI distributions. The ISI distribution may not therefore be the best measure for reproducibility in this case.</p></sec><sec sec-type="discussion"><title>Discussion</title><p>In this article we have presented PyNN, a Python-based common simulator interface, which allows simulator-independent model specification. PyNN is already in use in a number of research groups, and has been a key technology enabling improved communication between labs in a pan-European collaborative project with a major component of modelling and of neuromorphic hardware development (the FACETS project: <uri xlink:type="simple" xlink:href="http://www.facets-project.org">http://www.facets-project.org</uri>).</p><p>By providing a standard simulation platform, PyNN also has the potential to act as the foundation for other, simulator agnostic but neuroscience-specific, tools such as analysis, visualization and data-management software.</p><p>PyNN is not the only project to address simulator-independent model specification and simulator interoperability (review in Cannon et al., <xref ref-type="bibr" rid="B3">2007</xref>). neuroConstruct (Gleeson et al., <xref ref-type="bibr" rid="B9">2007</xref>) is a tool to develop networks of morphologically-detailed neurons using a graphical user interface (GUI), that can generate code for both the NEURON and GENESIS simulators. A limitation with respect to PyNN is that since it uses code generation rather than a direct interface, neuroConstruct cannot receive information back from the simulator except by reading the data files it generates. A second limitation is that features that are not available through the GUI cannot be incorporated in a model. The NeuroML standards (Crook et al., <xref ref-type="bibr" rid="B5">2005</xref>, <uri xlink:type="simple" xlink:href="http://www.neuroml.org">http://www.neuroml.org</uri>) are intended to provide an infrastructure for exchanging model specifications between groups in a simulator-independent way. Their scope includes much more detailed levels of modelling, e.g. membrane ion channels and detailed dendritic morphology, than are supported by PyNN. They have the advantage over PyNN of being language-independent, since specifications are written in XML, for which tools exist in all major programming languages. The major disadvantage of purely declarative specifications is lack of flexibility: if a concept or entity is not defined in the standard, it is not possible to specify models that use it, whereas with a procedural/imperative or mixed declarative-procedural specification such as is achievable with PyNN, arbitrary specifications are possible.</p><p>Although we emphasize here the differences between the GUI, pure-declarative, and programming-interface approaches to simulator-independent model specification, in fact they are highly complementary. Graphical interfaces are particularly good for beginners, for teaching, for giving high-level overviews of a system, and for integrating analysis and visualization tools. It would be very useful for neuroConstruct to be able to generate PyNN code, for example, in addition to code for NEURON and GENESIS. Declarative specifications reach the highest levels of system-independence, for the range of concepts that are supported. They are also particularly suitable for transformation into human-readable formats and for automated GUI generation. As such, they seem to be best suited for domains in which the modelling approach is fairly stable, e.g. for describing neuron morphologies or non-stochastic ion channel models. In PyNN, we plan to support simulator-independent multi-compartmental models using NeuroML: in this scenario cell models would be specified in NeuroML while PyNN would be used for network specification and for simulation setup and control.</p><p>Our main priorities for future development of PyNN are to increase the number of supported simulators (simulator developers who are interested in PyNN support for their simulator are encouraged to contact us), improve the support for multi-compartmental modelling, and extend the interface towards higher-level abstractions, such as cortical columns and more abstract modelling approaches. PyNN is open source software (CeCILL licence, <uri xlink:type="simple" xlink:href="http://www.cecill.info">http://www.cecill.info</uri>) and has an open development model: anyone who wishes to contribute is welcome and invited to do so.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><ack><p>This work was supported by the European Union (FACETS project, FP6-2004-IST-FETPI-015879). Jens Kremkow is also supported by the German Federal Ministry of Education and Research (BMBF grant 01GQ0420 to BCCN, Freiburg).</p></ack><ref-list><title>References</title><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname><given-names>R.</given-names></name><name><surname>Gerstner</surname><given-names>W.</given-names></name></person-group> (<year>2005</year>). <article-title>Adaptive exponential integrate-and-fire model as an effective description of neuronal activity</article-title>. <source>J. Neurophysiol.</source><volume>94</volume>, <fpage>3637</fpage>&#x02013;<lpage>3642</lpage><pub-id pub-id-type="doi">10.1152/jn.00686.2005</pub-id><pub-id pub-id-type="pmid">16014787</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname><given-names>R.</given-names></name><name><surname>Rudolph</surname><given-names>M.</given-names></name><name><surname>Carnevale</surname><given-names>T.</given-names></name><name><surname>Hines</surname><given-names>M.</given-names></name><name><surname>Beeman</surname><given-names>D.</given-names></name><name><surname>Bower</surname><given-names>J.</given-names></name><name><surname>Diesmann</surname><given-names>M.</given-names></name><name><surname>Morrison</surname><given-names>A.</given-names></name><name><surname>Goodman</surname><given-names>P. H.</given-names></name><name><surname>Harris</surname><given-names>F.</given-names><suffix>Jr</suffix></name><name><surname>Zirpe</surname><given-names>M.</given-names></name><name><surname>Natschlager</surname><given-names>T.</given-names></name><name><surname>Pecevski</surname><given-names>D.</given-names></name><name><surname>Ermentrout</surname><given-names>B.</given-names></name><name><surname>Djurfeldt</surname><given-names>M.</given-names></name><name><surname>Lansner</surname><given-names>A.</given-names></name><name><surname>Rochel</surname><given-names>O.</given-names></name><name><surname>Vieville</surname><given-names>T.</given-names></name><name><surname>Muller</surname><given-names>E.</given-names></name><name><surname>Davison</surname><given-names>A.</given-names></name><name><surname>El Boustani</surname><given-names>S.</given-names></name><name><surname>Destexhe</surname><given-names>A.</given-names></name></person-group> (<year>2007</year>). <article-title>Simulation of networks of spiking neurons: a review of tools and strategies</article-title>. <source>J Comput. Neurosci.</source><volume>23</volume>, <fpage>349</fpage>&#x02013;<lpage>398</lpage><pub-id pub-id-type="doi">10.1007/s10827-007-0038-6</pub-id><pub-id pub-id-type="pmid">17629781</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Cannon</surname><given-names>R.</given-names></name><name><surname>Gewaltig</surname><given-names>M.</given-names></name><name><surname>Gleeson</surname><given-names>P.</given-names></name><name><surname>Bhalla</surname><given-names>U.</given-names></name><name><surname>Cornelis</surname><given-names>H.</given-names></name><name><surname>Hines</surname><given-names>M.</given-names></name><name><surname>Howell</surname><given-names>F.</given-names></name><name><surname>Muller</surname><given-names>E.</given-names></name><name><surname>Stiles</surname><given-names>J.</given-names></name><name><surname>Wils</surname><given-names>S.</given-names></name><name><surname>De Schutter</surname><given-names>E.</given-names></name></person-group> (<year>2007</year>). <article-title>Interoperability of neuroscience modeling software: current status and future directions</article-title>. <source>Neuroinformatics</source><volume>5</volume>, <fpage>127</fpage>&#x02013;<lpage>138</lpage><pub-id pub-id-type="doi">10.1007/s12021-007-0004-5</pub-id><pub-id pub-id-type="pmid">17873374</pub-id></citation></ref><ref id="B4"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Carnevale</surname><given-names>N. T.</given-names></name><name><surname>Hines</surname><given-names>M. L.</given-names></name></person-group> (<year>2006</year>). <article-title>The NEURON Book</article-title>. <publisher-loc>Cambridge</publisher-loc>, <publisher-name>University Press.</publisher-name></citation></ref><ref id="B5"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Crook</surname><given-names>S.</given-names></name><name><surname>Beeman</surname><given-names>D.</given-names></name><name><surname>Gleeson</surname><given-names>P.</given-names></name><name><surname>Howell</surname><given-names>F.</given-names></name></person-group> (<year>2005</year>). <article-title>XML for model specification in neuroscience: an introduction and workshop summary</article-title>. <source>Brains Minds Media</source><volume>1</volume>, bmm228 (urn:nbn:de:0009&#x02013;3&#x02013;2282).</citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ekeberg</surname><given-names>&#x000d6;.</given-names></name><name><surname>Djurfeldt</surname><given-names>M.</given-names></name></person-group> (<year>2008</year>). <article-title>MUSIC &#x02013; multisimulation coordinator: request for comments</article-title>. <source>Nat. Precedings</source><uri xlink:type="simple" xlink:href="http://dx.doi.org/10.1038/npre.2008.1830.1">http://dx.doi.org/10.1038/npre.2008.1830.1</uri><pub-id pub-id-type="doi">10.1038/npre.2008.1830.1</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Eppler</surname><given-names>J.</given-names></name><name><surname>Helias</surname><given-names>M.</given-names></name><name><surname>Diesmann</surname><given-names>M.</given-names></name><name><surname>Gewaltig</surname><given-names>M.-O.</given-names></name></person-group> (<year>2008</year>). <article-title>PyNEST: a convenient interface to the NEST simulator</article-title>. <source>Front. Neuroinform.</source><volume>2</volume> <pub-id pub-id-type="doi">10.3389/neuro.11.012.2008</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gewaltig</surname><given-names>M.-O.</given-names></name><name><surname>Diesmann</surname><given-names>M.</given-names></name></person-group> (<year>2007</year>). <article-title>NEST (NEural Simulation Tool)</article-title>. <source>Scholarpedia</source><volume>2</volume>, <fpage>1430</fpage></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gleeson</surname><given-names>P.</given-names></name><name><surname>Steuber</surname><given-names>V.</given-names></name><name><surname>Silver</surname><given-names>R. A.</given-names></name></person-group> (<year>2007</year>). <article-title>neuroConstruct: a tool for modeling networks of neurons in 3D space</article-title>. <source>Neuron</source><volume>54</volume>, <fpage>219</fpage>&#x02013;<lpage>235</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.03.025</pub-id><pub-id pub-id-type="pmid">17442244</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>D.</given-names></name><name><surname>Brette</surname><given-names>R.</given-names></name></person-group> (<year>2008</year>). <article-title>Brian: a simulator for spiking neural networks in Python</article-title>. <source>Front. Neuroinform.</source><volume>2</volume> <pub-id pub-id-type="doi">10.3389/neuro.11.005.2008</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>M. L.</given-names></name><name><surname>Carnevale</surname><given-names>N. T.</given-names></name></person-group> (<year>1997</year>). <article-title>The NEURON simulation environment</article-title>. <source>Neural Comput.</source><volume>9</volume>, <fpage>1179</fpage>&#x02013;<lpage>1209</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.6.1179</pub-id><pub-id pub-id-type="pmid">9248061</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>M.</given-names></name><name><surname>Davison</surname><given-names>A.</given-names></name><name><surname>Muller</surname><given-names>E.</given-names></name></person-group> (<year>2008</year>). <article-title>NEURON and Python</article-title>. <source>Front. Neuroinform.</source><volume>2</volume> <pub-id pub-id-type="doi">10.3389/neuron.11.001.2009</pub-id></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Izhikevich</surname><given-names>E.</given-names></name></person-group> (<year>2003</year>). <article-title>Simple model of spiking neurons</article-title>. <source>IEEE Trans Neural Netw.</source><volume>14</volume>, <fpage>1569</fpage>&#x02013;<lpage>1572</lpage><pub-id pub-id-type="doi">10.1109/TNN.2003.820440</pub-id><pub-id pub-id-type="pmid">18244602</pub-id></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Tsodyks</surname><given-names>M.</given-names></name></person-group> (<year>1998</year>). <article-title>Differential signaling via the same axon of neocortical pyramidal neurons</article-title>. <source>Proc. Natl. Acad. Sci. USA</source><volume>95</volume>, <fpage>5323</fpage>&#x02013;<lpage>5328</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.9.5323</pub-id><pub-id pub-id-type="pmid">9560274</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Morrison</surname><given-names>A.</given-names></name><name><surname>Mehring</surname><given-names>C.</given-names></name><name><surname>Geisel</surname><given-names>T.</given-names></name><name><surname>Aertsen</surname><given-names>A.</given-names></name><name><surname>Diesmann</surname><given-names>M.</given-names></name></person-group> (<year>2005</year>). <article-title>Advancing the boundaries of high-connectivity network simulation with distributed computing</article-title>. <source>Neural Comput.</source><volume>17</volume>, <fpage>1776</fpage>&#x02013;<lpage>1801</lpage><pub-id pub-id-type="doi">10.1162/0899766054026648</pub-id><pub-id pub-id-type="pmid">15969917</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ray</surname><given-names>S.</given-names></name><name><surname>Bhalla</surname><given-names>U.</given-names></name></person-group> (<year>2008</year>). <article-title>PyMOOSE: interoperable scripting in Python for MOOSE</article-title>. <source>Front. Neuroinform.</source><volume>2</volume> <pub-id pub-id-type="doi">10.3389/neuron.11.006.2008.</pub-id></citation></ref><ref id="B17"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Schemmel</surname><given-names>J.</given-names></name><name><surname>Br&#x000fc;derle</surname><given-names>D.</given-names></name><name><surname>Meier</surname><given-names>K.</given-names></name><name><surname>Ostendorf</surname><given-names>B.</given-names></name></person-group> (<year>2007</year>). <article-title>Modeling synaptic plasticity within networks of highly accelerated I&#x00026;F neurons</article-title>. In Proceedings of the 2007 IEEE International Symposium on Circuits and Systems (ISCAS'07). <publisher-loc>New Orleans</publisher-loc>, <publisher-name>IEEE Press</publisher-name>, pp. <fpage>3367</fpage>&#x02013;<lpage>3370</lpage> <pub-id pub-id-type="doi">10.1109/ISCAS.2007.378289.</pub-id></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>T.</given-names></name><name><surname>Abbott</surname><given-names>L.</given-names></name></person-group> (<year>2005</year>). <article-title>Signal propagation and logic gating in networks of integrate-and-fire neurons</article-title>. <source>J. Neurosci.</source><volume>25</volume>, <fpage>10786</fpage>&#x02013;<lpage>10795</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3508-05.2005</pub-id><pub-id pub-id-type="pmid">16291952</pub-id></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>G.</given-names></name></person-group> (<year>2006</year>). <article-title>Where's the real bottleneck in scientific computing?</article-title><source>Am. Sci.</source><volume>94</volume>, <fpage>5</fpage>&#x02013;<lpage>6</lpage></citation></ref></ref-list></back></article>