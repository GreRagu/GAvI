<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article" xml:lang="EN"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Bioinformation</journal-id><journal-id journal-id-type="publisher-id">Bioinformation</journal-id><journal-title>Bioinformation</journal-title><issn pub-type="epub">0973-2063</issn><publisher><publisher-name>Biomedical Informatics Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19238249</article-id><article-id pub-id-type="pmc">2639693</article-id><article-id pub-id-type="publisher-id">002500032008</article-id><article-categories><subj-group subj-group-type="heading"><subject>Prediction Model</subject></subj-group></article-categories><title-group><article-title>Entropy based sub-dimensional evaluation and selection method for DNA microarray data classification</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Yi</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="COR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Yan</surname><given-names>Hong</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><aff id="A1"><label>1</label>School of Electrical and Information Engineering, University of Sydney, Sydney, NSW 2006 Australia</aff><aff id="A2"><label>2</label>Department of Electronic Engineering, City University of Hong Kong, Kowloon, Hong Kong</aff></contrib-group><author-notes><corresp id="COR1"><label>*</label>Yi Wang: <email>kingoneonewy@hotmail.com</email></corresp></author-notes><pub-date pub-type="collection"><year>2008</year></pub-date><pub-date pub-type="epub"><day>03</day><month>11</month><year>2008</year></pub-date><volume>3</volume><issue>3</issue><fpage>124</fpage><lpage>129</lpage><history><date date-type="received"><day>21</day><month>7</month><year>2008</year></date><date date-type="accepted"><day>13</day><month>9</month><year>2008</year></date></history><permissions><copyright-statement>&#x000a9; 2007 Biomedical Informatics Publishing Group</copyright-statement><copyright-year>2007</copyright-year><license license-type="open-access"><p>This is an open-access article, which permits unrestricted use, distribution, and reproduction in any medium, 		for non-commercial purposes, provided the original author and source are credited.</p></license></permissions><abstract><p>					DNA microarray allows the measurement of expression levels of tens of thousands of genes simultaneously and has many 			applications in biology and medicine. Microarray data are very noisy and this makes it difficult for data analysis and            classification. Sub-dimension based methods can overcome the noise problem by partitioning the conditions into sub-groups,             performing classification with each group and integrating the results. However, there can be many sub-dimensional groups,             which lead to a high computational complexity. In this paper, we propose an entropy-based method to evaluate and select             important sub-dimensions and eliminate unimportant ones. This improves the computational efficiency considerably. We             have tested our method on four microarray datasets and two other real-world datasets and the experiment results prove the             effectiveness of our method.           </p></abstract><kwd-group><kwd>DNA microarray</kwd><kwd>datasets</kwd><kwd>entropy</kwd><kwd>sub-dimension</kwd><kwd>probabilistic neural network</kwd></kwd-group></article-meta></front><body><sec id="s1"><title>Background</title><p>The development of microarray technology has made it possible to measure the expression levels of tens of thousands of genes in parallel and enhance our understanding of functional genomics. An important task in DNA microarray data analysis is to identify genes which have similar expression patterns in order to understand their biological functions and cellular processes. This process can be done manually, in which case the amount of effort would be tremendous and intensive. Thus, it is important to develop computerized data analysis techniques, such as classification algorithms, which are needed in many applications. In our previous study, we proposed a sub-dimension based probabilistic neural network to solve this problem [<xref ref-type="bibr" rid="R01">1</xref>]. Probabilistic neural network (PNN) was first developed by D. Specht [<xref ref-type="bibr" rid="R02">2</xref>], [<xref ref-type="bibr" rid="R03">3</xref>]. It provides a general solution to pattern classification problems by using the Bayes strategy for probability density functions. It is frequently employed in pattern classification and microarray data clustering due to its prominent time efficiency. It provides a considerable improvement in training speed compared to the conventional back-propagation network (BPN). Furthermore, as discussed in [<xref ref-type="bibr" rid="R04">4</xref>], PNN could attain the same accuracy as back-propagation neural network (BPN).</p><p>We assume that the input data consist of an <italic>n</italic> by <italic>d</italic> matrix <italic>X</italic>, where <italic>n</italic> is the number of genes (objects) and <italic>d</italic> thenumber of conditions (features). The sub-dimension based method partitions the dataset into several smaller partscalled sub-dimensions, which may or may not be disjoint [<xref ref-type="bibr" rid="R05">5</xref>]. It clusters the datasets based on their sub-dimensions. In our previous study, a voting system was used to combine all sub-dimension class results. We assigned two objects x<sub>1</sub> and x<sub>2</sub> to the same group if more than half of the sub-dimensions x<sub>1j</sub> and x<sub>2j</sub> belong to the same group. Experiment results show that the method is effective [<xref ref-type="bibr" rid="R01">1</xref>]. However, the enormous number of features in the real world microarray datasets makes it difficult to select the optimal sub-dimensions. One method is to reduce thedimensionality. In the classification, the contribution of each sub-dimension is not equal. Some may be corrupted or less relative to others, which can be discarded without degrading the performance of the system. In this paper, we employ the feature evaluation and selection technique to determine the sub-dimensions that are not as important as others in order to reduce the number of sub-dimensionswithout affecting the classification accuracy.</p><p>The aim of feature selection is to discriminate features which contain the most or the least effective informationfrom an original candidate set. Feature selection algorithms have been well researched in this area. In our study, weapply the entropy based measure combined with the subdimension method. Entropy based methods have been usedin many areas, such as mathematics, communication theory, and economics. In 1948, Shannon [<xref ref-type="bibr" rid="R06">6</xref>] first introduced the basic entropy and the information gain concept to the information domain. &#x0201c;Entropy is a measure of the amount of uncertainty in the outcome of a random experiment, or equivalently, a measure of the information obtained when the outcome is observed.&#x0201d; [<xref ref-type="bibr" rid="R07">7</xref>] In our study,the entropy can be said to be the measure of contribution that a single sub-dimension makes to the general classification. Aiming to show the convincing performance of the proposed method, normal PNN and sub-dimension combined PNN are used in experimental comparison. In this paper, we first briefly review the structure of the PNN, discuss the sub-dimension formulation, and introduce the entropy concept. Then, we describe the proposed method and present experiment results from six datasets.</p></sec><sec sec-type="methods" id="s2"><title>Methodology</title><p>Please see <xref ref-type="supplementary-material" rid="SD1">supplementary material</xref>.</p></sec><sec id="s3"><title>Discussion</title><p>Experiments based on the proposed method are performed on four microarray datasets including yeast cell cycle data, sporulation data, rodrigues data, and annot data [<xref ref-type="bibr" rid="R11">11</xref>&#x02013;<xref ref-type="bibr" rid="R14">14</xref>].To verify the proposed method, we also present the experiment results on other datasets, including wine data, Wisconsin diagnostic breast cancer (wdbc) data. For each dataset, we run the steps in section II 30 times and compute their average to evaluate the performance.</p><sec id="s3a"><title>Real world data</title><p>In order to evaluate the performance of the proposed method for noisy data, we added white Gaussian noise (wgn) randomly into the features of entire datasets as a form of corruption. The wine dataset contains 178 objects in three groups and 13 features. In our experiment, we adopt 78 objects as training samples and the remaining 100 objects for testing. As shown in Table 1 (<xref ref-type="supplementary-material" rid="SD1">supplementary material</xref>), the sub-dimension based PNN obtains 90 correct out of 100, compared with 71 correct out of 100 in normal PNN. However, with 89&#x000a2; accuracy, we can see that the proposed method provides a comparable performance with the sub-dimension based PNN.</p><p>The wdbc dataset has 576 objects in two classes and 30 features in which 276 training samples and 300 testingsamples are used to test the recognition results. As in the case for the wine data, the proposed method shows closeresults in the wdbc dataset, 279 correct classifications compared with 280 by the sub-dimension based PNN, andis superior to the normal PNN.</p></sec><sec id="s3b"><title>Microarray data</title><p>The yeast cell cycle dataset consisting of 6220 genes is published by Cho and colleagues [<xref ref-type="bibr" rid="R11">11</xref>]. In the study of the sub-dimension method [<xref ref-type="bibr" rid="R05">5</xref>], we adopt 384 genes and normalized each gene expression profile so that it has zero mean and unit variance. The dataset has five cycle phases which are the G1 phase, late G1 phase, S phase, S2 phase and M phase, and 17 time points. The results are given in Table 3 under <xref ref-type="supplementary-material" rid="SD1">supplementary material</xref>. The proposed method correctly classifies 149 out of 200 testing samples and the sub-dimension based PNN correctly classifies 150. The error is only 0.5&#x000a2;.</p><p>The sporulation dataset contains 6118 genes with seven features. In [<xref ref-type="bibr" rid="R05">5</xref>], after pre-processing, we use only 1136 genes of which the value of the root mean square of the log2 transformed the data greater than 1.13. The dataset has seven phases: metabolic, early I, early II early middle, middle, mid-late, and late. We use 736 genes for trainingand the remaining 400 genes for testing. As shown in Table 4(<xref ref-type="supplementary-material" rid="SD1">supplementary material</xref>), the proposed method works well with an accuracy rate of 48.5&#x000a2; (194 out of 400) compared with 49.5&#x000a2; for the sub-dimension based PNN.</p><p>Rodriguez dataset is available elsewhere [<xref ref-type="bibr" rid="R13">13</xref>]. It contains 974 genes clustered to nine groups with 47 features and 500 of the genes are used for testing. Clearly Table 5 (<xref ref-type="supplementary-material" rid="SD1">supplementary material</xref>) shows that the proposed method achieves an improvement of the same recognition accuracy with the sub-dimension based PNN (82.4&#x000a2;). As comparison, the normal PNN classification results are 79.6&#x000a2; accuracy. Similar results on the Annton dataset, containing 639 genes in five classes and 47 features, of which half are in the test set. As expected, the test setpresents almost the same success as the sub-dimension based PNN, at 73&#x000a2; accuracy. The normal PNN could only obtain 283 correct out of 400 testing data. As shown in the tables (under <xref ref-type="supplementary-material" rid="SD1">supplementary material</xref>), the proposed method performs very closely to the sub-dimension based PNN which uses all sub-dimension features.</p></sec></sec><sec id="s4"><title>Conclusion</title><p>Instead of considering all features of datasets in a classifier, our previous paper [<xref ref-type="bibr" rid="R01">1</xref>] implemented the PNN classification on single sub-dimensions. However, the number of combinations of sub-dimensions is large and this overallsystem computationally to complicated. In this paper, a feature evaluation and selection technique based on an entropy definition is used to measure the contribution of each sub-dimension. The sub-dimension with the lowest contribution to the overall classification is discarded. Experiments on two real world datasets and four microarray datasets show clearly that the achievement of the proposed technique is remarkable better than the normal PNN and as good as the sub-dimension based PNN. However the system complexity is significantly reduced and the classification speed is increased. The feature evaluation and selection are especially effective andconvenient when the input features are large and the datasets are noisy. At the rank of the corresponding information gain <bold><italic>G</italic></bold>, the importance of the sub-dimension decreases while <bold><italic>G</italic></bold> reduces. Good performance selection occurs particularly at the top of the rank. However, how many sub-dimensions should be considered as important is a critical issue which needs to be investigated further.</p></sec><sec sec-type="supplementary-material"><title>Supplementary material</title><supplementary-material content-type="local-data" id="SD1"><caption><title>Data 1</title></caption><media xlink:href="97320630003124S1.pdf" xlink:type="simple" id="N0x1eff0a0N0x3bbf690" position="anchor" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec></body><back><ref-list><title>References</title><ref id="R01"><label>1</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><etal/></person-group><source>Lecture Notes in Engineering and Computer Science, Hong Kong</source><year>2008</year><volume>222</volume></citation></ref><ref id="R02"><label>2</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Specht</surname><given-names>F</given-names></name></person-group><source>Proc. IEEE Int. Conf. Neural Networks, San Diego, CA</source><year>1988</year><volume>1</volume><fpage>525</fpage></citation></ref><ref id="R03"><label>3</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Specht</surname><given-names>F</given-names></name></person-group><source>IEEE Trans Neural Networks</source><year>1990</year><volume>111</volume></citation></ref><ref id="R04"><label>4</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Specht</surname><given-names>F</given-names></name><name><surname>Shapiro</surname><given-names>PD</given-names></name></person-group><source>IJCNN-91-Seattle Int Joint Conf Neural Networks</source><year>1991</year><volume>1</volume><fpage>887</fpage></citation></ref><ref id="R05"><label>5</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lam</surname><given-names>BSY</given-names></name><name><surname>Yan</surname><given-names>H</given-names></name></person-group><source>Physical Review E</source><year>2006</year><volume>74</volume><fpage>041906</fpage></citation></ref><ref id="R06"><label>6</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>E</given-names></name></person-group><source>The Bell System Technical Journal</source><year>1948</year><volume>27</volume><fpage>379</fpage></citation></ref><ref id="R07"><label>7</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>HM</given-names></name><etal/></person-group><source>IEEE Trans Syst, Man, Cybern C</source><year>2001</year><volume>31</volume></citation></ref><ref id="R08"><label>8</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kartalopoulos</surname><given-names>SV</given-names></name></person-group><source>USA: IEEE Neural Networks Council</source><year>1996</year><fpage>104</fpage></citation></ref><ref id="R09"><label>9</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lavalle</surname><given-names>MM</given-names></name><etal/></person-group><source>IEEE Proc</source><year>2006</year></citation></ref><ref id="R10"><label>10</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname><given-names>Z</given-names></name><name><surname>Yan</surname><given-names>H</given-names></name></person-group><source>,</source><year>1995</year><volume>34</volume><fpage>12</fpage></citation></ref><ref id="R11"><label>11</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>R</given-names></name><etal/></person-group><source>Molecular Cell</source><year>1998</year><volume>2</volume><fpage>65</fpage><pub-id pub-id-type="pmid">9702192</pub-id></citation></ref><ref id="R12"><label>12</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chu</surname><given-names>S</given-names></name><etal/></person-group><source>Journal of Bacteriology</source><year>2006</year><volume>188</volume><fpage>8178</fpage><pub-id pub-id-type="pmid">16997946</pub-id></citation></ref><ref id="R13"><label>13</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Whitfield</surname><given-names>ML</given-names></name><etal/></person-group><source>Mol Biol Cell</source><year>2002</year><volume>13</volume><fpage>1977</fpage><pub-id pub-id-type="pmid">12058064</pub-id></citation></ref><ref id="R14"><label>14</label><citation citation-type="web"><ext-link ext-link-type="uri" xlink:href="http://www.ics.uci.edu/~mlearn.MLSymmary.html">	http://www.ics.uci.edu/~mlearn.MLSymmary.html</ext-link></citation></ref></ref-list></back></article>