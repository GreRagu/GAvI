<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19208168</article-id><article-id pub-id-type="pmc">2648754</article-id><article-id pub-id-type="publisher-id">1471-2105-10-S1-S64</article-id><article-id pub-id-type="doi">10.1186/1471-2105-10-S1-S64</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>A new classification method using array Comparative Genome Hybridization data, based on the concept of Limited Jumping Emerging Patterns</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Gambin</surname><given-names>Tomasz</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>T.Gambin@ii.pw.edu.pl</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Walczak</surname><given-names>Krzysztof</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>K.Walczak@ii.pw.edu.pl</email></contrib></contrib-group><aff id="I1"><label>1</label>Faculty of Electronics and Information Technology of Warsaw University of Technology, Institute of Computer Science, Nowowiejska 15/19, Warsaw, 00-665, Poland</aff><pub-date pub-type="collection"><year>2009</year></pub-date><pub-date pub-type="epub"><day>30</day><month>1</month><year>2009</year></pub-date><volume>10</volume><issue>Suppl 1</issue><supplement><named-content content-type="supplement-title">Selected papers from the Seventh Asia-Pacific Bioinformatics Conference (APBC 2009)</named-content><named-content content-type="supplement-editor">Michael Q Zhang, Michael S Waterman and Xuegong Zhang</named-content></supplement><fpage>S64</fpage><lpage>S64</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10/S1/S64"/><permissions><copyright-statement>Copyright &#x000a9; 2009 Gambin and Walczak; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2009</copyright-year><copyright-holder>Gambin and Walczak; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an open access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>               Gambin               Tomasz                              T.Gambin@ii.pw.edu.pl            </dc:author><dc:title>            A new classification method using array Comparative Genome Hybridization data, based on the concept of Limited Jumping Emerging Patterns         </dc:title><dc:date>2009</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 10(Suppl 1): S64-. (2009)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2009)10:Suppl 1&#x0003c;S64&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>Classification using aCGH data is an important and insufficiently investigated problem in bioinformatics. In this paper we propose a new classification method of DNA copy number data based on the concept of limited Jumping Emerging Patterns. We present the comparison of our limJEPClassifier to SVM which is considered the most successful classifier in the case of high-throughput data.</p></sec><sec><title>Results</title><p>Our results revealed that the classification performance using limJEPClassifier is significantly higher than other methods. Furthermore, we show that application of the limited JEP's can significantly improve classification, when strongly unbalanced data are given.</p></sec><sec><title>Conclusion</title><p>Nowadays, aCGH has become a very important tool, used in research of cancer or genomic disorders. Therefore, improving classification of aCGH data can have a great impact on many medical issues such as the process of diagnosis and finding disease-related genes. The performed experiment shows that the application of Jumping Emerging Patterns can be effective in the classification of high-dimensional data, including these from aCGH experiments.</p></sec></abstract><conference><conf-date>13&#x02013;16 January 2009</conf-date><conf-name>The Seventh Asia Pacific Bioinformatics Conference (APBC 2009)</conf-name><conf-loc>Beijing, China</conf-loc></conference></article-meta></front><body><sec><title>Background</title><sec><title>Introduction</title><p>Array-based Comparative Genomic Hybridization (aCGH) is a powerful technique used to detect DNA copy number variations (CNV) across the genome. One of the most important aims of this technique is diagnosis, which can be achieved with help of classification of aCGH data.</p><p>One of the most important problems with the classification of aCGH data is dealing with a great number of attributes, which often exceed the number of given samples. In a typical experiment one can deal with dozens of samples, while microarray may consist of millions of spots. It is a real challenge to select from the huge amount of data the most interesting features, while most of them are not related to the given classification problem.</p><p>In reference to this issue, reduction of the dimensionality of data by applying feature elimination algorithms has been proposed in the previous works [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B2">2</xref>]. One of the most interesting approaches, which is based on interval merging, has been presented in [<xref ref-type="bibr" rid="B1">1</xref>]. Unfortunately, the solution described by the authors contains some fallacies, which result from an improper mix of training and test data in their algorithm. In the Methods section, we revise interval merging approach and we propose a new corrected version.</p><p>The other problem of processing aCGH data is the unbalance in class distribution. For instance, it is quite common that the number of samples from one class exceeds by several times the number of samples from the other class. This makes classification much harder, and most of algorithms assume an uniform class distribution.</p><p>Moreover, very often, one specified class is much more important than others. Let's suppose, we want to distinguish two groups of individuals: healthy and unhealthy. We will concentrate more on unhealthy class. The mistake made by classifying the unhealthy as healthy (False Negative) is more costly than an opposite mistake (False Positive). It implies the necessity of being careful in analysing and comparing the quality of classification models.</p><p>On the other hand, it is common in practice, that researchers present the results of classification performance by showing only the accuracy [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B3">3</xref>]. Obviously, it is an insufficient measure, when unbalanced data are given. This problem is discussed more deeply in the Results and Discussion section. Obviously, the wrong assumption about evaluation measures leads to incorrect conclusions about classifiers performance. In this paper, we show that the classification methods considered most successful for high-dimensional data extremely decrease their performance dramatically when the samples distribution is out of balance. Therefore we decided to develop a new classification method, which is based on the concept of Jumping Emerging Patterns (JEP). The difficulties of algorithmic design, such as computational complexity and limitations, are explained in detail in the Methods section.</p><p>Thanks to very high discriminative power of JEP's, our limJEPClassifier could be more appropriate in classifying aCGH data than other methods. In order to compare our new algorithm to SVM [<xref ref-type="bibr" rid="B4">4</xref>], we prepare an experimental pipeline, precisely described in the section Methods. Finally, in the Results and Discussion section, we present the results of this comparison.</p><p>In the following section, we present details of aCGH technology.</p></sec><sec><title>aCGH technology</title><p>Comparative Genome Hybridization (CGH) is a technique which allows for detection of segmental DNA copy number changes (CNC's) [<xref ref-type="bibr" rid="B5">5</xref>-<xref ref-type="bibr" rid="B7">7</xref>]. Recently, CGH has been widely used in many medical applications. In particular, it helps in the diagnosis of cancer [<xref ref-type="bibr" rid="B8">8</xref>] or genomic disorders [<xref ref-type="bibr" rid="B9">9</xref>], improves our knowledge about genes responsible for diseases and advances studies on personal genomic differences between humans [<xref ref-type="bibr" rid="B10">10</xref>].</p><p>In array-based CGH (aCGH) experiment, two differentially labeled samples are co-hybridized to targets, where the copy number between the two samples is reflected by their signal intensity ratios. In Figure <xref ref-type="fig" rid="F1">1</xref> we present essential steps in this technique.</p><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold>Workflow of aCGH experiment</bold>. Representation of aCGH technique [<xref ref-type="bibr" rid="B30">30</xref>].</p></caption><graphic xlink:href="1471-2105-10-S1-S64-1"/></fig><p>Through the years, the technology was upgraded to obtain a higher resolution by reducing length of DNA targets (probes). Nowadays, some aCGH platforms (e.g. oligonucleotide aCGH) support arrays with more than 1,000,000 probes.</p><p>Another kind of technique is SNP arrays (single nucleotide polymorphism), which differs from aCGH technology by its higher resolution. It allows to detect very small DNA changes.</p><p>Because of the huge amount of data, which are processed in aCGH experiments and some imperfections in technology, it becomes necessary to use the wide range of informatics and statistical tools at each stage of the analysis.</p><p>The first algorithmic challenge appears already in the preparation step. The issue is how to choose suitable probes, which will not cross-hybridize with other targets in the array. In this context several approaches have been proposed [<xref ref-type="bibr" rid="B11">11</xref>,<xref ref-type="bibr" rid="B12">12</xref>].</p><p>The next problem is related to the post-processing of signal data obtained from microarray. Because of many circumstances, which may affect results, it is essential to perform a normalization procedure, described in [<xref ref-type="bibr" rid="B13">13</xref>].</p><p>To obtain more clear results, smoothing, segmentation and aberration calling are usually used [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B14">14</xref>-<xref ref-type="bibr" rid="B16">16</xref>]. Nowadays, it is the most investigated area of aCGH analysis.</p><p>As we mentioned above, aCGH data is used in a diagnostic process. At this point a classification method has to be applied. In several papers, it has been suggested that the most accurate is a Support Vector Machine (SVM) [<xref ref-type="bibr" rid="B4">4</xref>]. In the next sections, we will evaluate this statement, by comparing SVM to our new classifier based on Jumping Emerging Patterns.</p></sec><sec><title>Related works</title><p>In recent works, we have found a lot of information about processing aCGH data, such as smoothing and clustering [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B14">14</xref>-<xref ref-type="bibr" rid="B16">16</xref>]. However, there are many fewer papers, which deal with a classification problem.</p><p>In [<xref ref-type="bibr" rid="B17">17</xref>] and [<xref ref-type="bibr" rid="B18">18</xref>] the classifications of combined data (aCGH and gene expression or steorological data) are considered.</p><p>Another work [<xref ref-type="bibr" rid="B3">3</xref>] investigates the tumor classification based on DNA copy number aberration determined using SNP arrays. In that paper three classification methods: Naive Bayes, K-nn, and SVM have been tested with a varying number of features. Although the K-nn achieved the best leave-one-out cross-validation accuracy, the performances of other methods were comparable. The second conclusion of this work was that the best performance of classifiers was achieved when 5&#x02013;30 features were selected. This fact reveals the importance of feature reduction algorithm in the classification of high-dimensional data. A new feature elimination methods for cancer classification using aCGH data have been proposed in the article [<xref ref-type="bibr" rid="B1">1</xref>]. Authors introduce a feature reduction algorithm based on an Interval Tree. In order to prove its efficiency, they compare the performance of a SVM classifier on two types of data: 1) raw log2ratio data; 2) data processed by their feature reduction algorithm. The results show that their approach led to a significantly better classification. Although the feature reduction procedure described by authors seems to be very useful, we have to point out some fallacies that we have found in their argumentation.</p><p>The first problem concerns the improper use of cross-validation. The cross-validation assumes that at the beginning of each run, data should be split into training and test data sets, and then all the operations before testing step should be done on both sets separately. However, in the system presented in [<xref ref-type="bibr" rid="B1">1</xref>], the feature reduction based on Interval Tree was performed on training and testing data together. In other words, the knowledge about test samples is utilized indirectly to train the model, which is clearly improper. The second major drawback of experimental design is that the authors used only one measure to evaluate a classification performance &#x02013; the accuracy. We assume that it is insufficient and does not show a real classification performance, since one of the tested data sets (TP53) [<xref ref-type="bibr" rid="B19">19</xref>], is strongly unbalanced. In our paper, we investigate the same TP53 data set with revised experimental pipeline and more adequate evaluation measures. Beside testing the SVM, we also check our new method &#x02013; limJEPclassifier.</p></sec></sec><sec sec-type="methods"><title>Methods</title><sec><title>Jumping Emerging Patterns</title><p>The classifiers based on Emerging Patterns (EP) or Jumping Emerging Patterns (JEP), have been considered one of the most successful classification systems [<xref ref-type="bibr" rid="B20">20</xref>]. However, it has hardly ever been tested with high-throughput data, because of the high computational complexity of EP. In this paper, we show how to overcome this issue by using "Limited JEP's".</p><p>First of all we present the definition of Jumping Emerging Patterns.</p><p><bold>Definition 1</bold>. Given two data sets D1, D2 we define a Jumping Emerging Pattern from <italic>D</italic>1 to <italic>D</italic>2 as an itemset <italic>X </italic>(an attribute value pairs), for which <italic>supp</italic><sub><italic>D</italic>1</sub>(<italic>X</italic>) &#x02260; 0 and <italic>supp</italic><sub><italic>D</italic>2</sub>(<italic>X</italic>) = 0.</p><p>We will denote each JEP from class P to class N by attribute-value pairs.</p><p><bold>Definition 2</bold>. The minimal JEP from P to N, which contains pattern (itemset) <italic>X</italic>, is such a JEP for which, does not exist a JEP, which contains pattern <italic>Y </italic>and <italic>Y </italic>&#x02282; <italic>X</italic>. In other words, JEP is minimal when all the patterns contained in its itemset do not form any other JEP.</p><p><bold>Example 1</bold>. Consider a decision table &#x02013; Table <xref ref-type="table" rid="T1">1</xref>. We have given two data sets &#x02013; classes P and N. By the definition patterns: {(a2, 1)}; {(a3, 1), (a4, 1)}; {(a2, 1), (a3, 1), (a4, 1)} are examples of JEP's, from class P to N. However the JEP {(a2, 1), (a3, 1), (a4, 1)} is not minimal, because it contains itemsets {(a2, 1)}, which form other JEP.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Example of decision table</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">nr</td><td align="center">a1</td><td align="center">a2</td><td align="center">a3</td><td align="center">a4</td><td align="center">Class</td></tr></thead><tbody><tr><td align="center">1</td><td align="center">0</td><td align="center">0</td><td align="center">0</td><td align="center">0</td><td align="center">P</td></tr><tr><td align="center">2</td><td align="center">1</td><td align="center">0</td><td align="center">0</td><td align="center">0</td><td align="center">P</td></tr><tr><td align="center">3</td><td align="center">0</td><td align="center">1</td><td align="center">1</td><td align="center">0</td><td align="center">P</td></tr><tr><td align="center">4</td><td align="center">0</td><td align="center">1</td><td align="center">1</td><td align="center">1</td><td align="center">P</td></tr><tr><td align="center">5</td><td align="center">1</td><td align="center">0</td><td align="center">0</td><td align="center">0</td><td align="center">N</td></tr><tr><td align="center">6</td><td align="center">1</td><td align="center">0</td><td align="center">0</td><td align="center">0</td><td align="center">N</td></tr><tr><td align="center">7</td><td align="center">0</td><td align="center">0</td><td align="center">1</td><td align="center">0</td><td align="center">N</td></tr><tr><td align="center">8</td><td align="center">0</td><td align="center">0</td><td align="center">0</td><td align="center">1</td><td align="center">N</td></tr></tbody></table></table-wrap><p>In this paper we consider only minimal JEP's.</p></sec><sec><title>Limited JEP's</title><p><bold>Definition 3</bold>. Limited JEP's at the level K are a set of minimal JEP's, for which the number of attributes in each itemset equals K. We denote limited JEP's at level K by <italic>limJEP</italic><sub><italic>K</italic></sub>.</p><p><bold>Example 2</bold>. According to the table <xref ref-type="table" rid="T1">1</xref>, there are:</p><p>&#x02022; one <italic>limJEP</italic><sub>1 </sub>from P to N - {(a2, 1)};</p><p>&#x02022; one <italic>limJEP</italic><sub>2 </sub>from P to N - {(a3, 1), (a4, 1)} and three <italic>limJEP</italic><sub>2 </sub>from N to P - {(a3, 0), (a4, 1)}, {(a2, 0), (a4, 1)}, {(a2, 0), (a3, 1)};</p><p>&#x02022; no <italic>limJEP</italic><sub>3 </sub>and <italic>limJEP</italic><sub>4</sub>.</p><p>The major problem with classifiers based on JEP's is their high complexity, which increases rapidly with the number of attributes. In [<xref ref-type="bibr" rid="B21">21</xref>] the authors show that the emerging pattern problem is MAX-SNP hard. In order to overcome this issue several algorithms were developed, which significantly reduce the computation time: JEPproducer [<xref ref-type="bibr" rid="B22">22</xref>], CP-tree [<xref ref-type="bibr" rid="B23">23</xref>], FP-tree [<xref ref-type="bibr" rid="B24">24</xref>,<xref ref-type="bibr" rid="B25">25</xref>], classifier based on local projected JEP [<xref ref-type="bibr" rid="B26">26</xref>]. However, even with those methods it is still not possible to search all JEP's when the number of features exceeds few dozens.</p><p>In this paper, we claim, that in the case of high-throughput data, we can build a classifier based on limited JEP's for lower <italic>limJEP </italic>levels only, instead of computing all the patterns. In the following sections, we will show that this solution leads us to the construction of an efficient, and successful classifier. Below, we present how the discriminative power of JEP's may vary among levels of limited JEP's.</p></sec><sec><title>REAL and UNREAL JEP's</title><p>We introduce the concept of REAL and UNREAL JEP's to illustrate the quality of patterns at different <italic>limJEP </italic>levels.</p><p><bold>Definition 4</bold>. Let <italic>U </italic>denote a decision table of itemsets, where two classes (<italic>P </italic>and <italic>N</italic>) are given. From table <italic>U </italic>we select a subset of samples denoted by <italic>A</italic>. We say that the pattern <italic>p </italic>is a REAL JEP in decision table <italic>A </italic>&#x021d4; if the <italic>p </italic>is still a JEP in table <italic>U</italic>. Otherwise <italic>p </italic>is UNREAL. We will denote all JEP's (REAL &#x0222a; UNREAL) as ALL.</p><p><bold>Example 3</bold>. Consider Table <xref ref-type="table" rid="T1">1</xref> as <italic>U</italic>. Suppose we select from <italic>U </italic>rows 3,4,5 and 6. We denote the new table by <italic>U'</italic>. Consider two patterns: <italic>p</italic>1 = {(<italic>a</italic>1, 0)} and <italic>p</italic>2 = {(<italic>a</italic>2, 1)}. Both patterns are JEP's in <italic>U'</italic>, however only <italic>p</italic>2 is still a JEP in <italic>U</italic>. In reference to definition 4, <italic>p</italic>1 is UNREAL JEP, and <italic>p</italic>2 is <italic>REAL </italic>JEP in <italic>U'</italic>.</p><p>In our study ratios of REAL to ALL JEP's are used, to measure the quality of <italic>limJEP's </italic>at the given <italic>limJEP </italic>level.</p><p><bold>Proposition 1</bold>. For a given decision table <italic>U </italic>with the number of rows |<italic>U|</italic>, let <italic>U' </italic>be a training data set derived from <italic>U</italic>. Similarly, we denote <italic>U" </italic>as a set of rows selected from <italic>U'</italic>. We assume, that |<italic>U'</italic>| - |<italic>U"</italic>| &#x0226a; |<italic>U"</italic>|. It means that a great majority of rows was selected. Then, if we compute JEP's for tables <italic>U</italic>, <italic>U' </italic>and <italic>U"</italic>, we can easily determine the number of REAL JEP's in <italic>U" </italic>with respect to <italic>U' </italic>and the number of REAL JEP's in <italic>U' </italic>in respect to <italic>U </italic>for each of the <italic>limJEP </italic>level. We put it that for a given <italic>limJEP </italic>level the ratio REAL/ALL JEP's is almost constant among tables <italic>U" </italic>and <italic>U"</italic>.</p><p>The proposition 1 allows us to infer from the training data about the quality of JEP's at a given <italic>limJEP </italic>level without a knowledge about test samples. Note, that in order to obtain the quality of investigated decision table <italic>U</italic>, we need to use only the training data (<italic>U'</italic>) and the table <italic>U" </italic>which is derived as a subset of <italic>U'</italic>.</p><p>We tried to confirm proposition 1 by testing TP-53 data set, described in the next section. Because of the high computational complexity, we used a decision table with the number of features reduced to 50. Data set was processed 10 times, as follows:</p><p>1. From original data set (<italic>U</italic>) we select randomly 80% of rows as training data (<italic>U'</italic>).</p><p>2. From <italic>U' </italic>we select 90% of rows, and denote it by <italic>U"</italic>.</p><p>3. We compute all JEP's at <italic>limJEP </italic>levels 1, 2 and 3 for tables <italic>U</italic>, <italic>U' </italic>and <italic>U"</italic>.</p><p>4. We calculate and compare REAL/ALL JEP's ratios in <italic>U' </italic>and <italic>U" </italic>at each level.</p><p>In Table <xref ref-type="table" rid="T2">2</xref> we present three samples of REAL/ALL ratios at each <italic>limJEP </italic>level and mean ratios from whole experiment (10 runs).</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>REAL/ALL ratios Comparison of REAL/ALL ratios at different <italic>limJEP </italic>levels between tables <italic>U' </italic>and <italic>U'</italic>.</p></caption><table frame="hsides" rules="groups"><thead><tr><td></td><td align="center" colspan="3">REAL/ALL JEP's</td></tr></thead><tbody><tr><td></td><td align="center">limJEP1</td><td align="center">limJEP2</td><td align="center">limJEP3</td></tr><tr><td colspan="4"><hr></hr></td></tr><tr><td align="left">run 1-<italic>U'</italic></td><td align="center">0.97</td><td align="center">0.96</td><td align="center">0.65</td></tr><tr><td align="left">run 1-<italic>U"</italic></td><td align="center">1</td><td align="center">0.97</td><td align="center">0.67</td></tr><tr><td colspan="4"><hr></hr></td></tr><tr><td align="left">run 2-<italic>U'</italic></td><td align="center">1</td><td align="center">0.98</td><td align="center">0.63</td></tr><tr><td align="left">run 2-<italic>U"</italic></td><td align="center">1</td><td align="center">0.96</td><td align="center">0.68</td></tr><tr><td colspan="4"><hr></hr></td></tr><tr><td align="left">run 3-<italic>U'</italic></td><td align="center">0.97</td><td align="center">0.96</td><td align="center">0.65</td></tr><tr><td align="left">run 3-<italic>U"</italic></td><td align="center">1</td><td align="center">0.97</td><td align="center">0.67</td></tr><tr><td colspan="4"><hr></hr></td></tr><tr><td align="left">mean-<italic>U'</italic></td><td align="center">0.97</td><td align="center">0.9</td><td align="center">0.53</td></tr><tr><td align="left">mean-<italic>U"</italic></td><td align="center">0.98</td><td align="center">0.85</td><td align="center">0.5</td></tr></tbody></table></table-wrap><p>As we can see, REAL/ALL ratios behave similarly for <italic>U' </italic>and <italic>U"</italic>. For a fixed <italic>limJEP </italic>level the differences in ratio between <italic>U' </italic>and <italic>U" </italic>at each run are relatively low. What is more, all the rows preserve the same, descending order of ratios. The best score is observed for <italic>limJEP</italic><sub>1</sub>, next for <italic>limJEP</italic><sub>2 </sub>and <italic>limJEP</italic><sub>3</sub>. We have mentioned above that proposition 1 gives us the possibility to predict a quality of JEP's at given <italic>limJEP </italic>level. It seems reasonable, that in a similar way, we can investigate other features of our data set, such as the distribution of REAL JEP's in classes. This could be valuable information, especially when we build a classifier, which works with unbalanced data.</p></sec><sec><title>Hierarchical strategy of classification</title><p>The fundamental application of JEP's is a classification. During the training step a classification model is built, which means that JEP's contained in training data are computed. In order to classify new data, the test sample is compared with all JEP's from the model. If the given JEP matches the sample, the algorithm increments the support for a class, which is associated with this JEP. Finally, the class with the greatest support is selected as a result.</p><p><bold>Example 4</bold>. Suppose that the Table <xref ref-type="table" rid="T1">1</xref> is the given training data set. In this case, the derived model will contains two minimal JEP's from class P to N: {(a2, 1)}, {(a3, 1), (a4, 1)}; and three from N to P: {(a2, 0), (a3, 1)}, {(a2, 0), (a4, 1)}, {(a3, 0), (a4, 1)}. Consider the test instance {1, 1, 1, 1}. The classifier searches for all JEP's in the model which match this instance. In particular example there are two such JEP's from class P to N and no JEP's from class N to P. Based on this knowledge, the algorithm can easily decide that test sample should be classified as class P.</p><p>The strategies of classification with JEP's usually take into consideration the whole population of found JEP's [<xref ref-type="bibr" rid="B20">20</xref>]. Several algorithms filter or weight JEP's in reference to their support in the training data set [<xref ref-type="bibr" rid="B23">23</xref>].</p><p>Unfortunately, these approaches did not prove useful in the case of aCGH data. The accuracy of classification performed by traditional JEP techniques was revealed to be lower than the accuracy of other classifiers such as SVM. What is more, these methods are computational intensive when we deal with high-throughput data.</p><p>These facts motivated us to look for a more accurate strategy of JEP's classification. In this paper, we propose a new algorithm of classification with Jumping Emerging Patterns named limJEPClassifier which is based on the knowledge of variability of discrimination power at different <italic>limJEP </italic>levels. To the best of our knowledge, up to now there was no strategy that refers to <italic>limJEP </italic>levels. The steps of algorithm are presented below.</p><p>For a given model, computed sets of <italic>limJEP's </italic>at levels 1, 2,..., <italic>m</italic>, where <italic>m </italic>&#x02264; number of attributes, limJEPClassifier processes a test sample as follows:</p><p>1. Sorts levels of limJEP's in reference to REAL/ALL ratio in the way explained in proposition 1.</p><p>2. Selects the best level and tries to classify the sample with JEP's from this level.</p><p>3. If success (support of one class dominates another one), returns the result; or else removes the last-considered level and goes back to point 2.</p><p>4. If no class is selected, chooses the most frequent class or randomizes it from appropriate distribution.</p><p>It is important that the REAL/ALL ratio is derived from the table <italic>U"</italic>, which is constructed based on the samples selected from a training data.</p><p>The other thing which is worth noticing, is that in our implementation of presented algorithm, we mainly use only the first two <italic>limJEP </italic>levels: <italic>limJEP</italic><sub>1 </sub>and <italic>limJEP</italic><sub>2</sub>. We found out that applying more levels does not improve the performance of classifier.</p><p>In the result section, we will show the advantages of our approach over other classification methods.</p></sec><sec><title>Classification pipeline</title><p>In order to test various classification models, we have developed a pipeline which includes preprocessing, model building and result analysis steps. The whole pipeline is presented in Figure <xref ref-type="fig" rid="F2">2</xref>. Below, we describe each step in detail.</p><fig position="float" id="F2"><label>Figure 2</label><caption><p><bold>Experimental pipeline</bold>. Experimental pipeline for testing classifiers.</p></caption><graphic xlink:href="1471-2105-10-S1-S64-2"/></fig><sec><title>Thresholding</title><p>In this step, we apply thresholding for all the data in order to determine normal, gain and loss regions. This procedure is done with the two values which indicate positive and negative thresholds, as follows:</p><p>&#x02022; If the value of spot is above the positive threshold, we mark it as a gain.</p><p>&#x02022; If the value of spot is below the negative threshold, we mark it as a loss.</p><p>&#x02022; In other cases, spot is marked as normal.</p><p>For each spot, we assign new values: 0 for normals, 1 for gains and -1 for losses.</p></sec><sec><title>Sampling training data</title><p>In order to perform cross-validation we sample training data from the table achieved in the previous step. The following operations are performed only on training data until testing stage.</p></sec><sec><title>Feature reduction</title><p>Feature elimination is divided into the following two steps:</p><p>&#x02022; Applying feature reduction algorithm based on interval tree (mentioned in the previous section).</p><p>&#x02022; Selecting the most valuable features obtained by Information Gain approach.</p><p>Below we present both algorithms in detail.</p></sec><sec><title>Merging intervals</title><p>The core idea of this algorithm is to compress the data by merging segments (continuous sequences of spots with equal values). Arisen intervals can be used as new features. In the procedure all continuous sets of columns with the same values in each row are retrieved and transformed into one single column.</p><p>It is clear that the derived-in-this-way attributes, contain more statistical information about distribution in classes than the previous set with separated columns. This statement was confirmed in the paper [<xref ref-type="bibr" rid="B1">1</xref>], in which it was shown that the usage of this procedure significantly improved the accuracy of classification. It is worth noticing that the presented algorithm was applied only for training data, unlike in article [<xref ref-type="bibr" rid="B1">1</xref>]. We claim that such an attitude is more proper in relation to classification problem, because we do not use any test data during the training step.</p><p>However, this approach involves some complicating circumstances. Note that each sample from the test data has to be adjusted into a structure of a table derived from the training data set. In order to do that, it is required to transform a test sample by combining the same sets of columns, which were merged in the training data table. The main problem is how to assign proper values for merged attributes. In the case when all the values are equal, it is trivial and we assign this value. Otherwise, we select heuristically the value which appears most frequently in the given interval. We claim that this problem should be more deeply investigated in future research.</p></sec><sec><title>Information gain</title><p>Although applying the merging interval algorithm significantly reduces the number of attributes, there are still a great number of columns left. We decided to use an Information Gain approach [<xref ref-type="bibr" rid="B27">27</xref>] to weight the importance of each feature and sort them in a descending order. The algorithm measures the number of bits of information obtained for category prediction by knowing the presence or absence of a feature. The Information Gain of feature F is defined as:</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" name="1471-2105-10-S1-S64-i1" overflow="scroll">                        <mml:semantics>                           <mml:mrow>                              <mml:mtable columnalign="right">                                 <mml:mtr columnalign="right">                                    <mml:mtd columnalign="right">                                       <mml:mrow>                                          <mml:mi>I</mml:mi>                                          <mml:mi>G</mml:mi>                                          <mml:mo stretchy="false">(</mml:mo>                                          <mml:mi>f</mml:mi>                                          <mml:mo stretchy="false">)</mml:mo>                                          <mml:mo>=</mml:mo>                                          <mml:mo>&#x02212;</mml:mo>                                          <mml:mstyle displaystyle="true">                                             <mml:munderover>                                                <mml:mo>&#x02211;</mml:mo>                                                <mml:mrow>                                                   <mml:mi>i</mml:mi>                                                   <mml:mo>=</mml:mo>                                                   <mml:mn>1</mml:mn>                                                </mml:mrow>                                                <mml:mi>m</mml:mi>                                             </mml:munderover>                                             <mml:mrow>                                                <mml:mi>P</mml:mi>                                                <mml:mo stretchy="false">(</mml:mo>                                                <mml:msub>                                                   <mml:mi>y</mml:mi>                                                   <mml:mi>i</mml:mi>                                                </mml:msub>                                                <mml:mo stretchy="false">)</mml:mo>                                                <mml:mi>log</mml:mi>                                                <mml:mo>&#x02061;</mml:mo>                                                <mml:mi>P</mml:mi>                                                <mml:mo stretchy="false">(</mml:mo>                                                <mml:msub>                                                   <mml:mi>y</mml:mi>                                                   <mml:mi>i</mml:mi>                                                </mml:msub>                                                <mml:mo stretchy="false">)</mml:mo>                                             </mml:mrow>                                          </mml:mstyle>                                       </mml:mrow>                                    </mml:mtd>                                 </mml:mtr>                                 <mml:mtr columnalign="right">                                    <mml:mtd columnalign="right">                                       <mml:mrow>                                          <mml:mo>+</mml:mo>                                          <mml:mstyle displaystyle="true">                                             <mml:munder>                                                <mml:mo>&#x02211;</mml:mo>                                                <mml:mrow>                                                   <mml:mi>v</mml:mi>                                                   <mml:mo>&#x02208;</mml:mo>                                                   <mml:mi>V</mml:mi>                                                </mml:mrow>                                             </mml:munder>                                             <mml:mrow>                                                <mml:mstyle displaystyle="true">                                                   <mml:munderover>                                                      <mml:mo>&#x02211;</mml:mo>                                                      <mml:mrow>                                                         <mml:mi>i</mml:mi>                                                         <mml:mo>=</mml:mo>                                                         <mml:mn>1</mml:mn>                                                      </mml:mrow>                                                      <mml:mi>m</mml:mi>                                                   </mml:munderover>                                                   <mml:mrow>                                                      <mml:mi>P</mml:mi>                                                      <mml:mo stretchy="false">(</mml:mo>                                                      <mml:mi>f</mml:mi>                                                      <mml:mo>=</mml:mo>                                                      <mml:mi>v</mml:mi>                                                      <mml:mo stretchy="false">)</mml:mo>                                                      <mml:mi>P</mml:mi>                                                      <mml:mo stretchy="false">(</mml:mo>                                                      <mml:msub>                                                         <mml:mi>y</mml:mi>                                                         <mml:mi>i</mml:mi>                                                      </mml:msub>                                                      <mml:mo>|</mml:mo>                                                      <mml:mi>f</mml:mi>                                                      <mml:mo>=</mml:mo>                                                      <mml:mi>v</mml:mi>                                                      <mml:mo stretchy="false">)</mml:mo>                                                      <mml:mi>log</mml:mi>                                                      <mml:mo>&#x02061;</mml:mo>                                                      <mml:mi>P</mml:mi>                                                      <mml:mo stretchy="false">(</mml:mo>                                                      <mml:msub>                                                         <mml:mi>y</mml:mi>                                                         <mml:mi>i</mml:mi>                                                      </mml:msub>                                                      <mml:mo>|</mml:mo>                                                      <mml:mi>f</mml:mi>                                                      <mml:mo>=</mml:mo>                                                      <mml:mi>v</mml:mi>                                                      <mml:mo stretchy="false">)</mml:mo>                                                   </mml:mrow>                                                </mml:mstyle>                                                <mml:mo>,</mml:mo>                                             </mml:mrow>                                          </mml:mstyle>                                       </mml:mrow>                                    </mml:mtd>                                 </mml:mtr>                              </mml:mtable>                           </mml:mrow>                                                   </mml:semantics>                     </mml:math></disp-formula></p><p>where <italic>y</italic><sub><italic>i </italic></sub>: <italic>i </italic>= 1...<italic>m </italic>are the set of categories and <italic>V </italic>set of possible values of <italic>f</italic>. At the end, the fixed number of top-ranked features are selected in order to form a final decision table.</p></sec><sec><title>Classification</title><p>When we obtain a final decision table, we train on it two classification models SVM (Support Vector Machine) [<xref ref-type="bibr" rid="B4">4</xref>] and limJEPClassifier.</p><p>The first classifier, SVM, was selected because it is commonly used in the case of high dimensional data. What is more, there can be found many applications of SVM to microarray gene-expression data, which are quite similar to aCGH data.</p><p>The core idea of SVM is to construct a separating hyperplane in the space of n-dimensional data between two sets of vectors (samples from two classes). The hyperplane should maximize the margin, defined as a sum of distances from hyperplane to the closest positive and negative samples [<xref ref-type="bibr" rid="B1">1</xref>].</p><p>The second classification method is our new approach &#x02013; limJEPClassifier which was described in the previous section.</p></sec></sec><sec><title>Data source</title><p>In our study we used a commonly investigated data set &#x02013; TP 53, published by [<xref ref-type="bibr" rid="B19">19</xref>]. The data are freely available at [<xref ref-type="bibr" rid="B28">28</xref>]. The aCGH data come from BAC arrays hybridized with <italic>oral squamous cell carcinomas </italic>(SCCs). The data set contains 14 TP53 mutant samples (unhealthy subjects) and 61 wildtype samples (healthy subjects). Each sample in a data set is featured by 1975 clones (log2ratio values).</p></sec></sec><sec><title>Results and discussion</title><sec><title>Evaluation measures</title><p>A typical problem with aCGH data is an unbalanced class distribution. In the case of the considered TP53 data set, the proportion between classes is 14:61. Moreover, the weight of each class is different. The rare class of sick subjects (14 samples) is more significant than the large set of samples of a control group. Traditionally, the rare and more important class is marked as "positive", while prevalent class is called "negative".</p><p>When one tries to investigate a classification with unbalanced data, one has to be very careful in selecting the measure of classification performance. In particular, the <italic>accuracy </italic>alone, is insufficient, because it does not tell us about a performance of predicting the positive class.</p><p>The common approaches of deriving alternative evaluation measures are based on <italic>confusion matrix</italic>, where all tested samples are grouped in four categories: "True positives" (TP), "False positives" (FP), "True negatives" (TN) and "False negatives" (FN), with respect to the classification results.</p><p>The <italic>sensitivity </italic>and the <italic>specificity </italic>of classifier are defined as follows:</p><p>&#x02022; <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M2" name="1471-2105-10-S1-S64-i2" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula></p><p>&#x02022; <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M3" name="1471-2105-10-S1-S64-i3" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula></p><p>In order to obtain a single measure of classifier performance, the <italic>sensitivity </italic>and the <italic>specificity </italic>can be integrated into the geometrical mean:</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M4" name="1471-2105-10-S1-S64-i4" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:mi>G</mml:mi>                           <mml:mtext>-</mml:mtext>                           <mml:mi>m</mml:mi>                           <mml:mi>e</mml:mi>                           <mml:mi>a</mml:mi>                           <mml:mi>n</mml:mi>                           <mml:mo>=</mml:mo>                           <mml:msqrt>                              <mml:mrow>                                 <mml:mi>s</mml:mi>                                 <mml:mi>e</mml:mi>                                 <mml:mi>n</mml:mi>                                 <mml:mi>s</mml:mi>                                 <mml:mi>i</mml:mi>                                 <mml:mi>t</mml:mi>                                 <mml:mi>i</mml:mi>                                 <mml:mi>v</mml:mi>                                 <mml:mi>i</mml:mi>                                 <mml:mi>t</mml:mi>                                 <mml:mi>y</mml:mi>                                 <mml:mo>&#x000d7;</mml:mo>                                 <mml:mi>s</mml:mi>                                 <mml:mi>p</mml:mi>                                 <mml:mi>e</mml:mi>                                 <mml:mi>c</mml:mi>                                 <mml:mi>i</mml:mi>                                 <mml:mi>f</mml:mi>                                 <mml:mi>i</mml:mi>                                 <mml:mi>c</mml:mi>                                 <mml:mi>i</mml:mi>                                 <mml:mi>t</mml:mi>                                 <mml:mi>y</mml:mi>                              </mml:mrow>                           </mml:msqrt>                           <mml:mo>.</mml:mo>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p></sec><sec><title>Experimental setup</title><p>In this section, we present a comparison of two classification methods &#x02013; SVM and limJEPClassifier. Both algorithms were tested according to the pipeline and on the data set, described in Methods section. We decided to compare our method with SVM, because it is considered to be the best algorithm used for classification of aCGH data and it has been commonly tested in other works. We did not compare our algorithm with other methods based on JEP's, because traditional strategies of JEP's classification are slow and ineffective.</p><p>LimJEPClassifier, as well as the whole experimental pipeline were implemented in the R language [<xref ref-type="bibr" rid="B29">29</xref>]. In order to process log2ratio data, we used the freely available "aCGH" package, designed to deal with aCGH data. Another popular R package &#x02013; "e-1071" was used to perform tests with SVM classifier.</p><p>Classifiers were tested with cross-validation where the training to test data ratio was 80%. In order to investigate the influence of the thresholding procedure on the classification results we performed the two runs of experiment with different pairs of positive and negative threshold values: (0.25, -0.25) and (0.5, -0.5). For each pair of threshold levels the cross-validation was repeated 10 times with various numbers of selected features (10, 20,..., 100). Note that feature selection, based on both interval merging and information gain approach, were applied under cross-validation.</p></sec><sec><title>limJEPClassifier vs SVM</title><p>In Figures <xref ref-type="fig" rid="F3">3</xref>, <xref ref-type="fig" rid="F4">4</xref> and <xref ref-type="fig" rid="F5">5</xref> we present the comparison of classification performance of SVM and limJEPClassifier. In each figure the different measure of performance is reported. The (a) and (b) versions of plots correspond to two pairs of threshold levels applied in the experiment.</p><fig position="float" id="F3"><label>Figure 3</label><caption><p><bold>Accuracy of limJEPClassifier vs SVM</bold>. Comparison of classification accuracy of SVM and limJEPClassifier for two threshold levels: (a) 0.25 and -0.25; (b) 0.5 and -0.5.</p></caption><graphic xlink:href="1471-2105-10-S1-S64-3"/></fig><fig position="float" id="F4"><label>Figure 4</label><caption><p><bold>Sensitivity of limJEPClassifier vs SVM</bold>. Comparison of classification sensitivity of SVM and limJEPClassifier for two threshold levels: (a) 0.25 and -0.25; (b) 0.5 and -0.5.</p></caption><graphic xlink:href="1471-2105-10-S1-S64-4"/></fig><fig position="float" id="F5"><label>Figure 5</label><caption><p><bold>G-mean of limJEPClassifier vs SVM</bold>. Comparison of G-mean measure of SVM and limJEPClassifier for two threshold levels: (a) 0.25 and -0.25; (b) 0.5 and -0.5.</p></caption><graphic xlink:href="1471-2105-10-S1-S64-5"/></fig><p>Figures <xref ref-type="fig" rid="F3">3a</xref> and <xref ref-type="fig" rid="F3">3b</xref> show the accuracy of classifiers for various numbers of features. In both cases the values change between 0.74 and 0.85. For the threshold (0.25, -0.25) the differences between SVM and limJEPClassifier are negligible. In figure <xref ref-type="fig" rid="F3">3b</xref> we see that the accuracy is still similar, however, a little supremacy of SVM could be observed.</p><p>On the other hand, it is worth noticing that the number of features have a very low impact on accuracy that is not a proper measure of classification performance in the case of unbalanced data. The last observation seems to confirm this statement.</p><p>Figures &#x02013; <xref ref-type="fig" rid="F4">4a</xref> and <xref ref-type="fig" rid="F4">4b</xref>, show the comparison of sensitivity which is the performance of a prediction of the class of affected subjects. We can observe from the results that for both threshold levels, limJEPClassifier is much better than SVM. Furthermore, the sensitivity of SVM is below 0.5 which means that the classifier cannot really distinguish between positive and negative class. The other difference between two classification methods is the way how the sensitivity changes along the X axis. In case of limJEPClassifier the sensitivity is relatively low at the beginning and increases with the number of features, while for SVM we cannot observe such a tendency.</p><p>The last pair of plots (Figure <xref ref-type="fig" rid="F5">5a</xref> and <xref ref-type="fig" rid="F5">5b</xref>) present the <italic>G-mean </italic>value in which the <italic>sensitivity </italic>and <italic>specificity </italic>are included. Like in the previous case, the limJEPClassifier significantly predominated over SVM.</p></sec></sec><sec><title>Conclusion</title><p>It is clear that improving a classification of aCGH data can contribute to great progress in many medical applications. Unfortunately, because of the imperfections of this technology, low signal to noise ratio and rapidly increasing microarray resolutions, classification still remains a very difficult problem.</p><p>In this paper, we have investigated the problem of the classification  using DNA copy number data which are characterized by an extremely high dimensionality and unbalanced distribution. We have introduced the concept of limited JEP's and we have shown how they can be applied in aCGH data classification. What is more, we suggest that the performance of a given <italic>limJEP </italic>level can be measured by analysing the structure of the training data.</p><p>To confirm the performance of our approach we have developed an experimental pipeline and we have compared limJEPClassifier with SVM. Experiments have been performed using widely tested TP-53 data set [<xref ref-type="bibr" rid="B19">19</xref>]. Although the SVM is considered one of the most successful methods of classifying high-throughput data, limJEPClassifier has revealed a much better performance in predicting the class of sick subjects. The main advantage of limJEPClassifier over SVM is that it deals more effectively with unbalanced data. We have not confronted our results with the previous one, presented in article [<xref ref-type="bibr" rid="B1">1</xref>], because we claim they are incomparable. We put it, that both experimental setup and results presentation showed in [<xref ref-type="bibr" rid="B1">1</xref>] were done in an improper way.</p><p>In this research, we have found out that applying limited JEP's can be useful in aCGH data classification. However, this issue should be investigated more deeply in future studies. It would be interesting to test limJEPClassifier with other data sets. Furthermore, it would be valuable to check the influence of different segmentation methods on the performance of limJEPClassifier.</p></sec><sec><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec><sec><title>Authors' contributions</title><p>KW and TG designed all experiments and algorithm of limJEPClassifier. KW directed the study, guided the whole project and assisted the manuscript writing. TG implemented limJEPClassifier and testing pipeline and wrote the manuscript.</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>We would like to thank Pawe&#x00142; Terlecki, Pawe&#x00142; Stankiewicz and APBC reviewers for their valuable comments, which helped us improve the quality of the paper.</p><p>This article has been published as part of <italic>BMC Bioinformatics </italic>Volume 10 Supplement 1, 2009: Proceedings of The Seventh Asia Pacific Bioinformatics Conference (APBC) 2009. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10?issue=S1"/></p></sec></ack><ref-list><ref id="B1"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Girard</surname><given-names>L</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>Pollack</surname><given-names>JR</given-names></name><name><surname>Minna</surname><given-names>JD</given-names></name></person-group><article-title>An Interval Tree Based Feature Reduction Method For Cancer Classification Using High-Throughput DNA Copy Number Data</article-title><source>BIOCOMP</source><year>2007</year><fpage>248</fpage><lpage>255</lpage></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Willenbrock</surname><given-names>H</given-names></name><name><surname>Fridlyand</surname><given-names>J</given-names></name></person-group><article-title>A comparison study: applying segmentation to array CGH data for downstream analyses</article-title><source>Bioinformatics</source><year>2005</year><volume>15</volume><fpage>4084</fpage><lpage>4091</lpage><pub-id pub-id-type="pmid">16159913</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Makedon</surname><given-names>F</given-names></name><name><surname>Pearlman</surname><given-names>J</given-names></name></person-group><article-title>Tumor Classification basedon DNA copy number aberrations determined using SNP arrays</article-title><source>Oncology reports</source><year>2006</year><volume>15</volume><fpage>1057</fpage><lpage>9</lpage><comment>[PMID: 16525700]</comment><pub-id pub-id-type="pmid">16525700</pub-id></citation></ref><ref id="B4"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>VN</given-names></name></person-group><source>Statistical Learning Theory</source><year>1998</year><publisher-name>Wiley-Interscience</publisher-name></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pinkel</surname><given-names>D</given-names></name><name><surname>Segraves</surname><given-names>R</given-names></name><name><surname>Sudar</surname><given-names>D</given-names></name><name><surname>Clark</surname><given-names>S</given-names></name><name><surname>Poole</surname><given-names>I</given-names></name><name><surname>Kowbel</surname><given-names>D</given-names></name><name><surname>Collins</surname><given-names>C</given-names></name><name><surname>Kuo</surname><given-names>WL</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Zhai</surname><given-names>Y</given-names></name><name><surname>Dairkee</surname><given-names>SH</given-names></name><name><surname>Ljung</surname><given-names>BM</given-names></name><name><surname>Gray</surname><given-names>JW</given-names></name><name><surname>Albertson</surname><given-names>DG</given-names></name></person-group><article-title>High resolution analysis of DNA copy number variationusing comparative genomic hybridization to microarrays</article-title><source>Nature genetics</source><year>1998</year><volume>20</volume><fpage>207</fpage><lpage>11</lpage><comment>[PMID: 9771718]</comment><pub-id pub-id-type="pmid">9771718</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pollack</surname><given-names>JR</given-names></name><name><surname>Perou</surname><given-names>CM</given-names></name><name><surname>Alizadeh</surname><given-names>AA</given-names></name><name><surname>Eisen</surname><given-names>MB</given-names></name><name><surname>Pergamenschikov</surname><given-names>A</given-names></name><name><surname>Williams</surname><given-names>CF</given-names></name><name><surname>Jeffrey</surname><given-names>SS</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name></person-group><article-title>Genome-wide analysis of DNA copy-number changes using cDNA microarrays</article-title><source>Nature genetics</source><year>1999</year><volume>23</volume><fpage>41</fpage><lpage>6</lpage><comment>[PMID: 10471496]</comment><pub-id pub-id-type="pmid">10471496</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Snijders</surname><given-names>AM</given-names></name><name><surname>Nowak</surname><given-names>N</given-names></name><name><surname>Segraves</surname><given-names>R</given-names></name><name><surname>Blackwood</surname><given-names>S</given-names></name><name><surname>Brown</surname><given-names>N</given-names></name><name><surname>Conroy</surname><given-names>J</given-names></name><name><surname>Hamilton</surname><given-names>G</given-names></name><name><surname>Hindle</surname><given-names>AK</given-names></name><name><surname>Huey</surname><given-names>B</given-names></name><name><surname>Kimura</surname><given-names>K</given-names></name><name><surname>Law</surname><given-names>S</given-names></name><name><surname>Myambo</surname><given-names>K</given-names></name><name><surname>Palmer</surname><given-names>J</given-names></name><name><surname>Ylstra</surname><given-names>B</given-names></name><name><surname>Yue</surname><given-names>JP</given-names></name><name><surname>Gray</surname><given-names>JW</given-names></name><name><surname>Jain</surname><given-names>AN</given-names></name><name><surname>Pinkel</surname><given-names>D</given-names></name><name><surname>Albertson</surname><given-names>DG</given-names></name></person-group><article-title>Assembly of microarrays for genome-wide measurement of DNA copy number</article-title><source>Nature genetics</source><year>2001</year><volume>29</volume><fpage>263</fpage><lpage>4</lpage><comment>[PMID: 11687795]</comment><pub-id pub-id-type="pmid">11687795</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Leo</surname><given-names>C</given-names></name><name><surname>Feng</surname><given-names>B</given-names></name><name><surname>Cauwels</surname><given-names>C</given-names></name><name><surname>Aguirre</surname><given-names>AJ</given-names></name><name><surname>Kim</surname><given-names>M</given-names></name><name><surname>Protopopov</surname><given-names>A</given-names></name><name><surname>Chin</surname><given-names>L</given-names></name></person-group><article-title>High-resolution global profiling of genomic alterations with long oligonucleotide microarray</article-title><source>Cancer research</source><year>2004</year><volume>64</volume><fpage>4744</fpage><lpage>8</lpage><comment>[PMID: 15256441]</comment><pub-id pub-id-type="pmid">15256441</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shaw</surname><given-names>CJ</given-names></name><name><surname>Shaw</surname><given-names>CA</given-names></name><name><surname>Yu</surname><given-names>W</given-names></name><name><surname>Stankiewicz</surname><given-names>P</given-names></name><name><surname>White</surname><given-names>LD</given-names></name><name><surname>Beaudet</surname><given-names>AL</given-names></name><name><surname>Lupski</surname><given-names>JR</given-names></name></person-group><article-title>Comparative genomic hybridisation using a proximal 17p BAC/PAC array detects rearrangements responsible for four genomic disorders</article-title><source>J Med Genet</source><year>2004</year><volume>41</volume><fpage>113</fpage><lpage>119</lpage><pub-id pub-id-type="pmid">14757858</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Perry</surname><given-names>GH</given-names></name><name><surname>Ben-Dor</surname><given-names>A</given-names></name><name><surname>Tsalenko</surname><given-names>A</given-names></name><name><surname>Sampas</surname><given-names>N</given-names></name><name><surname>Rodriguez-Revenga</surname><given-names>L</given-names></name><name><surname>Tran</surname><given-names>CW</given-names></name><name><surname>Scheffer</surname><given-names>A</given-names></name><name><surname>Steinfeld</surname><given-names>I</given-names></name><name><surname>Tsang</surname><given-names>P</given-names></name><name><surname>Yamada</surname><given-names>NA</given-names></name><name><surname>Park</surname><given-names>HS</given-names></name><name><surname>Kim</surname><given-names>JI</given-names></name><name><surname>Seo</surname><given-names>JS</given-names></name><name><surname>Yakhini</surname><given-names>Z</given-names></name><name><surname>Laderman</surname><given-names>S</given-names></name><name><surname>Bruhn</surname><given-names>L</given-names></name><name><surname>Lee</surname><given-names>C</given-names></name></person-group><article-title>The fine-scale and complex architecture of human copy-number variation</article-title><source>American journal of human genetics</source><year>2008</year><volume>82</volume><fpage>685</fpage><lpage>95</lpage><comment>[PMID: 18304495]</comment><pub-id pub-id-type="pmid">18304495</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lipson</surname><given-names>D</given-names></name><name><surname>Yakhini</surname><given-names>Z</given-names></name><name><surname>Aumann</surname><given-names>Y</given-names></name></person-group><article-title>Optimization of probe coverage for high-resolution oligonucleotide aCGH</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><fpage>e77</fpage><lpage>83</lpage><pub-id pub-id-type="pmid">17237109</pub-id></citation></ref><ref id="B12"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Lipson</surname><given-names>D</given-names></name><name><surname>Webb</surname><given-names>P</given-names></name><name><surname>Yakhini</surname><given-names>Z</given-names></name></person-group><source>Designing Specific Oligonucleotide Probes for the Entire S cerevisiae Transcriptome</source><year>2002</year><fpage>491</fpage><lpage>505</lpage></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Staaf</surname><given-names>J</given-names></name><name><surname>Jonsson</surname><given-names>G</given-names></name><name><surname>Ringner</surname><given-names>M</given-names></name><name><surname>Vallon-Christersson</surname><given-names>J</given-names></name></person-group><article-title>Normalization of array-CGH data: influence of copy number imbalances</article-title><source>BMC Genomics</source><year>2007</year><volume>8</volume><fpage>382</fpage><pub-id pub-id-type="pmid">17953745</pub-id></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hupe</surname><given-names>P</given-names></name><name><surname>Stransky</surname><given-names>N</given-names></name><name><surname>Thiery</surname><given-names>JP</given-names></name><name><surname>Radvanyi</surname><given-names>F</given-names></name><name><surname>Barillot</surname><given-names>E</given-names></name></person-group><article-title>Analysis of array CGH data: from signal ratio to gain and loss of DNA regions</article-title><source>Bioinformatics</source><year>2004</year><volume>20</volume><fpage>3413</fpage><lpage>3422</lpage><pub-id pub-id-type="pmid">15381628</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Olshen</surname><given-names>AB</given-names></name><name><surname>Venkatraman</surname><given-names>ES</given-names></name><name><surname>Lucito</surname><given-names>R</given-names></name><name><surname>Wigler</surname><given-names>M</given-names></name></person-group><article-title>Circularbinary segmentation for the analysis of array-based DNA copy number data</article-title><source>Biostatistics (Oxford, England)</source><year>2004</year><volume>5</volume><fpage>557</fpage><lpage>72</lpage><comment>[PMID: 15475419]</comment><pub-id pub-id-type="pmid">15475419</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Fridlyand</surname><given-names>J</given-names></name><name><surname>Snijders</surname><given-names>AM</given-names></name><name><surname>Pinkel</surname><given-names>D</given-names></name><name><surname>Albertson</surname><given-names>DG</given-names></name><name><surname>Jain</surname><given-names>AN</given-names></name></person-group><article-title>Hidden Markov models approach to the analysis of array CGH data</article-title><source>J Multivar Anal</source><year>2004</year><volume>90</volume><fpage>132</fpage><lpage>153</lpage></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Riccadonna</surname><given-names>S</given-names></name><name><surname>Jurman</surname><given-names>G</given-names></name><name><surname>Merler</surname><given-names>S</given-names></name><name><surname>Paoli</surname><given-names>S</given-names></name><name><surname>Quattrone</surname><given-names>A</given-names></name><name><surname>Furlanello</surname><given-names>C</given-names></name></person-group><article-title>Supervised Classification of combined copy number and gene expression data</article-title><source>Journal of Integrative Bioinformatics</source><year>2007</year><volume>4</volume></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mattfeldt</surname><given-names>T</given-names></name><name><surname>Gottfried</surname><given-names>HW</given-names></name><name><surname>Wolter</surname><given-names>H</given-names></name><name><surname>Schmidt</surname><given-names>V</given-names></name><name><surname>Kestler</surname><given-names>HA</given-names></name><name><surname>Mayer</surname><given-names>J</given-names></name></person-group><article-title>Classification of Prostatic Carcinoma with Artificial Neural Networks Using Comparative Genomic Hybridization and Quantitative Stereological Data</article-title><source>Pathol Res Pract</source><year>2003</year><volume>199</volume><fpage>773</fpage><lpage>784</lpage><pub-id pub-id-type="pmid">14989489</pub-id></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Snijders</surname><given-names>AM</given-names></name><name><surname>Schmidt</surname><given-names>BL</given-names></name><name><surname>Fridlyand</surname><given-names>J</given-names></name><name><surname>Dekker</surname><given-names>N</given-names></name><name><surname>Pinkel</surname><given-names>D</given-names></name><name><surname>Jordan</surname><given-names>RCK</given-names></name><name><surname>Albertson</surname><given-names>DG</given-names></name></person-group><article-title>Rare amplicons implicate frequent deregulation of cell fate specification pathways in oral squamous cell carcinoma</article-title><source>Oncogene</source><year>2005</year><volume>24</volume><fpage>4232</fpage><lpage>42</lpage><comment>[PMID: 15824737]</comment><pub-id pub-id-type="pmid">15824737</pub-id></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>G</given-names></name><name><surname>Ramamohanarao</surname><given-names>K</given-names></name></person-group><article-title>Making Use of the Most Expressive Jumping Emerging Patterns for Classification</article-title><source>Knowledge and Information Systems</source><year>2001</year><volume>3</volume><fpage>131</fpage><lpage>145</lpage></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Zhao</surname><given-names>H</given-names></name><name><surname>Dong</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>J</given-names></name></person-group><article-title>On the complexity of finding emerging patterns</article-title><source>Theoretical Computer Science</source><year>2005</year><volume>335</volume><fpage>15</fpage><lpage>27</lpage></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Dong</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>J</given-names></name></person-group><article-title>Mining border descriptions of emerging patterns from dataset pairs</article-title><source>Knowledge and Information Systems</source><year>2005</year><volume>8</volume><fpage>178</fpage><lpage>202</lpage></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>H</given-names></name><name><surname>Ramamohanarao</surname><given-names>K</given-names></name></person-group><article-title>Fast discovery and the generalization of strong jumping emerging patterns for building compact and accurate classifiers</article-title><source>Knowledge and Data Engineering, IEEE Transactions on</source><year>2006</year><volume>18</volume><fpage>721</fpage><lpage>737</lpage></citation></ref><ref id="B24"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Han</surname><given-names>J</given-names></name><name><surname>Pei</surname><given-names>J</given-names></name><name><surname>Yin</surname><given-names>Y</given-names></name></person-group><article-title>Mining frequent patterns without candidate generation</article-title><source>Knowledge and Data Engineering, IEEE Transactions on</source><year>2000</year><publisher-name>ACM Press</publisher-name><fpage>1</fpage><lpage>12</lpage></citation></ref><ref id="B25"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>G</given-names></name><name><surname>Wong</surname><given-names>L</given-names></name></person-group><article-title>Mining statistically important equivalence classes and delta-discriminative emerging patterns</article-title><source>KDD '07: Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining</source><year>2007</year><publisher-name>New York, NY, USA: ACM</publisher-name><fpage>430</fpage><lpage>439</lpage></citation></ref><ref id="B26"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Terlecki</surname><given-names>P</given-names></name><name><surname>Walczak</surname><given-names>K</given-names></name></person-group><article-title>Local Projection in Jumping Emerging Patterns Discovery in Transaction Databases</article-title><source>PAKDD 2008</source><year>2008</year><volume>5012</volume><publisher-name>LNAI, Osaka, Japan</publisher-name><fpage>723</fpage><lpage>730</lpage></citation></ref><ref id="B27"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Mitchell</surname><given-names>TM</given-names></name></person-group><source>Machine Learning</source><year>1997</year><publisher-name>McGraw-Hill Higher Education</publisher-name></citation></ref><ref id="B28"><citation citation-type="other"><article-title>The TP53 aCGH data set</article-title><ext-link ext-link-type="uri" xlink:href="http://www.cbs.dtu.dk/~hanni/aCGH/"/></citation></ref><ref id="B29"><citation citation-type="book"><person-group person-group-type="author"><collab>R Development Core Team</collab></person-group><source>R: A Language and Environment for Statistical Computing</source><year>2006</year><publisher-name>R Foundation for Statistical Computing, Vienna, Austria</publisher-name><ext-link ext-link-type="uri" xlink:href="http://www.R-project.org"/><comment>ISBN 3-900051-07-0</comment></citation></ref><ref id="B30"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lockwood</surname><given-names>WW</given-names></name><name><surname>Chari</surname><given-names>R</given-names></name><name><surname>Chi</surname><given-names>B</given-names></name><name><surname>Lam</surname><given-names>WL</given-names></name></person-group><article-title>Recent advances in array comparative genomic hybridization technologies and their applications in human genetics</article-title><source>Eur J Hum Genet</source><year>2005</year><volume>14</volume><fpage>139</fpage><lpage>148</lpage><pub-id pub-id-type="pmid">16288307</pub-id></citation></ref></ref-list></back></article>