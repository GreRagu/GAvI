<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">J Biomed Discov Collab</journal-id><journal-title-group><journal-title>Journal of Biomedical Discovery and Collaboration</journal-title></journal-title-group><issn pub-type="epub">1747-5333</issn><publisher><publisher-name>University of Illinois at Chicago Library</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19126221</article-id><article-id pub-id-type="pmc">2631451</article-id><article-id pub-id-type="publisher-id">1747-5333-4-1</article-id><article-id pub-id-type="doi">10.1186/1747-5333-4-1</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Are figure legends sufficient? Evaluating the contribution of associated text to biomedical figure comprehension</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="A1"><name><surname>Yu</surname><given-names>Hong</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><email>hongyu@uwm.edu</email></contrib><contrib contrib-type="author" id="A2"><name><surname>Agarwal</surname><given-names>Shashank</given-names></name><xref ref-type="aff" rid="I3">3</xref><email>agarwal@uwm.edu</email></contrib><contrib contrib-type="author" id="A3"><name><surname>Johnston</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="I4">4</xref><email>johnsto@uwm.edu</email></contrib><contrib contrib-type="author" id="A4"><name><surname>Cohen</surname><given-names>Aaron</given-names></name><xref ref-type="aff" rid="I5">5</xref><email>cohenaa@ohsu.edu</email></contrib></contrib-group><aff id="I1"><label>1</label>Department of Health Sciences, University of Wisconsin-Milwaukee, P.O. Box 413, Milwaukee, WI 53201-0413, USA</aff><aff id="I2"><label>2</label>Department of Computer Science, College of Engineering, University of Wisconsin-Milwaukee, P.O. Box 413, Milwaukee, WI 53201-0413, USA</aff><aff id="I3"><label>3</label>Medical Informatics, College of Engineering, University of Wisconsin-Milwaukee, P.O. Box 413, Milwaukee, WI 53201-0413, USA</aff><aff id="I4"><label>4</label>Department of Occupation Therapy, College of Health Sciences, Oregon Health &#x00026; Science University, 3181 S.W. Sam Jackson Park Road, Portland, Oregon, USA 97239-3098, USA</aff><aff id="I5"><label>5</label>Department of Medical Informatics and Clinical Epidemiology, School of Medicine, Oregon Health &#x00026; Science University, 3181 S.W. Sam Jackson Park Road, Portland, Oregon, 97239-3098, USA</aff><pub-date pub-type="collection"><year>2009</year></pub-date><pub-date pub-type="epub"><day>6</day><month>1</month><year>2009</year></pub-date><volume>4</volume><fpage>1</fpage><lpage>1</lpage><history><date date-type="received"><day>24</day><month>9</month><year>2008</year></date><date date-type="accepted"><day>6</day><month>1</month><year>2009</year></date></history><permissions><copyright-statement>Copyright &#x000a9;2009 Yu et al; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2009</copyright-year><copyright-holder>Yu et al; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0">http://creativecommons.org/licenses/by/2.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="http://www.j-biomed-discovery.com/content/4/1/1"/><abstract><sec><title>Background</title><p>Biomedical scientists need to access figures to validate research facts and to formulate or to test novel research hypotheses. However, figures are difficult to comprehend without associated text (e.g., figure legend and other reference text). We are developing automated systems to extract the relevant explanatory information along with figures extracted from full text articles. Such systems could be very useful in improving figure retrieval and in reducing the workload of biomedical scientists, who otherwise have to retrieve and read the entire full-text journal article to determine which figures are relevant to their research. As a crucial step, we studied the importance of associated text in biomedical figure comprehension.</p></sec><sec><title>Methods</title><p>Twenty subjects evaluated three figure-text combinations: figure+legend, figure+legend+title+abstract, and figure+full-text. Using a Likert scale, each subject scored each figure+text according to the extent to which the subject thought he/she understood the meaning of the figure and the confidence in providing the assigned score. Additionally, each subject entered a free text summary for each figure-text. We identified missing information using indicator words present within the text summaries. Both the Likert scores and the missing information were statistically analyzed for differences among the figure-text types. We also evaluated the quality of text summaries with the text-summarization evaluation method the ROUGE score.</p></sec><sec><title>Results</title><p>Our results showed statistically significant differences in figure comprehension when varying levels of text were provided. When the full-text article is not available, presenting just the figure+legend left biomedical researchers lacking 39&#x02013;68% of the information about a figure as compared to having complete figure comprehension; adding the title and abstract improved the situation, but still left biomedical researchers missing 30% of the information. When the full-text article is available, figure comprehension increased to 86&#x02013;97%; this indicates that researchers felt that only 3&#x02013;14% of the necessary information for full figure comprehension was missing when full text was available to them. Clearly there is information in the abstract and in the full text that biomedical scientists deem important for understanding the figures that appear in full-text biomedical articles.</p></sec><sec><title>Conclusion</title><p>We conclude that the texts that appear in full-text biomedical articles are useful for understanding the meaning of a figure, and an effective figure-mining system needs to unlock the information beyond figure legend. Our work provides important guidance to the figure mining systems that extract information only from figure and figure legend.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>The biomedical literature is a rich resource of biomedical knowledge. The importance of developing valid information retrieval systems for biomedical scientists has motivated the development of domain-specific information systems and annotated databases worldwide. However, most information retrieval systems target only textual information and fail to provide access to other important data contained in journal articles such as image data, including figures. More than any other type of information, figures usually represent the evidence of discovery in the biomedical literature [<xref ref-type="bibr" rid="B1">1</xref>]. Full-text biomedical journal articles nearly always incorporate figures. For example, we found an average of five figures per biomedical article in the journal <italic>Proceedings of the National Academy of Sciences </italic>(PNAS) [<xref ref-type="bibr" rid="B1">1</xref>]. Biomedical scientists need to access figures to validate research facts and to formulate or test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain "false facts") [<xref ref-type="bibr" rid="B2">2</xref>]. Therefore, access to the experimental evidence in the form of figures is an important aspect of scientific communication and peer review.</p><p>Unfortunately, this wealth of information remains highly underutilized and inaccessible without automatic systems to mine these figures. A recent review article [<xref ref-type="bibr" rid="B3">3</xref>] describes emerging research interest in mining figures. Specifically, the subcellular location image finder (SLIF) system [<xref ref-type="bibr" rid="B4">4</xref>,<xref ref-type="bibr" rid="B5">5</xref>] extracts information from fluorescence microscopy images and aligns image panels to their corresponding sub-legend. Rafkind et al. [<xref ref-type="bibr" rid="B6">6</xref>] integrated text features in figure legend with image features for figure classification. Shatkay et al. [<xref ref-type="bibr" rid="B7">7</xref>] integrated image features with text to enhance document classification. BioText [<xref ref-type="bibr" rid="B8">8</xref>,<xref ref-type="bibr" rid="B9">9</xref>] indexes figure legends and returns figure+legend in response to a text query.</p><p>The work described above, however, explored only figure legends, and not other types of associated text. As described in [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B10">10</xref>,<xref ref-type="bibr" rid="B11">11</xref>], the figure legend is not the only type of text that describes figure content. The other types of associated text include the title, the abstract, and other kinds of text that appear in full-text articles. Although numerous studies have shown the importance of figure legend for literature-mining [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B6">6</xref>,<xref ref-type="bibr" rid="B8">8</xref>-<xref ref-type="bibr" rid="B14">14</xref>], the importance of other types of associated text has not yet been evaluated. The question we raise is that, are other associated texts important, and if so, to what extend, for understanding the meaning of figures? Is it necessary for a figure mining system to ignore other associated texts?</p><p>The questions we raised are of important value. Because biomedical full-text articles are highly structured, titles and figure legend are typically easy to detect. In contrast, detecting the associated texts that appear in the abstract and in the full-text body is a much harder task, and requires sophisticated natural language processing approaches (NLP) we have recently developed [<xref ref-type="bibr" rid="B1">1</xref>]. A recent study showed that seven of the eight biologists who used the BioText figure-legend retrieval system said that the BioText legend search was useful [<xref ref-type="bibr" rid="B8">8</xref>], note that the study didn't explore other associated texts. Many journals (e.g., Nature) have requested authors to make the figure legend comprehensive. If figure legend is sufficient for figure-mining tasks, then the need for sophisticated NLP techniques for automatically linking figures to other associated texts may be limited.</p><p>Therefore before designing automated systems to specifically augment figure understanding with full text, it is essential to determine the quantitative difference in figure understanding to be gained by providing access to the relevant portions of the full text article as compared to the simpler alternatives of legend, title, and abstract.</p><p>In this study, we identify what types of text are necessary for biomedical scientists to understand the meaning of a figure that appears in a full-text biomedical article. We have previously observed that associated texts in abstracts are better than other types of associated text, including figure legend, for summarizing figure content [<xref ref-type="bibr" rid="B10">10</xref>]. We hypothesize that in addition to legend, the texts that appear in full-text biomedical articles are useful for understanding the meaning of a figure. We have designed a randomized study to systematically evaluate whether texts other than figure legend are important aids to biomedical researchers in figure comprehension.</p></sec><sec sec-type="methods"><title>Methods</title><p>We designed a randomized trial protocol to test whether figure comprehension would increase with incremental levels of associated text. The evaluation protocol was approved by the University of Wisconsin-Milwaukee Institutional Review Board (IRB).</p><sec><title>Test conditions</title><p>Three types of figure-text combinations were compared: (1) figure+legend, (2) figure+legend+title+abstract, and (3) figure+full-text.</p></sec><sec><title>Subject</title><p>We recruited a total of 25 subjects by email and online advertisements (e.g., Nature Jobs). All subjects were bench biomedical scientists who were either PhD candidates or post-docs in the biomedical domain. The subjects' specialties include biological science, cell biology, ecology, genetics, pathology, physiology, plant, molecular biology, and structural biology. We consulted five subjects on methods for quantitatively measuring figure comprehension. The rest of the 20 subjects participated in the evaluation. The subjects self-reported that they frequently search literature resources for their research. Subjects were paid for their participation (~$10/hr).</p></sec><sec><title>Figure Selection</title><p>We selected evaluating figures from a representative biomedical literature collection, the TREC 2006 Genomics Track text collection, which was derived from 49 biomedical journals and includes a total of 162,259 full-text biomedical articles [<xref ref-type="bibr" rid="B15">15</xref>].</p><p>To select figures for evaluation, we randomized the order of the articles in the Genomics Track text collection, and picked the first figure of the correct type from the first article for each of the five defined figure types (i.e., gel image, graph, image-of-a-thing, model, and mixed type) [<xref ref-type="bibr" rid="B6">6</xref>]. No more than one figure was selected from a given full-text article. We continued this process in a round-robin fashion until we had selected a total of 25 figures from 25 full-text articles. These 25 full-text articles belonged to 9 journals. Our strategy for selecting articles and figures ensured that the articles and figures represented a randomized collection of biomedical literature and figures in genomics.</p></sec><sec><title>Measures</title><p>Previous work has shown that figure comprehension can be measured quantitatively [<xref ref-type="bibr" rid="B16">16</xref>]. By consulting with five biomedical scientists, we developed two measures for figure comprehension. First, we measured figure comprehension on a Likert-like scale (SCORE): we asked subjects to give a score between 1 and 10 (poorest and best, respectively) to indicate the extent she/he understood the figure's meaning (MEANING) when different figure-text combinations were provided, and also to give a score between 1 and 10 (poorest and best, respectively) to indicate their confidence in providing the assigned score (CONFIDENCE). There were no time constraints during the evaluation process.</p><p>Second, we measured figure comprehension by evaluating the text summaries (TEXT-SUMMARY) for each figure-text combination. We asked each subject to write a text summary to describe the content of the figure, and we then quantitatively analyzed the text summaries. Previous work has shown that biomedical text can be classified into different rhetorical units including <italic>Background</italic>, <italic>Methods</italic>, <italic>Results</italic>, and <italic>Conclusions and Indications </italic>[<xref ref-type="bibr" rid="B17">17</xref>,<xref ref-type="bibr" rid="B18">18</xref>]. We hypothesized that a text summary of a figure can also be structured by rhetorical units.</p><p>To test this hypothesis, we consulted five subjects about what types of information were important for representing figure content. We provided each subject with all the rhetorical units defined in other studies (e.g., [<xref ref-type="bibr" rid="B17">17</xref>,<xref ref-type="bibr" rid="B18">18</xref>]), and asked the subject to select the rhetorical units that can be used to structure the meaning of a figure. We also asked each subject to freely enter any other types of rhetorical units. All five subjects selected the following four rhetorical units: purpose of the study, methods, results, and conclusions and indications, and commented that each unit can be described with text. These four rhetorical units were then used for evaluating figure comprehension.</p><p>We implemented a web-based user interface to allow the 20 subjects to view a figure+text combination and to score it by MEANING and by CONFIDENCE. Additionally, subjects were told to enter free-text summaries on the basis of the four rhetorical units (purpose, methods, results, conclusions and indications). In addition to the rhetorical units, for each figure and figure-text combination we provided a text box for each subject to enter "other important criteria for understanding the figure content." An example of the free text provided by the subjects is shown in Table <xref ref-type="table" rid="T1">1</xref>. Again, we did not impose any time constraints to subjects who completed the evaluation study.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Sample data for rhetorical unit evaluation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Figure + Associated text</th><th align="left" colspan="5">Subject-Generated Text Structured by Rhetorical Units</th></tr><tr><th/><th colspan="5"><hr/></th></tr><tr><th/><th align="left">Purpose</th><th align="left">Methods</th><th align="left">Results</th><th align="left">Conclusions</th><th align="left">Other Criteria</th></tr></thead><tbody><tr><td align="left">Fig+L</td><td align="left">Effect of pH...</td><td align="left">Enzyme kinetics</td><td align="left">Hard to say, need detailed fitting data</td><td align="left">Don't know</td><td align="left">Full text</td></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left">Fig+L+T+A</td><td align="left">The goal of...</td><td align="left">X-ray crystallography...</td><td align="left">Hydrogen bonds...</td><td align="left">RNA binding of protein...</td><td align="left">Methods are still lacking</td></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left">F+Legend+T+A</td><td align="left">To show that...</td><td align="left">A is just a chart of...</td><td align="left">Affinity of TF...</td><td align="left">A model for the formation...</td><td align="left">How was the final model arrived at (part c)?</td></tr></tbody></table><table-wrap-foot><p>Space limitations prevent us from including all the text descriptions for the four rhetorical units. Legend (L), title (T), and abstract (A). The original data are available at <ext-link ext-link-type="uri" xlink:href="http://www.bioex.us.com/evaluation_data/JudgeEvaluation.xml">http://www.bioex.us.com/evaluation_data/JudgeEvaluation.xml</ext-link>.</p></table-wrap-foot></table-wrap></sec><sec><title>Procedures</title><p>Each subject of the 20 total was presented with five figures, one of each of the five figure types. Each figure was randomly selected from the five figures that belonged to a specific figure type. The order of the figure presentation was randomized. For each figure, the subject was shown first the figure+legend, then the figure+legend+title+abstract, and finally the figure plus the full-text article. At each level of figure text, the subject was asked to assign scores by MEANING and CONFIDENCE, and was requested to enter a summary text for each rhetorical unit and for other important criteria for comprehending figures. There was no time limit for a subject to complete the evaluation for any figure-text type.</p></sec><sec><title>SCORE Analysis</title><p>We analyzed the SCORE results using McNemar's test on paired samples, where the subject and figure were the same and only the amount of associated text differed. As is commonly done with Likert data, the SCORE samples were thresholded into two dichotomous groups. Since we are concerned with scientists having a high level of figure comprehension, scores less than 8 were considered to represent poor understanding of the figure on the part of the subject. A score of 8 or greater represented good understanding of the figure on the part of the subject. By thresholding the data into two groups, distinctions between strong understanding and a lesser level of understanding could be statistically analyzed in a simple and robust manner.</p></sec><sec><title>MISSING INFORMATION Analysis</title><p>We applied a straightforward approach to quantitatively evaluate the quality of text summaries of figures. We counted, with each figure-text, whether a text summary indicated that information was missing. A text summary was counted as containing missing information if the text incorporated cue text such as "don't know," "don't understand," "no clue," "lacking," "missing," "hard to say," "requires more," etc. Since each text summary is structured into the four rhetorical units, we counted whether information was missing in each of the rhetorical units separately. For example, Table <xref ref-type="table" rid="T1">1</xref> shows that for the figure+legend, the results, and the conclusions and indications were missing. For the two data samples shown for the figure+legend+title+abstract combination, the methods are missing in the first, and both the results, and the conclusions and indications are missing for the second.</p><p>We generated a coding guideline based on the cue text. A rhetorical unit was judged as complete (not missing any required information) if none of the cue text was present. We then asked two biomedical scientists (PhD in biology) who were not among the 20 subjects to serve as judges. Following the coding guidelines, the two judges independently identified whether there was information missing from the text summary of each of the subject/figure/text/rhetorical-unit combinations. The judges provided a dichotomous rating: missing or complete (non-missing). We blinded the judges to the figure-text combination information, the figure type information, and the information about which subjects wrote the summaries. The data were randomized for order presentation to each judge. We measured the inter-rater agreement between the two judges by the measures of overall agreement and Cohen's kappa [<xref ref-type="bibr" rid="B19">19</xref>]. As with the SCORE data, the relationships between test conditions and dichotomized comprehension scores were tested using McNemar's test on paired samples, where the subject and figure were the same and only the amount of associated text differed.</p></sec><sec><title>ROUGE Score Analysis</title><p>Another approach to evaluating the quality of text summaries is to apply the methods developed for summarization evaluation tasks [<xref ref-type="bibr" rid="B20">20</xref>]. Ideally, summaries should be assessed either by human judgments on their quality and utility [<xref ref-type="bibr" rid="B21">21</xref>] or by a task-based setting to determine their usefulness as part of an information browsing and access interface (extrinsic evaluation) [<xref ref-type="bibr" rid="B22">22</xref>,<xref ref-type="bibr" rid="B23">23</xref>]. However, such evaluations are time-consuming, expensive, and require a considerable amount of time and careful planning.</p><p>Automatic summarization methods have been developed to reduce the bottleneck of human intervention. <italic>Recall-oriented Understudy for Gisting Evaluation </italic>(ROUGE) is the most frequently used automated summary evaluation package, and has been used in Document Understanding Conference (DUC) [<xref ref-type="bibr" rid="B24">24</xref>]. A summary is evaluated by comparing it with a human-generated gold standard based on the computation of n-gram overlap between the summary and the gold standard. An n-gram is a subsequence of n items (words) from a given text span. An n-gram of size 1 is a "unigram" (1-gram); one of size 2 is a "bigram" (2-gram), etc.</p><p>A high-quality gold standard is crucial to the ROUGE evaluation. In our study, biomedical scientists generated all the text summaries. However, for each figure, the text summaries were generated with different figure-text combinations. Since the full text provides the subjects with the most complete descriptions of figures, it is reasonable to assume that the full-text-figure combination is the closest (among the three) to the gold standard. We therefore applied the text summaries generated by use of the figure+full-text as the gold standard, and obtained the ROUGE scores by comparing them to other text summaries generated by using other figure-text combinations. We evaluated the text summaries by rhetorical unit. In addition, we aggregated the four rhetorical-unit text summaries of a figure to form one full-text summary and evaluated that full-text summary. Each text summary was evaluated against the gold standard for the same figure. Since, for each figure, four subjects evaluated the same figure+full-text combination, we applied one subject's summary as a gold standard to evaluate the three other subjects' judgments. By doing so, we can compare the ROUGE scores among all three figure-text types.</p><p>The ROUGE package [<xref ref-type="bibr" rid="B24">24</xref>] uses recall-based metrics based on n-gram matching between candidate summaries and reference summaries. The package has numerous parameters, including stemming (a process for reducing inflected words to their root form), stop-word removal (to filter out common words such as "a", "the", etc.), and the choice of n-gram. ROUGE-n (n is a number) is n-gram recall; ROUGE-L is based on the longest common subsequence (the common subsequence with maximum length); and ROUGE-W is a weighted longest common subsequence that takes into account distances when applying the longest common subsequence. Different settings reportedly work better for different summarization tasks [<xref ref-type="bibr" rid="B24">24</xref>] and, therefore, different parameters need to be tested for new tasks. In our evaluation, we computed different settings (ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-W [weight = 1.2]) and explored stemming and stop-word removal.</p></sec></sec><sec><title>Results and Discussion</title><p>We report two different approaches to evaluate figure comprehension. First we asked 20 subjects to enter scores based on their self-evaluation of their understanding. Secondly, we asked the same subjects to enter text to summarize figure content. We then evaluated the text summaries of those figures for estimating figure comprehension using two methods. We identified whether the text summary incorporated any cue text that indicated information in the text summary was missing, and applied automatic summarization evaluation techniques to evaluate the quality of text summaries.</p><p>In all, 16 subjects completed both the SCORE and MISSING INFORMATION tasks; this corresponds to the total of 240 subject/figure/figure-text combinations (16 &#x000d7; 5 &#x000d7; 3). Two subjects completed one and two subject/figure/figure-text combinations, respectively. Two subjects dropped out because of their time availability. The total number of subject/figure/figure-text combinations on which we collected data was 243. We found that the average (&#x000b1; SD) time spent for a subject to complete the task was 25.7 &#x000b1; 70.4 min for figure+caption, 33.6 &#x000b1; 147.7 min for figure+caption+title+abstract, and 20.0 &#x000b1; 26.9 min for figure+full-text. Since all evaluation was completed online and the evaluators were not instructed to complete the evaluation task within a block of time, it is not surprising that there were wide confidence intervals. We did not find statistically significant differences in time spent among various figure-text combinations. Note that because of our sequential evaluation design, i.e., we show each evaluator first figure+caption, then figure+caption+title+abstract, and finally the full-text article, we can not exclude that there are statistical differences among different figure-text combinations.</p><p>Table <xref ref-type="table" rid="T2">2</xref> shows the average score for SCORE and for CONFIDENCE. Table <xref ref-type="table" rid="T3">3</xref> shows the McNemar testing of SCORE and CONFIDENCE. The results showed that there were significant differences between all pairs of text conditions for SCORE. Figure+legend+title+abstract was significantly different from figure+legend at p &#x02264; 0.001. Figure+full-text was significantly different from figure+legend at p &#x02264; 0.0001. Figure+legend+title+abstract was significantly different from figure+full-text at p = 0.027. The CONFIDENCE analysis using McNemar testing showed a significant difference between Figure+full-text and figure+legend at p = 0.006, but not between the other two text pairings.</p><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>Score of comprehension and Confidence for each Text Type</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Text Type</th><th align="left">Score of comprehension</th><th align="left">Confidence</th></tr></thead><tbody><tr><td align="left">Fig+L</td><td align="left">5.82 &#x000b1; 2.90</td><td align="left">7.63 &#x000b1; 2.28</td></tr><tr><td colspan="3"><hr/></td></tr><tr><td align="left">Fig+L+T+A</td><td align="left">7.41 &#x000b1; 2.19</td><td align="left">8.17 &#x000b1; 1.70</td></tr><tr><td colspan="3"><hr/></td></tr><tr><td align="left">Full text</td><td align="left">8.17 &#x000b1; 1.97</td><td align="left">8.46 &#x000b1; 1.81</td></tr></tbody></table></table-wrap><table-wrap id="T3" position="float"><label>Table 3</label><caption><p>The P-value of McNemar test of SCORE and CONFIDENCE</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Text Type 1</th><th align="left">Text Type 2</th><th align="left">Binary SCORE</th><th align="left">Binary CONFIDENCE</th></tr></thead><tbody><tr><td align="left">Fig+L</td><td align="left">Fig+L+T+A</td><td align="left">0.0006</td><td align="left">0.2120</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Fig+L</td><td align="left">Full text</td><td align="left">p &#x02264; 0.0001</td><td align="left">0.0055</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Fig+L+T+A</td><td align="left">Full text</td><td align="left">0.0271</td><td align="left">0.0995</td></tr></tbody></table></table-wrap><p>Table <xref ref-type="table" rid="T2">2</xref> shows that both comprehension and confidence of comprehension increase when more associated text are available. Using the image+caption category as a baseline, the score for figure comprehension increased 27.3% when title and abstract were added; when full-text was available, the score increased 40.4%. As shown in Table <xref ref-type="table" rid="T3">3</xref>, the McNemar testing on SCORE concluded that there were significant differences between all pairs of figure-text types when we asked subjects to give a score to indicate to what extent she/he understood the figure. This is clear evidence that the amount of associated text is important to a scientist's subjective understanding of a biomedical figure. This was true across all increments of text. The increase from legend to legend+title+abstract helped the subjects somewhat, but the full text was necessary to achieve the highest levels of understanding consistently.</p><p>Table <xref ref-type="table" rid="T4">4</xref> shows the average number of words in the summaries written by the subjects for the purpose, methods, results, and conclusions in different figure-text combinations. When other associated text are provided in addition to figure legend, our data show that the number of words significantly increase (p &#x02264; 0.0001, paired t-test) in three rhetorical units (except for Methods); between legend and full-text, the number of words increases significantly (p &#x02264; 0.0001, paired t-test) in all four rhetorical units. Between legend+title+abstract and full-text, the number of words increases (p &#x02264; 0.0002, paired t-test) in Methods.</p><table-wrap id="T4" position="float"><label>Table 4</label><caption><p>Average number of words and standard deviation in annotator summaries</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th align="left">Purpose</th><th align="left">Methods</th><th align="left">Results</th><th align="left">Conclusions</th></tr></thead><tbody><tr><td align="left">Fig+L</td><td align="left">11 &#x000b1; 8</td><td align="left">8 &#x000b1; 11</td><td align="left">18 &#x000b1; 18</td><td align="left">8 &#x000b1; 7</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left">Fig+L+T+A</td><td align="left">18 &#x000b1; 11</td><td align="left">10 &#x000b1; 13</td><td align="left">45 &#x000b1; 40</td><td align="left">21 &#x000b1; 15</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left">Full text</td><td align="left">18 &#x000b1; 11</td><td align="left">18 &#x000b1; 13</td><td align="left">44 &#x000b1; 35</td><td align="left">26 &#x000b1; 19</td></tr></tbody></table></table-wrap><p>We found statistical differences in number of words among the four rhetorical units, and did not find any statistically significant word-number differences between any pair of the five figure types (data not shown). During the evaluation subjects frequently commented that the full text, as well as other related figures, was important for understanding the content of a figure. Clearly, subjects in general needed more words to describe the figure when they had more associated text available to them. This implies that the subjects learned new information that they thought was important to the summary as they read more text.</p><p>Table <xref ref-type="table" rid="T5">5</xref> shows the inter-rater agreement between the two judges for the MISSING INFORMATION analysis task. The two judges had a kappa agreement of greater than 0.75 for identifying information missing from the figure+legend+title+abstract in the Methods and Conclusions, and information missing from the figure+legend in the Purpose and Methods. The two judges had a kappa agreement of 0.40&#x02013;0.75 for the rest of text-figure-rhetorical-unit combinations. Kappa values of 0.40&#x02013;0.75 represent fair to good agreement beyond chance, and values greater than 0.75 represent excellent agreement [<xref ref-type="bibr" rid="B19">19</xref>].</p><table-wrap id="T5" position="float"><label>Table 5</label><caption><p>Inter-rater pairwise and kappa agreement</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th align="left">Purpose Missing</th><th align="left">Method Missing</th><th align="left">Result Missing</th><th align="left">Conclusion Missing</th></tr></thead><tbody><tr><td align="left">Fig+L</td><td align="left">89.87% (0.79)</td><td align="left">93.67% (0.87)</td><td align="left">79.75% (0.59)</td><td align="left">86.08% (0.66)</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left">Fig+L+T+A</td><td align="left">96.15% (0.71)</td><td align="left">93.59% (0.83)</td><td align="left">87.18% (0.65)</td><td align="left">91.03% (0.78)</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left">Full text</td><td align="left">98.68% (0.66)</td><td align="left">93.42% (0.58)</td><td align="left">88.16% (0.40)</td><td align="left">88.16% (0.54)</td></tr></tbody></table></table-wrap><p>Our results show a good inter-rater agreement (kappa &#x02265; 0.4) between two judges for missing information comprehension; this demonstrates the effectiveness of applying cue text for identifying missing information from the text summaries. We found that the inconsistency was mainly introduced by differences in interpreting the information in the "other important criteria" field. The inconsistency can be contributed by the ambiguity introduced by the subject who entered the text, the inconsistency in interpreting the text, and challenges in separating related information. For example, when a subject questioned about the type of cell used, as in the example of "<italic>what type of cells are they using? (S. cerevisiae isn't mentioned</italic>)", one could interpret that information is missing in Method. Accordingly, when the methods are missing, the purpose, results and conclusions may also be indicated as unclear as well, because purpose motivates methods, methods inform the interpretation of results, and results inform the conclusions.</p><p>We manually examined the text summaries to determine what caused the disagreement between the judges. We found that two judges agreed entirely when missing information was identified by cue text within a rhetorical unit. All disagreements were introduced by the differences in interpreting the text in "other important criteria" entered by subjects. For example, one judge considered that information was missing in the Purpose, while the other judge interpreted as missing in Method when the text in "other important criteria" was "<italic>What type of cells are they using? (S. cerevisiae isn't mentioned</italic>)." When the "other important criteria" was "background of this protein," one judge considered that information was missing in Purpose, while the other judge did not.</p><p>Tables <xref ref-type="table" rid="T6">6</xref> and <xref ref-type="table" rid="T7">7</xref> show the p-values of McNemar's test for significant differences between figure-text types on dependent rhetorical unit variables for judges 1 and 2, respectively. We found that for both judges, the differences in comprehension as the amount of associated text increased were highly significant for all four rhetorical units when comparing figure+legend with either figure+legend+title+abstract or figure+full-text (p &#x0003c; 0.05). Comparing figure+legend+title+abstract to figure+full-text revealed significant differences for missing methods (judge 1, p = 0.004, and judge 2, p = 0.0099), and for missing conclusions (judge 2, p = 0.0246). The results further support that both comprehension and confidence of comprehension increase when more associated texts are available.</p><table-wrap id="T6" position="float"><label>Table 6</label><caption><p>P-value of McNemar's test for differences between figure &#x02013; text types (judge 1)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Text Type 1</th><th align="left">Text Type 2</th><th align="left">Purpose Missing</th><th align="left">Method Missing</th><th align="left">Result Missing</th><th align="left">Conclusion Missing</th></tr></thead><tbody><tr><td align="left">Fig+L</td><td align="left">Fig+L+T+A</td><td align="left">0.0000</td><td align="left">0.0154</td><td align="left">0.0004</td><td align="left">0.0000</td></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left">Fig+L</td><td align="left">Full Text</td><td align="left">0.0000</td><td align="left">0.0000</td><td align="left">0.0000</td><td align="left">0.0000</td></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left">Fig+L+T+A</td><td align="left">Full Text</td><td align="left">0.4868</td><td align="left">0.0043</td><td align="left">0.0851</td><td align="left">0.1910</td></tr></tbody></table></table-wrap><table-wrap id="T7" position="float"><label>Table 7</label><caption><p>P-value of McNemar's test score for differences between figure &#x02013; text types (judge 2)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Text Type 1</th><th align="left">Text Type 2</th><th align="left">Purpose Missing</th><th align="left">Method Missing</th><th align="left">Result Missing</th><th align="left">Conclusion Missing</th></tr></thead><tbody><tr><td align="left">Fig+L</td><td align="left">Fig+L+T+A</td><td align="left">0.0000</td><td align="left">0.0205</td><td align="left">0.0000</td><td align="left">0.0000</td></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left">Fig+L</td><td align="left">Full text</td><td align="left">0.0000</td><td align="left">0.0000</td><td align="left">0.0000</td><td align="left">0.0000</td></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left">Fig+L+T+A</td><td align="left">Full text</td><td align="left">0.2300</td><td align="left">0.0099</td><td align="left">0.0533</td><td align="left">0.0246</td></tr></tbody></table></table-wrap><p>The figure comprehension rates by the INFORMATION MISSING results are shown in Table <xref ref-type="table" rid="T8">8</xref>. We found that the differences in comprehension as the amount of associated text increased were highly significant for all four rhetorical units (p &#x02264; 0.0001, chi-square test for trend). Post hoc pair-wise Fisher's exact analysis showed that comprehension associated with full text was significantly greater than figure+legend for all four rhetorical units (p &#x02264; 0.001) and better than figure+legend+title+abstract for Methods, Results, and Conclusions (p &#x02264; 0.05 with Bonferroni correction). When the full-text article is not available, presenting just the figure+legend left biomedical researchers lacking 39&#x02013;68% of the information about a figure as compared to having complete figure comprehension; adding the title and abstract improved the situation, but still left biomedical researchers missing 30% of the information. When the full-text article is available, figure comprehension increased to 86&#x02013;97%; this indicates that researchers felt that only 3&#x02013;14% of the necessary information for full figure comprehension was missing when full text was available to them. Clearly there is information in the abstract and in the full text that biomedical scientists deem important for understanding the figures that appear in full-text biomedical articles.</p><table-wrap id="T8" position="float"><label>Table 8</label><caption><p>Comprehension rates (non-missing rhetorical units) associated with figure &#x02013; text types for four rhetorical units.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th align="left">Fig+L</th><th align="left">Fig+L+T+A</th><th align="left">Full Text</th></tr></thead><tbody><tr><td align="left">Purpose</td><td align="left">0.57</td><td align="left">0.92</td><td align="left">0.97</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Methods</td><td align="left">0.61</td><td align="left">0.76</td><td align="left">0.91</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Results</td><td align="left">0.49</td><td align="left">0.76</td><td align="left">0.89</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Conclusions</td><td align="left">0.32</td><td align="left">0.73</td><td align="left">0.86</td></tr></tbody></table></table-wrap><p>Table <xref ref-type="table" rid="T9">9</xref> lists F-scores of the ROUGE-L measurement. We evaluated the text summaries by rhetorical unit. We also aggregated the four units to form a full summary and evaluated the full summaries. Each text summary was evaluated against figure+full-text judgments for the same figure. For summaries generated with the figure+full-text combination, one subject's summary was measured against other subjects' judgments. The score of a figure-text combination is the average of all summaries or rhetorical units of that category. The table shows that the quality of summaries or rhetorical units largely depends on the figure-text combinations. For the figure+legend combination, all four rhetorical units and full summaries received the lowest scores. By adding the title+abstract, average F-scores increased, and the full-text significantly improved the scores. This is consistent with the other evaluations of the differences among the different figure-text combinations. We also computed ROUGE-1, ROUGE-2, and ROUGE-W (data not shown), and the trend for these did not differ from ROUGE-L.</p><table-wrap id="T9" position="float"><label>Table 9</label><caption><p>ROUGE-L F-scores for different figure &#x02013; text types.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th align="left">Fig+L</th><th align="left">Fig+L+T+A</th><th align="left">Full text</th></tr></thead><tbody><tr><td align="left">Purpose</td><td align="left">0.46</td><td align="left">0.60</td><td align="left">0.61</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Methods</td><td align="left">0.38</td><td align="left">0.48</td><td align="left">0.59</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Results</td><td align="left">0.28</td><td align="left">0.35</td><td align="left">0.58</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Conclusions</td><td align="left">0.22</td><td align="left">0.28</td><td align="left">0.56</td></tr><tr><td colspan="4"><hr/></td></tr><tr><td align="left">Full summaries</td><td align="left">0.32</td><td align="left">0.40</td><td align="left">0.58</td></tr></tbody></table></table-wrap><p>The ROUGE analysis shows that for the figure+legend combination, all four rhetorical units and full summaries received the lowest ROUGE scores. By adding the title+abstract, the average F-scores increased, and the full text significantly improved the scores. For example, for the full summaries, the F-score for the full summary increased 25%, from 0.32 in figure+legend to 0.40 in figure+legend+title+abstract, and further increased 45%, to 0.58 in figure+full-text. The differences are statistically significant. The results support that the quality of text summaries increases when the amount of associated text increases.</p><p>On the other hand, the overall ROUGE scores in our study are relatively low compared to the DUC ROUGE evaluation results [<xref ref-type="bibr" rid="B16">16</xref>]. This is not surprising, because ROUGE measures whether a particular summary has the same words (or n-grams) as a reference summary. ROUGE is mainly used for extractive summarization evaluation where summaries include only sentences selected from the original texts. However, in our experiments, summaries are not extractive. Instead, they are generated by subjects without constraints on word choice. It is very common that summaries of almost identical meanings have very different words. This results in low ROUGE scores. As our purpose is to measure the difference between different combinations rather than summaries of one particular combination, ROUGE is still an effective measure in our evaluation, although it might not be the best one. An alternative is to apply the pyramid evaluation [<xref ref-type="bibr" rid="B17">17</xref>]. The pyramid method differs from ROUGE primarily in assigning weights to summary content units, not bags of words. A content unit represents a minimum semantic unit in a text summary. To carry out the pyramid method, we will need to recruit additional biomedical domain experts to manually analyze the text summaries by content unit. Obviously this is much more labor intensive than the method used in our study and remains for future work.</p><p>Analyzing by content unit is an informative approach and will continue to be part of our future work when researching text-summary-based figure comprehension. When we manually examined the text summaries generated by biomedical experts, we found that both the depth and breath of information in a text summary increase as more associated text is provided. An example is shown in the following which is a list of text summaries (shown within quotation markers) generated by one subject with different figure-text types on Figure 2 in the article [<xref ref-type="bibr" rid="B25">25</xref>]:</p><p><bold>Figure+legend</bold>: <italic>Purpose: </italic>"Compare growth rates between wild type and ybr159 mutant cells." <italic>Methods: </italic>"Measure the growth rate with photometer." <italic>Results: </italic>"Growth rate of the mutant cells is lower." <italic>Conclusions: </italic>"The maximum growth in wild type cells occurs earlier."</p><p><bold>Fig+L+T+A</bold>: <italic>Purpose: </italic>"Examine effect of Ybr159w disruption in S. cerevisiae mutant cells." <italic>Methods: </italic>"Disruption of Ybr159w gene, compare growth between wild type and mutant cells via optical method." <italic>Results: </italic>"Mutants are slow growing and display high temperature sensitivity." <italic>Conclusions: </italic>"Disruption of YBR159w is not lethal since there is some growth in mutant cells."</p><p><bold>Full text</bold>: <italic>Purpose: </italic>"Identify a gene required for the reconstitution of heterologous elongase activity. Examine the requirement of YBR159w for yeast viability." <italic>Methods: </italic>"they used a loss-of-heterologous-function screen to genetically identify a component of the microsomal fatty acid elongase." <italic>Results: </italic>"cells are able to grow at 30&#x000b0;C in rich medium (in the absence of fatty acid supplements) but at a slower rate than wild type." <italic>Conclusions: </italic>"initial slow growth may be related to some form of adaptive response to the loss of this microsomal elongase component. After this adaptation the mutant spore colonies formed are viable in the absence of fatty acid supplement, although growing at a much slower rate than wild type cells."</p><p>As shown in the text summaries above, it is clear that both the quantity and quality of text summaries increase as more associated text was provided. However, such increase was implicit in our study, based on the lack of evidence of misunderstanding; the subject did not enter any symbolic cue text to indicate any missing information in text summaries. An advantage of our binary coding scheme was its simplicity and consistency. However, a multipoint rating or more complex and detailed coding guidelines may be necessary to detect more subtle differences in missing information and improve the analysis. Ultimately, a semantic interpretation analyzing text summaries by content unit may be the best approach to measure the level of figure comprehension. If we can accurately measure the implicit missing information, we speculate that the differences in figure comprehension will be even higher among different figure-text types.</p><p>One limitation in our evaluation design is that the associated text was given to each subject incrementally. The advantage of such an evaluation design is that we can support the statistical validity with much fewer examples than needed for a randomized evaluation using paired comparisons. On the other hand, the design may introduce a bias: the longer a subject is exposed to the figure, the better the subject understood the figure. We think that such a bias was minimized in our study because we did not pose any time constraint on each subject for his/her evaluation at each level of text. In fact, our data show that there was no statistical difference in the time spent among different types of associated text. Furthermore, many summaries, such as the one shown above, clearly demonstrate that the difference in missing information was caused by the content of associated text available to the subject, and not by the exposure time given to a figure.</p><p>In summary, all the evaluation approaches used in this study strongly indicate that there are statistically significant differences in biomedical researchers' figure comprehension when they are given three different levels of text &#x02013; the legend, legend+title+abstract, and full-text. Clearly, the figure+legend is insufficient, the figure+legend+title+abstract is somewhat better, but having access to the full text article is necessary to really understand the full meaning of a figure.</p></sec><sec><title>Conclusion</title><p>We conclude that associated text other than the figure legend is very important for biomedical scientists' understanding of the meaning of a figure in a full-text biomedical article. Systems that ignore the information in the full-text article, and that only present to users abstract and figure legend could risk losing 30% of information in figure comprehension. We predict that automated systems that extract the relevant explanatory information or a summary from the full text along with extracted figures could be very useful in reducing the workload of biomedical scientists who would otherwise have to retrieve and read the associated full-text journal article to determine which figures are relevant to their research and understand the biomedical evidence presented in those figures.</p></sec><sec><title>Authors' contributions</title><p>HY and AC designed the experiments. MJ participated in the experiment design. SA carried out the experiments with subjects. HY wrote the paper. AC reviewed and edited the paper.</p></sec></body><back><sec><title>Acknowledgements</title><p>We thank Fang Huang for analyzing the data using ROUGE scores. We also thank William R. Hersh for some helpful discussion and David Levy for editing. The University of Wisconsin-Milwaukee's RGI in 2007&#x02013;2008 to Hong Yu provided the support for this research. Hong Yu also acknowledges the support of 5R01LM009836-02 and 5R21RR024933-02. Aaron Cohen acknowledges the support of NSF Grant ITR-0325160.</p></sec><ref-list><ref id="B1"><mixed-citation publication-type="journal"><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>M</given-names></name><article-title>Accessing bioscience images from abstract sentences</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>e547</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btl261</pub-id><pub-id pub-id-type="pmid">16873519</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><name><surname>Krauthammer</surname><given-names>M</given-names></name><name><surname>Kra</surname><given-names>P</given-names></name><name><surname>Iossifov</surname><given-names>I</given-names></name><name><surname>Gomez</surname><given-names>SM</given-names></name><name><surname>Hripcsak</surname><given-names>G</given-names></name><name><surname>Hatzivassiloglou</surname><given-names>V</given-names></name><name><surname>Friedman</surname><given-names>C</given-names></name><name><surname>Rzhetsky</surname><given-names>A</given-names></name><article-title>Of truth and pathways: chasing bits of information through myriads of articles</article-title><source>Bioinformatics</source><year>2002</year><volume>18</volume><issue>Suppl 1</issue><fpage>S249</fpage><lpage>257</lpage><pub-id pub-id-type="pmid">12169554</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><name><surname>Zweigenbaum</surname><given-names>P</given-names></name><name><surname>Demner-Fushman</surname><given-names>D</given-names></name><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Cohen</surname><given-names>KB</given-names></name><article-title>Frontiers of biomedical text mining: current progress</article-title><source>Brief Bioinform</source><year>2007</year><volume>8</volume><fpage>358</fpage><lpage>375</lpage><pub-id pub-id-type="pmid">17977867</pub-id><pub-id pub-id-type="doi">10.1093/bib/bbm045</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="other"><name><surname>Murphy</surname><given-names>R</given-names></name><name><surname>Kou</surname><given-names>Z</given-names></name><name><surname>Hua</surname><given-names>J</given-names></name><name><surname>Joffe</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>W</given-names></name><article-title>Extracting and structuring subcellular location information from online journal articles: the subcellular location image finder</article-title><source>Proceedings of IASTED International Conference on Knowledge Sharing and Collaborative Engineering (KSCE)</source><year>2004</year></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="other"><name><surname>Murphy</surname><given-names>R</given-names></name><name><surname>Velliste</surname><given-names>M</given-names></name><name><surname>Yao</surname><given-names>J</given-names></name><name><surname>Porreca</surname><given-names>G</given-names></name><article-title>Searching Online Journals for Fluorescence Microscope Images depicting Protein Subcellular Location Patterns</article-title><source>IEEE International Symposium on Bio-Informatics and Biomedical Engineering (BIBE)</source><year>2001</year><fpage>119</fpage><lpage>128</lpage></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="other"><name><surname>Rafkind</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>M</given-names></name><name><surname>Chang</surname><given-names>S</given-names></name><name><surname>Yu</surname><given-names>H</given-names></name><article-title>Exploring text and image features to classify images in bioscience literature</article-title><source>HLT-NAACL BioNLP. New York, USA</source><year>2006</year></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><name><surname>Shatkay</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>N</given-names></name><name><surname>Blostein</surname><given-names>D</given-names></name><article-title>Integrating image data into biomedical text categorization</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>e446</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btl235</pub-id><pub-id pub-id-type="pmid">16873506</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="other"><name><surname>Hearst</surname><given-names>M</given-names></name><name><surname>Divoli</surname><given-names>A</given-names></name><name><surname>Wooldridge</surname><given-names>MA</given-names></name><name><surname>Ye</surname><given-names>J</given-names></name><article-title>Exploring the efficacy of caption search for bioscience journal search interfaces</article-title><source>45th Annual Meeting of the Association for Computational Linguistics BioNLP workshop. Prague, Czech Repubic</source><year>2007</year></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><name><surname>Hearst</surname><given-names>MA</given-names></name><name><surname>Divoli</surname><given-names>A</given-names></name><name><surname>Guturu</surname><given-names>H</given-names></name><name><surname>Ksikes</surname><given-names>A</given-names></name><name><surname>Nakov</surname><given-names>P</given-names></name><name><surname>Wooldridge</surname><given-names>MA</given-names></name><name><surname>Ye</surname><given-names>J</given-names></name><article-title>BioText Search Engine: beyond abstract search</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><fpage>2196</fpage><lpage>2197</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btm301</pub-id><pub-id pub-id-type="pmid">17545178</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="other"><name><surname>Yu</surname><given-names>H</given-names></name><article-title>Towards Answering Biological Questions with Experimental Evidence: Automatically Identifying Text that Summarize Image Content in Full-Text Articles</article-title><source>AMIA Annu Symp Proc</source><year>2006</year><fpage>834</fpage><lpage>838</lpage><pub-id pub-id-type="pmid">17238458</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="other"><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>M</given-names></name><article-title>BioEx: a novel user-interface that accesses images from abstract sentences</article-title><source>HLT-NAACL. New York, USA</source><year>2006</year></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><name><surname>Yeh</surname><given-names>AS</given-names></name><name><surname>Hirschman</surname><given-names>L</given-names></name><name><surname>Morgan</surname><given-names>AA</given-names></name><article-title>Evaluation of text data mining for database curation: lessons learned from the KDD Challenge Cup</article-title><source>Bioinformatics</source><year>2003</year><volume>19</volume><issue>Suppl 1</issue><fpage>i331</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btg1046</pub-id><pub-id pub-id-type="pmid">12855478</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><name><surname>Regev</surname><given-names>Y</given-names></name><name><surname>Finkelstein-Landau</surname><given-names>M</given-names></name><name><surname>Feldman</surname><given-names>R</given-names></name><name><surname>Gorodetsky</surname><given-names>M</given-names></name><name><surname>Zheng</surname><given-names>X</given-names></name><name><surname>Levy</surname><given-names>S</given-names></name><name><surname>Charlab</surname><given-names>R</given-names></name><name><surname>Lawrence</surname><given-names>C</given-names></name><name><surname>Lippert</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name><name><surname>Shatkay</surname><given-names>H</given-names></name><article-title>Rule-based extraction of experimental evidence in the biomedical domain: the KDD Cup 2002 (task 1)</article-title><source>ACM SIGKDD Exploration Newsletter</source><year>2002</year><volume>4</volume><fpage>90</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1145/772862.772874</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><name><surname>Xu</surname><given-names>S</given-names></name><name><surname>McCusker</surname><given-names>J</given-names></name><name><surname>Krauthammer</surname><given-names>M</given-names></name><article-title>Yale Image Finder (YIF): a new search engine for retrieving biomedical images</article-title><source>Bioinformatics</source><year>2008</year><volume>24</volume><fpage>1968</fpage><lpage>1970</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btn340</pub-id><pub-id pub-id-type="pmid">18614584</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="other"><name><surname>Hersh</surname><given-names>W</given-names></name><name><surname>Cohen</surname><given-names>A</given-names></name><name><surname>Roberts</surname><given-names>P</given-names></name><name><surname>Rekapalli</surname><given-names>H</given-names></name><article-title>TREC 2006 Genomics Track overview</article-title><source>TREC Genomics Track conference</source><year>2006</year></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><name><surname>Patel</surname><given-names>V</given-names></name><name><surname>Eisemon</surname><given-names>T</given-names></name><name><surname>Arocha</surname><given-names>J</given-names></name><article-title>Comprehending instructions for using pharmaceutical products in rural kenya</article-title><source>Instructional Science</source><year>1990</year><volume>19</volume><fpage>71</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1007/BF00377986</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="other"><name><surname>McKnight</surname><given-names>L</given-names></name><name><surname>Srinivasan</surname><given-names>P</given-names></name><article-title>Categorization of sentence types in medical abstracts</article-title><source>AMIA Annu Symp Proc</source><year>2003</year><fpage>440</fpage><lpage>444</lpage><pub-id pub-id-type="pmid">14728211</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><name><surname>Mullen</surname><given-names>T</given-names></name><name><surname>Mizuta</surname><given-names>Y</given-names></name><name><surname>Collier</surname><given-names>N</given-names></name><article-title>A baseline feature set for learning rhetorical zones using full articles in the biomedical domain</article-title><source>ACM SIGKDD Explorations Newsletter</source><year>2005</year><volume>7</volume><fpage>52</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1145/1089815.1089823</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="book"><name><surname>Fleiss</surname><given-names>J</given-names></name><source>Statistical methods for rates and proportions</source><year>1981</year><publisher-name>New York: John Wiley &#x00026; Sons</publisher-name></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="other"><name><surname>Jing</surname><given-names>H</given-names></name><name><surname>Barzilay</surname><given-names>R</given-names></name><name><surname>McKeown</surname><given-names>K</given-names></name><name><surname>Elhadad</surname><given-names>M</given-names></name><article-title>Summarization evaluation methods: experiments and analysis</article-title><source>AAAI Symposium on Intelligent Summarization</source><year>1998</year></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="other"><name><surname>Lapata</surname><given-names>M</given-names></name><name><surname>Barzilay</surname><given-names>R</given-names></name><article-title>Automatic evaluation of text coherence: models and representations</article-title><source>Proceedings of the 19th International Joint Conference on Artificial Intelligence</source><year>2005</year></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><name><surname>Mani</surname><given-names>I</given-names></name><name><surname>Klein</surname><given-names>G</given-names></name><name><surname>House</surname><given-names>D</given-names></name><name><surname>Hirschman</surname><given-names>L</given-names></name><name><surname>Firmin</surname><given-names>T</given-names></name><name><surname>Sundheim</surname><given-names>B</given-names></name><article-title>SUMMAC: a text summarization evaluation</article-title><source>Natural Language Engineering</source><year>2002</year><volume>8</volume><fpage>43</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1017/S1351324901002741</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="other"><name><surname>McKeown</surname><given-names>K</given-names></name><name><surname>Passonneau</surname><given-names>R</given-names></name><name><surname>Elson</surname><given-names>D</given-names></name><name><surname>Nenkova</surname><given-names>A</given-names></name><name><surname>Hirschberg</surname><given-names>J</given-names></name><article-title>Do summaries help? a task-based evaluation of multi-document summarization</article-title><source>Proceedings of SIGIR</source><year>2005</year></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="other"><name><surname>Lin</surname><given-names>C</given-names></name><article-title>ROUGE: A package for automatic evaluation of summaries</article-title><source>Proceedings of the ACL Workshop: Text Summarization Braches Out 2004</source><year>2004</year><fpage>74</fpage><lpage>81</lpage></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><name><surname>Beaudoin</surname><given-names>F</given-names></name><name><surname>Gable</surname><given-names>K</given-names></name><name><surname>Sayanova</surname><given-names>O</given-names></name><name><surname>Dunn</surname><given-names>T</given-names></name><name><surname>Napier</surname><given-names>JA</given-names></name><article-title>A Saccharomyces cerevisiae gene required for heterologous fatty acid elongase activity encodes a microsomal beta-keto-reductase</article-title><source>J Biol Chem</source><year>2002</year><volume>277</volume><fpage>11481</fpage><lpage>11488</lpage><pub-id pub-id-type="doi">10.1074/jbc.M111441200</pub-id><pub-id pub-id-type="pmid">11792704</pub-id></mixed-citation></ref></ref-list></back></article>