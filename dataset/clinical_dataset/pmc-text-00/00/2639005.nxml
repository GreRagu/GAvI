<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article" xml:lang="EN"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id><journal-id journal-id-type="publisher-id">bioinformatics</journal-id><journal-id journal-id-type="hwp">bioinfo</journal-id><journal-title>Bioinformatics</journal-title><issn pub-type="ppub">1367-4803</issn><issn pub-type="epub">1460-2059</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19015141</article-id><article-id pub-id-type="pmc">2639005</article-id><article-id pub-id-type="doi">10.1093/bioinformatics/btn602</article-id><article-id pub-id-type="publisher-id">btn602</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Papers</subject><subj-group><subject>Systems Biology</subject></subj-group></subj-group></article-categories><title-group><article-title>Training set expansion: an approach to improving the reconstruction of biological networks from limited and uneven reliable interactions</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yip</surname><given-names>Kevin Y.</given-names></name><xref ref-type="aff" rid="AFF1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Gerstein</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="AFF1"><sup>1</sup></xref><xref ref-type="aff" rid="AFF1"><sup>2</sup></xref><xref ref-type="aff" rid="AFF1"><sup>3</sup></xref><xref ref-type="corresp" rid="COR1">*</xref></contrib></contrib-group><aff id="AFF1"><sup>1</sup>Department of Computer Science, Yale University, 51 Prospect Street, New Haven, CT 06511, <sup>2</sup>Program in Computational Biology and Bioinformatics, Yale University and <sup>3</sup>Department of Molecular Biophysics and Biochemistry, Yale University, 266 Whitney Avenue, New Haven, CT 06520, USA</aff><author-notes><corresp id="COR1">*To whom correspondence should be addressed.</corresp><fn><p>Associate Editor: Olga Troyanskaya</p></fn></author-notes><pub-date pub-type="ppub"><day>15</day><month>1</month><year>2009</year></pub-date><pub-date pub-type="epub"><day>17</day><month>11</month><year>2008</year></pub-date><pub-date pub-type="pmc-release"><day>17</day><month>11</month><year>2008</year></pub-date><volume>25</volume><issue>2</issue><fpage>243</fpage><lpage>250</lpage><history><date date-type="received"><day>17</day><month>8</month><year>2008</year></date><date date-type="rev-recd"><day>15</day><month>10</month><year>2008</year></date><date date-type="accepted"><day>13</day><month>11</month><year>2008</year></date></history><permissions><copyright-statement>&#x000a9; 2008 The Author(s)</copyright-statement><copyright-year>2008</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc/2.0/uk/"><p><!--CREATIVE COMMONS-->This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/2.0/uk/">http://creativecommons.org/licenses/by-nc/2.0/uk/</ext-link>) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.</p></license></permissions><abstract><p><bold>Motivation:</bold> An important problem in systems biology is reconstructing complete networks of interactions between biological objects by extrapolating from a few known interactions as examples. While there are many computational techniques proposed for this network reconstruction task, their accuracy is consistently limited by the small number of high-confidence examples, and the uneven distribution of these examples across the potential interaction space, with some objects having many known interactions and others few.</p><p><bold>Results:</bold> To address this issue, we propose two computational methods based on the concept of <italic>training set expansion</italic>. They work particularly effectively in conjunction with kernel approaches, which are a popular class of approaches for fusing together many disparate types of features. Both our methods are based on semi-supervised learning and involve augmenting the limited number of gold-standard training instances with carefully chosen and highly confident auxiliary examples. The first method, <italic>prediction propagation</italic>, propagates highly confident predictions of one local model to another as the auxiliary examples, thus learning from information-rich regions of the training network to help predict the information-poor regions. The second method, <italic>kernel initialization</italic>, takes the most similar and most dissimilar objects of each object in a global kernel as the auxiliary examples. Using several sets of experimentally verified protein&#x02013;protein interactions from yeast, we show that training set expansion gives a measurable performance gain over a number of representative, state-of-the-art network reconstruction methods, and it can correctly identify some interactions that are ranked low by other methods due to the lack of training examples of the involved proteins.</p><p><bold>Contact:</bold> <email>mark.gerstein@yale.edu</email></p><p><bold>Availability:</bold> The datasets and additional materials can be found at <ext-link ext-link-type="uri" xlink:href="http://networks.gersteinlab.org/tse">http://networks.gersteinlab.org/tse</ext-link>.</p></abstract></article-meta></front><body><sec sec-type="intro" id="SEC1"><title>1 INTRODUCTION</title><p>Many types of biological data are naturally represented as networks, in which a node corresponds to a biological object and an edge corresponds to an interaction between two objects. For example, in protein interaction networks, a node is a protein and an edge connects two proteins that physically interact. In gene regulatory networks, a node denotes a gene and its corresponding protein(s), and an edge connects a regulator protein to a gene it regulates. In genetic networks, a node is a gene and an edge connects two genes that have genetic interactions, such as synthetic lethality.</p><p>These networks provide important information for understanding the underlying biological processes, since they offer a global view of the relationships between biological objects. In recent years, high-throughput experiments have enabled large-scale reconstruction of the networks. However, as these data are usually incomplete and noisy, they can only be used as a first approximation of the complete networks. For example, a recent study reports that the false positive and negative rates of yeast two-hybrid protein&#x02013;protein interaction data could be as high as 25&#x02013;45% and 75&#x02013;90%, respectively (Huang <italic>et al</italic>., <xref ref-type="bibr" rid="B10">2007</xref>), and a recently published dataset combining multiple large-scale yeast two-hybrid screens is estimated to cover only 20% of the yeast binary interactome (Yu <italic>et al</italic>., <xref ref-type="bibr" rid="B28">2008</xref>). As another example, as of July 2008, the synthetic lethal interactions in the BioGRID database (Stark <italic>et al</italic>., <xref ref-type="bibr" rid="B21">2006</xref>) (version 2.0.42) only involve 2505 yeast genes, while there are about 5000 non-essential genes in yeast (Giaever <italic>et al</italic>., <xref ref-type="bibr" rid="B9">2002</xref>). A large part of the genetic network is likely not yet discovered.</p><p>To complement the experimental data, computational methods have been developed to assist the reconstruction of the networks. These methods learn from some example interactions, and predict the missing ones based on the learned models.</p><p>This problem is known as supervised network inference (Vert and Yamanishi, <xref ref-type="bibr" rid="B25">2005</xref>). The input to the problem is a graph <italic>G</italic> = (<italic>V, E, <overline>E</overline></italic>), where <italic>V</italic> is a set of nodes each representing a biological object (e.g. a protein), and <italic>E</italic>,<overline><italic>E</italic></overline> &#x02282; <italic>V</italic> &#x000d7; <italic>V</italic> are sets of known edges and non-edges, respectively, corresponding to object pairs that are known to interact and not interact, respectively. All the remaining pairs are not known to interact or not (<xref ref-type="fig" rid="F1">Fig. 1</xref>a). A model is to be learned from the data, so that when given any object pair (<italic>v<sub>i</sub></italic>, <italic>v<sub>j</sub></italic>) as input, it will output a prediction <italic>y</italic> &#x02208; [0, 1] where a larger value means a higher chance of interaction between the objects.<fig id="F1" position="float"><label>Fig. 1.</label><caption><p>The supervised network inference problem. (<bold>a</bold>) Adjacency matrix of known interactions (black boxes), known non-interactions (white boxes) and node pairs with an unknown interaction status (gray boxes with question marks). (<bold>b</bold>) Kernel matrix, with a darker color representing a larger inner product. (<bold>c</bold>) Partially complete adjacency matrix required by the supervised direct approach methods, with complete knowledge of a submatrix. In the basic local modeling approach, the dark gray portion cannot be predicted.</p></caption><graphic xlink:href="btn602f1"/></fig></p><p>The models are learned according to some data features that describe the objects. For example, in predicting protein&#x02013;protein interaction networks, functional genomic data are commonly used. In order to learn models that can make accurate predictions, it is usually required to integrate heterogeneous types of data as they contain different kinds of information. Since the data are in different formats (e.g. numeric values for gene expression, strings for protein sequences), integrating them is non-trivial. A natural choice for this complex data integration task is kernel methods (Sch&#x000f6;lkopf <italic>et al</italic>., <xref ref-type="bibr" rid="B18">2004</xref>), which unify the data representation as special matrices called kernels and facilitate easy integration of these kernels into a final kernel <italic>K</italic> through various means (Lanckriet <italic>et al</italic>., <xref ref-type="bibr" rid="B14">2004</xref>) (<xref ref-type="fig" rid="F1">Fig. 1</xref>b). As long as <italic>K</italic> is positive semi-definite, <italic>K</italic>(<italic>v<sub>i</sub></italic>, <italic>v<sub>j</sub></italic>) represents the inner product of objects <italic>v<sub>i</sub></italic> and <italic>v<sub>j</sub></italic> in a certain embedded space (Mercer, <xref ref-type="bibr" rid="B15">1909</xref>), which can be interpreted as the similarity between the objects. Kernel methods then learn the models from the training examples and the inner products (Aizerman <italic>et al</italic>., <xref ref-type="bibr" rid="B1">1964</xref>). Since network reconstruction involves many kinds of data, in this article we will focus on kernel methods for learning.</p><p>The supervised network inference problem differs from most other machine learning settings in that instead of making a prediction for each input object (such as a protein), the learning algorithm makes a prediction for each pair of objects, namely how likely these objects interact in the biological network. Since there is a quadratic number of object pairs, the computational cost could be very high. For instance, while learning a model for around 6000 genes of yeast is not a difficult task for contemporary computing machines, the corresponding task for around 18 million gene pairs remains challenging even for high-end computers. Specialized kernel methods have thus been developed for this learning problem.</p><p>For networks with noisy high-throughput data, reliable &#x02018;gold-standard&#x02019; training sets are to be obtained from data verified by small-scale experiments or evidenced by multiple methods. As the number of such interactions is small, there is a scarcity of training data. In addition, the training data from small-scale experiments are usually biased towards some well-studied proteins, creating an uneven distribution of training examples across proteins.</p><p>In the next section, we review some existing computational approaches to reconstructing biological networks. One recent proposal is <italic>local modeling</italic> (Bleakley <italic>et al</italic>., <xref ref-type="bibr" rid="B3">2007</xref>), which allows for the construction of very flexible models by letting each object construct a different <italic>local model</italic>, and has been shown promising in some network reconstruction tasks. However, when there is a scarcity of training data, the high flexibility could turn out to be a disadvantage, as there is a high risk of overfitting, i.e. the construction of overly complex models that fit the training data well but do not represent the general trend of the whole network. As a result, the prediction accuracy of the models could be affected.</p><p>In this article, we propose methods called <italic>training set expansion</italic> that alleviate the problem of local modeling, while preserving its modeling flexibility. They also handle the issue of uneven training examples by propagating knowledge from information-rich regions to information-poor regions. We will show that the resulting algorithms are highly competitive with the existing approaches in terms of prediction accuracy. We will also present some interesting findings based on the prediction results.</p></sec><sec id="SEC2"><title>2 RELATED WORK: EXISTING APPROACHES FOR NETWORK RECONSTRUCTION</title><sec id="SEC2.1"><title>2.1 The pairwise kernel approach</title><p>In the pairwise kernel (Pkernel) approach (Ben-Hur and Noble, <xref ref-type="bibr" rid="B2">2005</xref>), the goal is to use a standard kernel method (such as SVM) to make the predictions by treating each object pair as a data instance (<xref ref-type="fig" rid="F2">Fig. 2</xref>a, b). This requires the definition of an embedded space for object pairs. In other words, a kernel is to be defined, which takes two pairs of objects and returns their inner product. With <italic>n</italic> objects, the kernel matrix contains <italic>O</italic>(<italic>n</italic><sup>4</sup>) entries in total.<fig id="F2" position="float"><label>Fig. 2.</label><caption><p>Global and local modeling. (<bold>a</bold>) An interaction network with each green solid edge representing a known interaction, each red dotted edge representing a known non-interaction and the dashed edge representing a pair of objects with an unknown interaction status. (<bold>b</bold>) A global model based on a Pkernel. (<bold>c</bold>) A local model for object <italic>v</italic><sub>3</sub>.</p></caption><graphic xlink:href="btn602f2"/></fig></p><p>One systematic approach to constructing such <italic>Pkernel</italic> is to build them on top of an existing kernel for individual objects, in which each entry corresponds to the inner product of two objects. For example, suppose a kernel <italic>K</italic> for individual objects is given, and <italic>v</italic><sub>1</sub>, <italic>v</italic><sub>2</sub>, <italic>v</italic><sub>3</sub>, <italic>v</italic><sub>4</sub> are four objects, the following function can be used to build the Pkernel (Ben-Hur and Noble, <xref ref-type="bibr" rid="B2">2005</xref>):<disp-formula id="M1"><label>(1)</label><graphic xlink:href="btn602m1.jpg" position="float"/></disp-formula></p><p>Loosely speaking, two object pairs are similar if the two objects in the first pair are, respectively, similar to different objects in the second pair.</p></sec><sec id="SEC2.2"><title>2.2 The direct approach</title><p>The direct approach (Yamanishi <italic>et al</italic>., <xref ref-type="bibr" rid="B26">2004</xref>) avoids working in the embedded space of object pairs. Instead, only a kernel for individual objects is needed. Given such an input kernel <italic>K</italic> and a cutoff threshold <italic>t</italic>, the direct approach simply predicts each pair of objects (<italic>v<sub>i</sub></italic>, <italic>v<sub>j</sub></italic>) with <italic>K</italic>(<italic>v<sub>i</sub></italic>, <italic>v<sub>j</sub></italic>) &#x02267; <italic>t</italic> to interact, and each other pair to not interact. Since the example interactions and non-interactions are not used in making the predictions, this method is unsupervised.</p><p>The direct approach is related to the Pkernel approach through a simple Pkernel:<disp-formula id="M2"><label>(2)</label><graphic xlink:href="btn602m2.jpg" position="float"/></disp-formula></p><p>With this kernel, each object pair (<italic>v<sub>i</sub></italic>, <italic>v<sub>j</sub></italic>) is mapped to the point <italic>K</italic>(<italic>v<sub>i</sub></italic>, <italic>v<sub>j</sub></italic>) on the real line in the embedded space of object pairs. Thresholding the object pairs at a value <italic>t</italic> is equivalent to placing a hyperplane in the embedded space with all pairs (<italic>v<sub>i</sub></italic>, <italic>v<sub>j</sub></italic>) having <italic>K</italic>(<italic>v<sub>i</sub></italic>, <italic>v<sub>j</sub></italic>) &#x02267; <italic>t</italic> on one side and all other pairs on the other side. Therefore, if this Pkernel is used, then learning a linear classifier in the embedded space is equivalent to learning the best value for threshold <italic>t</italic>.</p><p>To make use of the training examples, two supervised versions of the direct approach have been proposed. They assume that the sub-network of a subset of objects is completely known, so that a submatrix of the adjacency matrix is totally filled (<xref ref-type="fig" rid="F1">Fig. 1</xref>c). The goal is to modify the similarity values of the objects defined by the kernel to values that are more consistent with the partial adjacency matrix. Thresholding is then performed on the resulting set of similarity values.</p><p>The two versions differ in the definition of consistency between the similarity values and the adjacency matrix. In the kernel canonical correlation analysis (kCCA) approach (Yamanishi <italic>et al</italic>., <xref ref-type="bibr" rid="B26">2004</xref>), the goal is to identify feature <italic>f</italic><sub>1</sub> from the input kernel and feature <italic>f</italic><sub>2</sub> from the diffusion kernel derived from the partial adjacency matrix so that the two features have the highest correlation under some smoothness requirements. Additional feature pairs orthogonal to the previous ones are identified in similar ways, and the first <italic>l</italic> pairs are used to redefine the similarity between objects.</p><p>In the kernel metric learning (kML) approach (Vert and Yamanishi, <xref ref-type="bibr" rid="B25">2005</xref>), a feature <italic>f</italic><sub>1</sub> is identified by optimizing a function that involves the distance between known interacting objects. Again, additional orthogonal features are identified, and the similarity between objects is redefined by these features.</p></sec><sec id="SEC2.3"><title>2.3 The matrix completion approach</title><p>The em approach (Tsuda <italic>et al</italic>., <xref ref-type="bibr" rid="B23">2003</xref>) also assumes a partially complete adjacency matrix. The goal is to complete it by filling in the missing entries, so that the resulting matrix is closest to a spectral variant of the kernel matrix as measured by Kullback&#x02013;Leibler (KL)-divergence. The algorithm iteratively searches for the filled adjacency matrix that is closest to the current spectral variant of the kernel matrix, and the spectral variant of the kernel matrix that is closest to the current filled adjacency matrix. When convergence is reached, the predictions are read from the final completed adjacency matrix.</p></sec><sec id="SEC2.4"><title>2.4 The local modeling approach</title><p>A potential problem of the previous approaches is that one single model is built for all object pairs. If there are different subgroups of interactions, a single model may not be able to separate all interacting pairs from non-interacting ones. For example, protein pairs involved in transient interactions may use a very different mechanism than those involved in permanent complexes. These two types of interactions may form two separate subgroups that cannot be fitted by one single model.</p><p>A similar problem has been discussed in Myers and Troyanskaya (<xref ref-type="bibr" rid="B16">2007</xref>). In this work, the biological context of each gene is taken into account by conditioning the probability terms of a Bayesian model by the biological context. The additional modeling power of having multiple context-dependent sub-models was demonstrated by improved accuracy in network prediction.</p><p>Another way to allow for a more flexible modeling of the subgroups is <italic>local modeling</italic> (Bleakley <italic>et al</italic>., <xref ref-type="bibr" rid="B3">2007</xref>). Instead of building a single global model for the whole network, one local model is built for each object, using the known interactions and non-interactions of it as the positive and negative examples. Each pair of objects thus receives two predictions, one from the local model of each object. In our implementation, the final prediction is a weighted sum of the two according to the training accuracy of the two local models.</p><p><xref ref-type="fig" rid="F2">Figure 2</xref> illustrates the concept of local modeling. <xref ref-type="fig" rid="F2">Figure 2</xref>a shows an interaction network, with solid green lines representing known interactions, dotted red lines representing known non-interactions, and the dashed black line representing an object pair of which the interaction status is unknown. <xref ref-type="fig" rid="F2">Figure 2</xref>b shows a global model with the locations of the object pairs determined by a Pkernel. The object pair (<italic>v</italic><sub>3</sub>,<italic>v</italic><sub>4</sub>) is on the side with many positive examples, and is predicted to interact. <xref ref-type="fig" rid="F2">Figure 2</xref>c shows a local model for object <italic>v</italic><sub>3</sub>. Object <italic>v</italic><sub>4</sub> is on the side with a negative example, and the pair (<italic>v</italic><sub>3</sub>,<italic>v</italic><sub>4</sub>) is predicted to not interact.</p><p>Since each object has its own local model, subgroup structures can be readily handled by having different kinds of local models for objects in different subgroups.</p></sec></sec><sec id="SEC3"><title>3 OUR PROPOSAL: THE TRAINING SET EXPANSION APPROACH</title><p>Local modeling has been shown to be very competitive in terms of prediction accuracy (Bleakley <italic>et al</italic>., <xref ref-type="bibr" rid="B3">2007</xref>). However, local models can only be learned for objects with a sufficiently large amount of known interactions and non-interactions. When the training sets are small, many objects would not have enough data for training their local models. Overfitting may occur, and in the extreme case where an object has no positive or negative examples, its local model simply cannot be learned. As to be shown in our empirical study presented below, this problem is especially serious when the embedded space is of very high dimension, since very complex models that overfit the data could be formed.</p><p>In the following, we propose ways to tackle this data scarcity issue, while maintaining the flexibility of local modeling. Our idea is to expand the training sets by generating auxiliary training examples. We call it the <italic>training set expansion</italic> approach. Obviously these auxiliary training examples need to be good estimates of the actual interaction status of the corresponding object pairs, for expanding the training sets by wrong examples could further worsen the learned models. We propose two methods for generating reliable examples: <italic>prediction propagation (PP)</italic> and <italic>kernel initialization (KI)</italic>.</p><sec id="SEC3.1"><title>3.1 Prediction propagation</title><p>Suppose <italic>v</italic><sub>1</sub> and <italic>v</italic><sub>2</sub> are two objects, where <italic>v</italic><sub>1</sub> has sufficient training examples while <italic>v</italic><sub>2</sub> does not have. We first train the local model for <italic>v</italic><sub>1</sub>. If the model predicts with high confidence that <italic>v</italic><sub>1</sub> interacts with <italic>v</italic><sub>2</sub>, then <italic>v</italic><sub>1</sub> can later be used as a positive example for training the local model of <italic>v</italic><sub>2</sub>. Alternatively, if the model predicts with high confidence that <italic>v</italic><sub>1</sub> does not interact with <italic>v</italic><sub>2</sub>, <italic>v</italic><sub>1</sub> can be used as a negative example for training the local model of <italic>v</italic><sub>2</sub>.</p><p>This idea is based on the observation that high confidence predictions are more likely correct. For example, if the local models are support vector machines, the predictions for objects far away from the separating hyperplane are more likely correct than those for objects falling in the margin. Therefore, to implement the idea, each prediction should be associated with a confidence value obtained from the local model. When expanding the training sets of other objects, only the most confident predictions should be involved.</p><p>We use support vector regression (SVR) (Smola and Sch&#x000f6;lkopf, <xref ref-type="bibr" rid="B19">2004</xref>) to produce the confidence values. When training the local model of an object <italic>v<sub>i</sub></italic>, the original positive and negative examples of it are given labels of 1 and &#x02212;1, respectively. Then a regression model is constructed to find the best fit. Objects close to the positive examples will receive a regressed value close to 1, and they correspond to objects that are likely to interact with <italic>v<sub>i</sub></italic>. Similarly, objects close to the negative examples will receive a regressed value close to &#x02212;1, and they correspond to objects that are likely to not interact with <italic>v<sub>i</sub></italic>. For other objects, the model is less confident in telling whether they interact with <italic>v<sub>i</sub></italic>. Therefore, the predictions with large positive regressed values can be used as positive examples for training other local models, and those with large negative regressed values can be used as negative examples, where the absolute regressed values represent the confidence.</p><p>Each time we use <italic>p</italic>% of the most confident predictions to expand the training sets of other objects, where the numbers of new positive and negative examples are in proportion to the ratio of positive and negative examples in the original training sets. The parameter <italic>p</italic> is called the training set expansion rate.</p><p>To further improve the approach, we order the training of local models so that objects with more (original and augmented training examples) are trained first, as the models learned from more training examples are generally more reliable. Essentially, this is handling the uneven distribution of training examples by propagating knowledge from the information-rich regions (objects with many training examples) to the information-poor regions (objects with no or few training examples).</p><p>Theoretically PP is related to co-training (Blum and Mitchell, <xref ref-type="bibr" rid="B4">1998</xref>), which uses the most confident predictions of a classifier as additional training examples of other classifiers. The major differences are that in co-training, the classifiers are to make predictions for the same set of data instances, and the classifiers are complementary to each other due to the use of different data features. In contrast, in PP, each model is trained for a different object, and the models are complementary to each other due to the use of different training examples.</p><p>Instead of regression, one can also use support vector classifier (SVC) to determine the confidence values, by measuring the distance of each object from the separating hyperplane. Since we only use the ranks of the confidence values to deduce the auxiliary examples but not their absolute magnitudes, we would expect the results to be similar. We implemented both versions and tested them in our experiments. The two sets of results are indeed comparable, with SVR having slightly higher accuracy on average. In the experiment section, we report the results for SVR, and the results for SVC can be found at the <ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/cgi/content/full/btn602/DC1">Supplementary web site</ext-link>.</p></sec><sec id="SEC3.2"><title>3.2 Kernel initialization</title><p>The PP method is effective when some objects have sufficient input training examples at the beginning to start the generation of auxiliary examples. Yet if all objects have very few input training examples, even the object with the largest training sets may not be able to form a local model that can generate accurate auxiliary examples.</p><p>An alternative way to generate auxiliary training examples is to estimate the interaction status of each pair of objects by its similarity value given by the input kernel. This is in line with the idea of the direct approach, that object pairs with a larger similarity value are more likely to interact. However, instead of thresholding the similarity values to directly give the predictions, they are used only to initialize the training sets for learning the local models. Also, to avoid generating wrong examples, only the ones with the largest and smallest similarity values are used, which correspond to the most confident predictions of the unsupervised direct method.</p><p>For each object, <italic>p</italic>% of the objects with the largest/smallest similarity values given by the kernel are treated as positive/negative training examples in proportion to the positive and negative examples in the original training sets. These auxiliary examples are then combined with the original input examples to train the local models.</p><p>The KI method can be seen as adding a special prior to the object pairs, which assigns a probability of 1 to the most similar pairs of each object and 0 to the most dissimilar pairs. We have also tried normalizing the inner products to the [0,1] range and using them directly as the initial estimate of the confidence of interaction. Yet the performance was not as good as the current method, which could be due to the large variance of confidence values of the object pairs with moderate similarity.</p><p>The two training set expansion methods fall within the class of semi-supervised learning methods (Chapelle <italic>et al</italic>., <xref ref-type="bibr" rid="B6">2006</xref>), which make use of both the training examples and some information about all data instances to learn the model. PP exploits the information about each object pair produced by other local models to help train the current local model. KI utilizes the similarity between objects in the feature space to place soft constraints on the local models, that the objects most similar to the current object should be put in the positive class and those most dissimilar to the current object should be put in the negative class.</p></sec><sec id="SEC3.3"><title>3.3 Combining the two methods (PP+KI)</title><p>Since KI is applied before learning while PP is applied during learning, the two can be used in combination. In some cases it leads to additional performance gain in our experiments.</p></sec></sec><sec id="SEC4"><title>4 PREDICTION ACCURACY</title><sec id="SEC4.1"><title>4.1 Data and setup</title><p>To test the effectiveness of the training set expansion approach, we compared its prediction accuracy with the other approaches on several protein&#x02013;protein interaction networks of the yeast <italic>Saccharomyces cerevisiae</italic> from BioGRID, DIP, MIPS and iPfam. We report below in detail the results on the BioGRID-10 dataset, which includes all yeast physical interactions in BioGRID from small-scale studies that report not more than 10 interactions. The cutoff was chosen so that the network is large enough to have relatively few missing interactions, while small enough to run the different algorithms in reasonable time. We have also tested the methods on a high quality but smaller dataset (DIP_MIPS_iPfam), and a larger dataset (BioGRID-100) that is believed to contain few missing interactions, but is too large that the Pkernel method could not be tested as it caused our machine to run out of memory. The details of the datasets and the complete sets of results can be found at the <ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/cgi/content/full/btn602/DC1">Supplementary web site</ext-link>.</p><p>We tested the performance of the different approaches on various kinds of genomic data features, including phylogenetic profiles, sub-cellular localization and gene expression datasets using the same kernels and parameters as in previous studies (Bleakley <italic>et al</italic>., <xref ref-type="bibr" rid="B3">2007</xref>; Yamanishi <italic>et al</italic>., <xref ref-type="bibr" rid="B27">2005</xref>). We also added in datasets from tandem affinity purification with mass spectrometry using the diffusion kernel, and the integration of all kernels by summing them after normalization, as in previous studies (Bleakley <italic>et al</italic>., <xref ref-type="bibr" rid="B3">2007</xref>; Yamanishi <italic>et al</italic>., <xref ref-type="bibr" rid="B27">2005</xref>). The list of datasets used is shown in <xref ref-type="table" rid="T1">Table 1</xref>.<table-wrap id="T1" position="float"><label>Table 1.</label><caption><p>List of datasets used in the comparison study</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Code</th><th rowspan="1" colspan="1">Data type</th><th rowspan="1" colspan="1">Source</th><th rowspan="1" colspan="1">Kernel</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">phy</td><td rowspan="1" colspan="1">Phylogenetic profiles</td><td rowspan="1" colspan="1">COG v7 (Tatusov <italic>et al</italic>., <xref ref-type="bibr" rid="B22">1997</xref>)</td><td rowspan="1" colspan="1">RBF (&#x003c3; = 3,8)</td></tr><tr><td rowspan="1" colspan="1">loc</td><td rowspan="1" colspan="1">Sub-cellular localization</td><td rowspan="1" colspan="1">(Huh <italic>et al</italic>., <xref ref-type="bibr" rid="B11">2003</xref>)</td><td rowspan="1" colspan="1">Linear</td></tr><tr><td rowspan="1" colspan="1">exp-gasch</td><td rowspan="1" colspan="1">Gene expression (environmental response)</td><td rowspan="1" colspan="1">(Gasch <italic>et al</italic>., <xref ref-type="bibr" rid="B7">2000</xref>)</td><td rowspan="1" colspan="1">RBF (&#x003c3; = 3,8)</td></tr><tr><td rowspan="1" colspan="1">exp-spellman</td><td rowspan="1" colspan="1">Gene expression (cell-cycle)</td><td rowspan="1" colspan="1">(Spellman <italic>et al</italic>., 1998)</td><td rowspan="1" colspan="1">RBF (&#x003c3; = 3,8)</td></tr><tr><td rowspan="1" colspan="1">y2h-ito</td><td rowspan="1" colspan="1">Yeast two-hybrid</td><td rowspan="1" colspan="1">(Ito <italic>et al</italic>., <xref ref-type="bibr" rid="B12">2000</xref>)</td><td rowspan="1" colspan="1">Diffusion (&#x003b2; = 0.01)</td></tr><tr><td rowspan="1" colspan="1">y2h-uetz</td><td rowspan="1" colspan="1">Yeast two-hybrid</td><td rowspan="1" colspan="1">(Uetz <italic>et al</italic>., <xref ref-type="bibr" rid="B24">2000</xref>)</td><td rowspan="1" colspan="1">Diffusion (&#x003b2; = 0.01)</td></tr><tr><td rowspan="1" colspan="1">tap-gavin</td><td rowspan="1" colspan="1">Tandem affinity purification</td><td rowspan="1" colspan="1">(Gavin <italic>et al</italic>., <xref ref-type="bibr" rid="B8">2006</xref>)</td><td rowspan="1" colspan="1">Diffusion (&#x003b2; = 0.01)</td></tr><tr><td rowspan="1" colspan="1">tap-krogan</td><td rowspan="1" colspan="1">Tandem affinity purification</td><td rowspan="1" colspan="1">(Krogan <italic>et al</italic>., <xref ref-type="bibr" rid="B13">2006</xref>)</td><td rowspan="1" colspan="1">Diffusion (&#x003b2; = 0.01)</td></tr><tr><td rowspan="1" colspan="1">int</td><td rowspan="1" colspan="1">Integration</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1">Summation</td></tr></tbody></table><table-wrap-foot><fn><p>Each row corresponds to a dataset from a publication in the Source column, and is turned into a kernel using the function in the Kernel column, as in previous studies (Bleakley <italic>et al</italic>., <xref ref-type="bibr" rid="B3">2007</xref>; Yamanishi <italic>et al</italic>., <xref ref-type="bibr" rid="B26">2004</xref>).</p></fn></table-wrap-foot></table-wrap></p><p>We performed 10-fold cross-validations and used the area under the receiver operator characteristic curve (AUC) as the performance metric. The cross-validations were done in two different modes. In the first mode, as in previous studies (Bleakley <italic>et al</italic>., <xref ref-type="bibr" rid="B3">2007</xref>; Yamanishi <italic>et al</italic>., <xref ref-type="bibr" rid="B26">2004</xref>), the proteins were divided into 10 sets. Each time one set was left out for testing, and the other nine were used for training. All known interactions with both proteins in the training set were used as positive training examples. As required by some of the previous approaches, the sub-network involving the proteins in the training set was assumed completely known (<xref ref-type="fig" rid="F1">Fig. 1</xref>c). As such, all pairs of proteins in the training set not known to interact were regarded as negative examples. All pairs of proteins with exactly one of the two proteins in the training set were used as testing examples (light gray entries in <xref ref-type="fig" rid="F1">Fig. 1</xref>c). Pairs with both proteins not in the training set were not included in the testing sets (dark gray entries in <xref ref-type="fig" rid="F1">Fig. 1</xref>c), as the original local modeling method cannot make such predictions.</p><p>Since all protein pairs in the submatrix are either positive or negative training examples, there are <italic>O</italic>(<italic>n</italic><sup>2</sup>) training examples in each fold. In the Pkernel approach, this translates to a kernel matrix with <italic>O</italic>(<italic>n</italic><sup>4</sup>) elements. It is in the order of 10<sup>12</sup> for 1000 proteins, which is too large to compute and to learn the SVC and SVR models. We, therefore, did not include the Pkernel method in the experiments that used the first mode of cross-validation.</p><p>Since some protein pairs treated as negative examples may actually interact, the reported accuracies may not completely reflect the absolute performance of the methods. However, as the tested methods were subject to the same setting, the results are still good indicators of the relative performance of the approaches.</p><p>In the second mode of cross-validation, we randomly sampled protein pairs not known to interact to form a negative training set with the same size as the positive set, as in previous studies (Ben-Hur and Noble, <xref ref-type="bibr" rid="B2">2005</xref>; Qiu and Noble, <xref ref-type="bibr" rid="B17">2008</xref>). Each of the two sets was divided into 10 subsets, which were used for left-out testing in turn. The main difference between the two modes of cross-validation is that the train-test split is based on proteins in the first mode and protein pairs in the second mode. Since the training examples do not constitute a complete submatrix, the kCCA, kML and em methods cannot be tested in the second mode. The second mode represents the more general case, where the positive and negative training examples do not necessarily form a complete sub-network.</p><p>We used the Matlab code provided by Jean-Philippe Vert for the unsupervised direct, kCCA, kML and em methods with the first mode of cross-validation. We implemented the other methods with both the first and second modes of cross-validation. We observed almost identical accuracy values from the two implementations of the direct approach in the first mode of cross-validation with the negligible differences due only to random train-test splits, which confirms that the reported values from the two sets of code can be fairly compared. For the Pkernel approach, we used the kernel in Equation (1).</p><p>We used the &#x003f5;-SVR and C-SVC implementations of the Java version of libsvm (Chang and Lin, <xref ref-type="bibr" rid="B5">2008</xref>). In a preliminary study, we observed that the prediction accuracy of SVR is not much affected by the value of the termination threshold &#x003f5;, while for both SVR and SVC the performance is quite stable as long as the value of the regularization parameter C is not too small. We thus fixed both parameters to 0.5. For <italic>PP</italic> and <italic>KI</italic>, we used a grid search to determine the value of the training set expansion rate <italic>p</italic>.</p></sec><sec id="SEC4.2"><title>4.2 Results</title><p>Since we use datasets different from the ones used in previous studies, the prediction results are expected to be different. To make sure that our implementations are correct and the testing procedure is valid, we compared our results on the DIP_MIPS_iPfam dataset with those reported in Bleakley <italic>et al</italic>. (<xref ref-type="bibr" rid="B3">2007</xref>) as the size of this dataset is most similar to the one used by them. Our results (available at the <ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/cgi/content/full/btn602/DC1">Supplementary web site</ext-link>) display a lot of similarities with those in Bleakley <italic>et al</italic>. (<xref ref-type="bibr" rid="B3">2007</xref>). For example, in the first mode of cross-validation, local modeling outperformed the other previous approaches when object similarity was defined by phylogenetic profiles and yeast two-hybrid data. Also, the em method had the best performance among all previous approaches with the integrated kernel in both studies. We are thus confident that our results represent a reliable comparison between the methods.</p><p>The comparison results for our main dataset, BioGRID-10, are shown in <xref ref-type="table" rid="T2">Table 2</xref>. In the table PP, KI and PP+KI are written as local+PP, local+KI and local+PP+KI, respectively, to emphasize that the two training set expansion methods are used on top of basic local modeling. Notice that the accuracies in the second mode of cross-validation are in general higher. We examined whether this is due to the presence of self-interactions in the gold-standard set of the second mode of cross-validation but not in the first mode, by removing the self-interactions and re-running the experiments. The results (available at the <ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/cgi/content/full/btn602/DC1">Supplementary web site</ext-link>) suggest that the performance gain due to the removal of self-interactions is too small to explain the performance difference between the two modes of cross-validation. The setting in the second mode may thus correspond to an easier problem. The reported accuracies of the two modes should therefore not to be compared directly.<table-wrap id="T2" position="float"><label>Table 2.</label><caption><p>Prediction accuracy (percentage of AUC) of the different approaches on the BioGRID-10 dataset</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">phy</th><th rowspan="1" colspan="1">loc</th><th rowspan="1" colspan="1">exp-gasch</th><th rowspan="1" colspan="1">exp-spellman</th><th rowspan="1" colspan="1">y2h-ito</th><th rowspan="1" colspan="1">y2h-uetz</th><th rowspan="1" colspan="1">tap-gavin</th><th rowspan="1" colspan="1">tap-krogan</th><th rowspan="1" colspan="1">int</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">Mode 1</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">Direct</td><td rowspan="1" colspan="1">58.04</td><td rowspan="1" colspan="1">66.55</td><td rowspan="1" colspan="1">64.61</td><td rowspan="1" colspan="1">57.41</td><td rowspan="1" colspan="1">51.52</td><td rowspan="1" colspan="1">52.13</td><td rowspan="1" colspan="1">59.37</td><td rowspan="1" colspan="1">61.62</td><td rowspan="1" colspan="1">70.91</td></tr><tr><td rowspan="1" colspan="1">kCCA</td><td rowspan="1" colspan="1">65.80</td><td rowspan="1" colspan="1">63.86</td><td rowspan="1" colspan="1">68.98</td><td rowspan="1" colspan="1">65.10</td><td rowspan="1" colspan="1">50.89</td><td rowspan="1" colspan="1">50.48</td><td rowspan="1" colspan="1">57.56</td><td rowspan="1" colspan="1">51.85</td><td rowspan="1" colspan="1">80.98</td></tr><tr><td rowspan="1" colspan="1">kML</td><td rowspan="1" colspan="1">63.87</td><td rowspan="1" colspan="1">68.10</td><td rowspan="1" colspan="1">69.67</td><td rowspan="1" colspan="1">68.99</td><td rowspan="1" colspan="1">52.76</td><td rowspan="1" colspan="1">53.85</td><td rowspan="1" colspan="1">60.86</td><td rowspan="1" colspan="1">57.69</td><td rowspan="1" colspan="1">73.47</td></tr><tr><td rowspan="1" colspan="1">em</td><td rowspan="1" colspan="1">71.22</td><td rowspan="1" colspan="1">75.14</td><td rowspan="1" colspan="1">67.53</td><td rowspan="1" colspan="1">64.96</td><td rowspan="1" colspan="1">55.90</td><td rowspan="1" colspan="1">53.13</td><td rowspan="1" colspan="1">63.74</td><td rowspan="1" colspan="1">68.20</td><td rowspan="1" colspan="1">81.65</td></tr><tr><td rowspan="1" colspan="1">Local</td><td rowspan="1" colspan="1">71.67</td><td rowspan="1" colspan="1">71.41</td><td rowspan="1" colspan="1">72.66</td><td rowspan="1" colspan="1">70.63</td><td rowspan="1" colspan="1">67.27</td><td rowspan="1" colspan="1">67.27</td><td rowspan="1" colspan="1">64.60</td><td rowspan="1" colspan="1">67.48</td><td rowspan="1" colspan="1">75.65</td></tr><tr><td rowspan="1" colspan="1">Local+PP</td><td rowspan="1" colspan="1">73.89</td><td rowspan="1" colspan="1">75.25</td><td rowspan="1" colspan="1">77.43</td><td rowspan="1" colspan="1">75.35</td><td rowspan="1" colspan="1">71.60</td><td rowspan="1" colspan="1">71.51</td><td rowspan="1" colspan="1">74.62</td><td rowspan="1" colspan="1">71.39</td><td rowspan="1" colspan="1">83.63</td></tr><tr><td rowspan="1" colspan="1">Local+KI</td><td rowspan="1" colspan="1">71.68</td><td rowspan="1" colspan="1">71.42</td><td rowspan="1" colspan="1">75.89</td><td rowspan="1" colspan="1">70.96</td><td rowspan="1" colspan="1">69.40</td><td rowspan="1" colspan="1">69.05</td><td rowspan="1" colspan="1">70.53</td><td rowspan="1" colspan="1">72.03</td><td rowspan="1" colspan="1">81.74</td></tr><tr><td rowspan="1" colspan="1">Local+PP+KI</td><td rowspan="1" colspan="1">72.40</td><td rowspan="1" colspan="1">75.19</td><td rowspan="1" colspan="1">77.41</td><td rowspan="1" colspan="1">73.81</td><td rowspan="1" colspan="1">70.44</td><td rowspan="1" colspan="1">70.57</td><td rowspan="1" colspan="1">73.59</td><td rowspan="1" colspan="1">72.64</td><td rowspan="1" colspan="1">83.59</td></tr><tr><td rowspan="1" colspan="1">Mode 2</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">Direct</td><td rowspan="1" colspan="1">59.99</td><td rowspan="1" colspan="1">67.81</td><td rowspan="1" colspan="1">66.18</td><td rowspan="1" colspan="1">59.22</td><td rowspan="1" colspan="1">54.02</td><td rowspan="1" colspan="1">54.64</td><td rowspan="1" colspan="1">62.28</td><td rowspan="1" colspan="1">63.69</td><td rowspan="1" colspan="1">72.34</td></tr><tr><td rowspan="1" colspan="1">Pkernel</td><td rowspan="1" colspan="1">72.98</td><td rowspan="1" colspan="1">69.84</td><td rowspan="1" colspan="1">78.61</td><td rowspan="1" colspan="1">77.30</td><td rowspan="1" colspan="1">57.01</td><td rowspan="1" colspan="1">54.65</td><td rowspan="1" colspan="1">71.16</td><td rowspan="1" colspan="1">70.36</td><td rowspan="1" colspan="1">87.34</td></tr><tr><td rowspan="1" colspan="1">Local</td><td rowspan="1" colspan="1">76.89</td><td rowspan="1" colspan="1">78.73</td><td rowspan="1" colspan="1">79.72</td><td rowspan="1" colspan="1">77.32</td><td rowspan="1" colspan="1">72.93</td><td rowspan="1" colspan="1">72.89</td><td rowspan="1" colspan="1">68.81</td><td rowspan="1" colspan="1">73.15</td><td rowspan="1" colspan="1">82.82</td></tr><tr><td rowspan="1" colspan="1">Local+PP</td><td rowspan="1" colspan="1">77.71</td><td rowspan="1" colspan="1">80.71</td><td rowspan="1" colspan="1">82.56</td><td rowspan="1" colspan="1">80.62</td><td rowspan="1" colspan="1">74.74</td><td rowspan="1" colspan="1">74.41</td><td rowspan="1" colspan="1">76.36</td><td rowspan="1" colspan="1">75.12</td><td rowspan="1" colspan="1">88.78</td></tr><tr><td rowspan="1" colspan="1">Local+KI</td><td rowspan="1" colspan="1">76.76</td><td rowspan="1" colspan="1">78.73</td><td rowspan="1" colspan="1">80.62</td><td rowspan="1" colspan="1">76.44</td><td rowspan="1" colspan="1">73.39</td><td rowspan="1" colspan="1">72.76</td><td rowspan="1" colspan="1">72.42</td><td rowspan="1" colspan="1">76.22</td><td rowspan="1" colspan="1">86.12</td></tr><tr><td rowspan="1" colspan="1">Local+PP+KI</td><td rowspan="1" colspan="1">77.45</td><td rowspan="1" colspan="1">80.57</td><td rowspan="1" colspan="1">81.93</td><td rowspan="1" colspan="1">78.92</td><td rowspan="1" colspan="1">74.14</td><td rowspan="1" colspan="1">74.01</td><td rowspan="1" colspan="1">75.59</td><td rowspan="1" colspan="1">76.59</td><td rowspan="1" colspan="1">88.56</td></tr></tbody></table><table-wrap-foot><fn><p>The best approach for each kernel and each mode of cross-validation is in bold face.</p></fn></table-wrap-foot></table-wrap></p><p>From the table, the advantages of the training set expansion methods over basic local modeling are clearly seen. In all cases, the accuracy of local modeling was improved by at least one of the expansion methods, and in many cases all three combinations (PP, KI and PP+KI) performed better than basic local modeling. With training set expansion, local modeling outperformed all the other approaches in all nine datasets.</p><p>Inspecting the performance of local modeling without training set expansion, it is observed that although local modeling usually outperformed the other previous methods, its performance with the integration kernel was unsatisfactory. This is probably due to overfitting. When kernels are summed, the resulting embedded space is the direct product of the ones defined by the kernels (Sch&#x000f6;lkopf <italic>et al</italic>., <xref ref-type="bibr" rid="B18">2004</xref>). Since the final kernel used for the integrated dataset is a summation of eight kernels, the corresponding embedded space is of very high dimension. With the high flexibility and the lack of training data, the models produced by basic local modeling were probably overfitted. In contrast, with the auxiliary training examples, the training set expansion methods appear to have largely overcome the problem.</p><p>Comparing the two training set expansion methods, in most cases PP resulted in a larger performance gain. This is reasonable since the input training examples were used in this method, but not in KI.</p><p>To better understand how the two training set expansion methods improve the predictions, we sub-sampled the gold-standard network at different sizes, and compared the performance of local modeling with and without training set expansion using the second mode of cross-validation. The results for two of the kernels are shown in <xref ref-type="fig" rid="F3">Figure 3</xref>, while the whole set of results can be found at the <ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/cgi/content/full/btn602/DC1">Supplementary web site</ext-link>.<fig id="F3" position="float"><label>Fig. 3.</label><caption><p>Prediction accuracy at different gold-standard set sizes. (<bold>a</bold>) Using int kernel. (<bold>b</bold>) Using exp-gasch kernel.</p></caption><graphic xlink:href="btn602f3"/></fig></p><p>In general training set expansion improved the accuracy the most with moderate gold-standard set sizes, at around 3000 interactions. For PP, this is expected since when the training set was too small, the local models were too inaccurate that even the most confident predictions could still be wrong, which made propagation undesirable. On the other hand, when there were many training examples, there were few missing interactions, so that the augmented training examples became relatively less important. The latter argument also applies to KI, that it resulted in larger performance gain when the gold-standard set was not too large. However, it is surprising to see that using the integrated kernel (<xref ref-type="fig" rid="F3">Fig. 3</xref>a), KI resulted in a drop in accuracy when there were only 500 interactions. Since the kernel remained the same at different gold-standard set sizes, one would expect to see a stable performance gain for KI regardless of the size of the gold-standard set. This stable performance gain is indeed observed when the Gasch or phylogenetic profile kernel was used (<xref ref-type="fig" rid="F3">Fig. 3</xref>b and Fig. S1). In contrast, PP, being dependent on the raw accuracy of local modeling, performed poorly when there were only 500 interactions for all nine datasets. This suggests that when the dataset is expected to contain a lot of missing interactions, KI is potentially more useful, but it also depends on the feature used in learning. On the other hand, PP is more useful when the dataset contains enough interactions for local modeling to achieve a reasonable accuracy.</p></sec></sec><sec id="SEC5"><title>5 ANALYSIS</title><p>With the observed performance gain of training set expansion, we would like to know what kind of correct predictions could it make that were ranked low by other methods. To answer the question, for each known interaction in the gold-standard positive set of BioGRID-10, we computed the rank of it in the predictions made by local+PP and local+KI using the integrated kernel in the first mode of cross-validation. Then we computed the highest rank of the interaction given by kCCA, kML, em and Local, and calculated the difference between the two. If the former is much higher than the latter (i.e. there is a large rank difference), then the interaction is uniquely identified by training set expansion but not by any of the four other methods.</p><p>Among the 2880 interactions in the gold-standard set that were tested by both local+PP and the four comparing methods, the ranks of 2121 of them are higher in the predictions made by local+PP than in any of the four methods. For each of them, we computed the minimum degree (number of known interactions in the gold-standard set) of the two interacting proteins as an indicator of the number of available training examples for the pair. Then we correlated the minimum degree with the rank difference. The resulting graph (<xref ref-type="fig" rid="F4">Fig. 4</xref>) shows a significant negative correlation (Spearman correlation =&#x02212;0.38, P&#x0003c;10<sup>&#x02212;16</sup>), which confirms that the correct predictions made by local+PP that were missed by the other four methods correspond to the protein pairs with few known examples. We have also tested the average degree instead of the minimum, and the Pearson correlation instead of Spearman correlation. The results all lead to the same conclusion (Fig. S2).<fig id="F4" position="float"><label>Fig. 4.</label><caption><p>Correlating the number of gold-standard examples and the rank difference between local+PP and the four methods.</p></caption><graphic xlink:href="btn602f4"/></fig></p><p>A concrete example of a gold-standard interaction predicted by local+PP, but ranked low by the four methods is the one between SEC11 and SPC1. They are both subunits of the signal peptidase complex (SPC), and are reported to interact in BioGRID according to multiple sources. In the BioGRID-10 dataset, SPC1 is the only known interaction partner of SEC11, while SPC1 only has one other known interaction (with SBH2). The extremely small numbers of known examples make it difficult to identify this interaction. Indeed, the best of the four previous methods could only give it a rank at the 74th percentile, indicating that they were all unable to identify this interaction. In contrast, local+PP was able to rank it at the top 7th percentile, i.e. with a rank difference of 67% (<xref ref-type="fig" rid="F4">Fig. 4</xref>). This example illustrates that interactions with very few known examples, while easily missed by the previous methods, could be identified by using PP.</p><p>For local+KI, among the 2880 commonly tested gold-standard interactions, 2025 received a higher rank from it than from any of the four comparing methods. Again, there is a negative correlation between the rank difference and the minimum degree and average degree (Fig. S2), which shows that KI is also able to predict interactions for proteins with few training examples. In addition, there is a positive correlation with moderate significance between the rank difference and the similarity between the interacting proteins according to the kernel (Fig. S2, Spearman correlation =0.04, <italic>P</italic> = 0.04), which is expected as the KI method uses protein pairs with high similarity as auxiliary positive training examples. Interestingly, for local+PP, a negative correlation is observed between the rank difference and protein similarity (Figure S2), which suggests that the PP method is able to identify non-trivial interactions, where the two interacting proteins are not necessarily similar according to the kernel.</p></sec><sec sec-type="discussion" id="SEC6"><title>6 DISCUSSION</title><p>Training set expansion is a general concept that can also be applied to other problems and used with other learning methods. The learning method is not required to make very accurate predictions for all object pairs, and the data features do not need to define an object similarity that is very consistent with the interactions. As long as the <italic>most confident</italic> predictions are likely correct, PP is useful, and as long as the most similar objects are likely to interact and the most dissimilar objects are unlikely to interact, KI is useful. In many biological applications at least one of these requirements is satisfied.</p></sec><sec sec-type="conclusions" id="SEC7"><title>7 CONCLUSION</title><p>In this article, we have described the semi-supervised training set expansion methods prediction propagation (PP) and kernel initialization (KI) that alleviate the overfitting problem of local modeling while preserving its modeling flexibility. The PP method learns from the information-rich regions, and uses the learned knowledge to help the information-poor regions. It is conceptually related to co-training. The KI method treats the most similar and dissimilar object pairs as positive and negative training examples, respectively. Prediction results on several high quality protein&#x02013;protein interaction networks from yeast show great improvements over basic local modeling by these methods, and the resulting algorithms outperformed all other methods using any of the nine genomic features. We have also identified cases that clearly illustrate the effectiveness of the training set expansion methods in helping the construction of local models. The concept of training set expansion can be applied to other problems with small or uneven training sets.</p></sec></body><back><ack><title>ACKNOWLEDGEMENTS</title><p>We would like to thank Kevin Bleakley for helpful discussions and Jean-Philippe Vert for providing the Matlab code. We would also like to thank the Yale University Biomedical High Performance Computing Center.</p><p><italic>Funding</italic>: National Institute of Health; the AL Williams Professorship.</p><p><italic>Conflict of Interest</italic>: none declared.</p></ack><ref-list><title>REFERENCES</title><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Aizerman</surname><given-names>M</given-names></name><etal/></person-group><article-title>Theoretical foundations of the potential function method in pattern recognition learning.</article-title><source>Automat. Rem. Contr.</source><year>1964</year><volume>25</volume><fpage>821</fpage><lpage>837</lpage></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ben-Hur</surname><given-names>A</given-names></name><name><surname>Noble</surname><given-names>WS</given-names></name></person-group><article-title>Kernel methods for predicting protein-protein interactions.</article-title><source>Bioinformatics</source><year>2005</year><volume>21</volume><issue>Suppl. 1</issue><fpage>i38</fpage><lpage>i46</lpage><pub-id pub-id-type="pmid">15961482</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bleakley</surname><given-names>K</given-names></name><etal/></person-group><article-title>Supervised reconstruction of biological networks with local models.</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><fpage>i57</fpage><lpage>i65</lpage><pub-id pub-id-type="pmid">17646345</pub-id></citation></ref><ref id="B4"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Blum</surname><given-names>A</given-names></name><name><surname>Mitchell</surname><given-names>T</given-names></name></person-group><article-title>Combining labeled and unlabeled data with co-training.</article-title><source>The Eleventh Annual Workshop on Computational Learning Theory.</source><year>1998</year><volume>92</volume><publisher-loc>San Francisco, California, USA</publisher-loc><publisher-name>Morgan Kaufmann Publishers</publisher-name><lpage>100</lpage></citation></ref><ref id="B5"><citation citation-type="web"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>C-C</given-names></name><name><surname>Lin</surname><given-names>C-J</given-names></name></person-group><article-title>LIBSVM: a library for support vector machine.</article-title><year>2008</year><access-date>(last accessed date on October 2008)</access-date><comment>Available at <ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf</ext-link></comment></citation></ref><ref id="B6"><citation citation-type="book"><person-group person-group-type="editor"><name><surname>Chapelle</surname><given-names>O</given-names></name><etal/></person-group><source>Semi-Supervised Learning.</source><year>2006</year><publisher-loc>Cambridge, Massachusetts, USA</publisher-loc><publisher-name>MIT Press</publisher-name></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gasch</surname><given-names>AP</given-names></name><etal/></person-group><article-title>Genomic expression programs in the response of yeast cells to environmental changes.</article-title><source>Mol. Biol. Cell</source><year>2000</year><volume>11</volume><fpage>4241</fpage><lpage>4257</lpage><pub-id pub-id-type="pmid">11102521</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gavin</surname><given-names>A-C</given-names></name><etal/></person-group><article-title>Proteome survey reveals modularity of the yeast cell machinery.</article-title><source>Nature</source><year>2006</year><volume>440</volume><fpage>631</fpage><lpage>636</lpage><pub-id pub-id-type="pmid">16429126</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Giaever</surname><given-names>G</given-names></name><etal/></person-group><article-title>Functional profiling of the Saccharomyces cerevisiae genome.</article-title><source>Nature</source><year>2002</year><volume>418</volume><fpage>387</fpage><lpage>391</lpage><pub-id pub-id-type="pmid">12140549</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>H</given-names></name><etal/></person-group><article-title>Where have all the interactions gone? Estimating the coverage of two-hybrid protein interaction maps.</article-title><source>PLoS Comput. Biol</source><year>2007</year><volume>3</volume><fpage>e214</fpage><pub-id pub-id-type="pmid">18039026</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Huh</surname><given-names>W-K</given-names></name><etal/></person-group><article-title>Global analysis of protein localization in budding yeast.</article-title><source>Nature</source><year>2003</year><volume>425</volume><fpage>686</fpage><lpage>691</lpage><pub-id pub-id-type="pmid">14562095</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>T</given-names></name><etal/></person-group><article-title>Toward a protein-protein interaction map of the budding yeast: A comprehensive system to examine two-hybrid interactions in all possible combinations between the yeast proteins.</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2000</year><volume>97</volume><fpage>1143</fpage><lpage>1147</lpage><pub-id pub-id-type="pmid">10655498</pub-id></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Krogan</surname><given-names>NJ</given-names></name><etal/></person-group><article-title>Global landscape of protein complexes in the yeast Saccharomyces cerevisiae.</article-title><source>Nature</source><year>2006</year><volume>440</volume><fpage>637</fpage><lpage>643</lpage><pub-id pub-id-type="pmid">16554755</pub-id></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lanckriet</surname><given-names>GRG</given-names></name><etal/></person-group><article-title>A statistical framework for genomic data fusion.</article-title><source>Bioinformatics</source><year>2004</year><volume>20</volume><fpage>2626</fpage><lpage>2635</lpage><pub-id pub-id-type="pmid">15130933</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mercer</surname><given-names>J</given-names></name></person-group><article-title>Functions of positive and negative type, and their connection with the theory of integral equations.</article-title><source>Philos. Trans. R. Soc. Lond.</source><year>1909</year><volume>209</volume><fpage>415</fpage><lpage>446</lpage></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname><given-names>CL</given-names></name><name><surname>Troyanskaya</surname><given-names>OG</given-names></name></person-group><article-title>Context-sensitive data integration and prediction of biological networks.</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><fpage>2322</fpage><lpage>2330</lpage><pub-id pub-id-type="pmid">17599939</pub-id></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>J</given-names></name><name><surname>Noble</surname><given-names>S</given-names></name></person-group><article-title>Predicting co-complexed protein pairs from heterogeneous data.</article-title><source>PLoS Comput. Biol</source><year>2008</year><volume>4</volume><fpage>e1000054</fpage><pub-id pub-id-type="pmid">18421371</pub-id></citation></ref><ref id="B18"><citation citation-type="book"><person-group person-group-type="editor"><name><surname>Sch&#x000f6;lkopf</surname><given-names>B</given-names></name><etal/></person-group><source>Kernel Methods in Computational Biology.</source><year>2004</year><publisher-loc>Cambridge, Massachusetts, USA</publisher-loc><publisher-name>MIT Press</publisher-name></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Smola</surname><given-names>AJ</given-names></name><name><surname>Sch&#x000f6;lkopf</surname><given-names>B</given-names></name></person-group><article-title>A tutorial on support vector regression.</article-title><source>Stat. Comput.</source><year>2004</year><volume>14</volume><fpage>199</fpage><lpage>222</lpage></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Spellman</surname><given-names>PT</given-names></name><etal/></person-group><article-title>Comprehensive identification of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization.</article-title><source>Mol. Biol. Cell</source><year>1998</year><volume>9</volume><fpage>3273</fpage><lpage>3297</lpage><pub-id pub-id-type="pmid">9843569</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Stark</surname><given-names>C</given-names></name><etal/></person-group><article-title>BioGRID: a general repository for interaction datasets.</article-title><source>Nucleic Acids Res.</source><year>2006</year><volume>34</volume><fpage>D535</fpage><lpage>D539</lpage><pub-id pub-id-type="pmid">16381927</pub-id></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tatusov</surname><given-names>RL</given-names></name><etal/></person-group><article-title>A genomic perspective on protein families.</article-title><source>Science</source><year>1997</year><volume>278</volume><fpage>631</fpage><lpage>637</lpage><pub-id pub-id-type="pmid">9381173</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tsuda</surname><given-names>K</given-names></name><etal/></person-group><article-title>The algorithm for kernel matrix completion with auxiliary data.</article-title><source>J. Mach. Learn. Res.</source><year>2003</year><volume>4</volume><fpage>67</fpage><lpage>81</lpage></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Uetz</surname><given-names>P</given-names></name><etal/></person-group><article-title>A comprehensive analysis of protein-protein interactions in Saccharomyces cerevisiae.</article-title><source>Nature</source><year>2000</year><volume>403</volume><fpage>623</fpage><lpage>627</lpage><pub-id pub-id-type="pmid">10688190</pub-id></citation></ref><ref id="B25"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Vert</surname><given-names>J-P</given-names></name><name><surname>Yamanishi</surname><given-names>Y</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Saul</surname><given-names>LK</given-names></name><etal/></person-group><article-title>Supervised graph inference.</article-title><source>Advances in Neural Information Processing Systems 17.</source><year>2005</year><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>1433</fpage><lpage>1440</lpage></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yamanishi</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Protein network inference from multiple genomic data: a supervised approach.</article-title><source>Bioinformatics</source><year>2004</year><volume>20</volume><issue>Suppl. 1</issue><fpage>i363</fpage><lpage>i370</lpage><pub-id pub-id-type="pmid">15262821</pub-id></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yamanishi</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Supervised enzyme network inference from the integration of genomic data and chemical information.</article-title><source>Bioinformatics</source><year>2005</year><volume>21</volume><issue>Suppl. 1</issue><fpage>i468</fpage><lpage>i477</lpage><pub-id pub-id-type="pmid">15961492</pub-id></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>H</given-names></name><etal/></person-group><article-title>High-quality binary protein interaction map of the yeast interactome network.</article-title><source>Science</source><year>2008</year><volume>322</volume><fpage>104</fpage><lpage>110</lpage><pub-id pub-id-type="pmid">18719252</pub-id></citation></ref></ref-list></back></article>