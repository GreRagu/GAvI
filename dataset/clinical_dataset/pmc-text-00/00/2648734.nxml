<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19208122</article-id><article-id pub-id-type="pmc">2648734</article-id><article-id pub-id-type="publisher-id">1471-2105-10-S1-S22</article-id><article-id pub-id-type="doi">10.1186/1471-2105-10-S1-S22</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Using random forest for reliable classification and cost-sensitive learning for medical diagnosis</article-title></title-group><contrib-group><contrib id="A1" equal-contrib="yes" contrib-type="author"><name><surname>Yang</surname><given-names>Fan</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>yang@xmu.edu.cn</email></contrib><contrib id="A2" equal-contrib="yes" contrib-type="author"><name><surname>Wang</surname><given-names>Hua-zhen</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>Hzwang@xmu.edu.cn</email></contrib><contrib id="A3" corresp="yes" contrib-type="author"><name><surname>Mi</surname><given-names>Hong</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>mihong2017@vip.sina.com.cn</email></contrib><contrib id="A4" contrib-type="author"><name><surname>Lin</surname><given-names>Cheng-de</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>cdlin@xmu.edu.cn</email></contrib><contrib id="A5" contrib-type="author"><name><surname>Cai</surname><given-names>Wei-wen</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>wcai@bcm.edu</email></contrib></contrib-group><aff id="I1"><label>1</label>Automation Department, Xiamen University, Xiamen, 361005, P.R.C</aff><aff id="I2"><label>2</label>Department of Molecular and Human Genetics, Baylor College of Medicine, Houston, TX 77030, USA</aff><pub-date pub-type="collection"><year>2009</year></pub-date><pub-date pub-type="epub"><day>30</day><month>1</month><year>2009</year></pub-date><volume>10</volume><issue>Suppl 1</issue><supplement><named-content content-type="supplement-title">Selected papers from the Seventh Asia-Pacific Bioinformatics Conference (APBC 2009)</named-content><named-content content-type="supplement-editor">Michael Q Zhang, Michael S Waterman and Xuegong Zhang</named-content></supplement><fpage>S22</fpage><lpage>S22</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10/S1/S22"/><permissions><copyright-statement>Copyright &#x000a9; 2009 Yang et al; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2009</copyright-year><copyright-holder>Yang et al; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an open access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>               Yang               Fan                              yang@xmu.edu.cn            </dc:author><dc:title>            Using random forest for reliable classification and cost-sensitive learning for medical diagnosis         </dc:title><dc:date>2009</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 10(Suppl 1): S22-. (2009)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2009)10:Suppl 1&#x0003c;S22&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>Most machine-learning classifiers output label predictions for new instances without indicating how reliable the predictions are. The applicability of these classifiers is limited in critical domains where incorrect predictions have serious consequences, like medical diagnosis. Further, the default assumption of equal misclassification costs is most likely violated in medical diagnosis.</p></sec><sec><title>Results</title><p>In this paper, we present a modified random forest classifier which is incorporated into the conformal predictor scheme. A conformal predictor is a transductive learning scheme, using Kolmogorov complexity to test the randomness of a particular sample with respect to the training sets. Our method show well-calibrated property that the performance can be set prior to classification and the accurate rate is exactly equal to the predefined confidence level. Further, to address the cost sensitive problem, we extend our method to a label-conditional predictor which takes into account different costs for misclassifications in different class and allows different confidence level to be specified for each class. Intensive experiments on benchmark datasets and real world applications show the resultant classifier is well-calibrated and able to control the specific risk of different class.</p></sec><sec><title>Conclusion</title><p>The method of using RF outlier measure to design a nonconformity measure benefits the resultant predictor. Further, a label-conditional classifier is developed and turn to be an alternative approach to the cost sensitive learning problem that relies on label-wise predefined confidence level. The target of minimizing the risk of misclassification is achieved by specifying the different confidence level for different class.</p></sec></abstract><conference><conf-date>13&#x02013;16 January 2009</conf-date><conf-name>The Seventh Asia Pacific Bioinformatics Conference (APBC 2009)</conf-name><conf-loc>Beijing, China</conf-loc></conference></article-meta></front><body><sec><title>Background</title><p>Most machine-learning classifiers output predictions for new instances without indicating how reliable the predictions are. The application of these classifiers is limited in the domains where incorrect predictions have serious consequences. Medical practitioners need a reliable assessment of risk of error for individual cases [<xref ref-type="bibr" rid="B1">1</xref>]. Thus, given the prediction tailed with a corresponding confidence value, a system can decide whether it is safe to classify. The recently introduced Conformal Predictor (CP) [<xref ref-type="bibr" rid="B2">2</xref>-<xref ref-type="bibr" rid="B5">5</xref>] is a promising framework that produces prediction coupled with confidence estimation. The exploiters advanced a welcome preference for formal relationship among Kolmogorov complexity, universal Turing Machines and strict minimum message length (MML). They assumed the transductive prediction as a randomness test which returns nonconformity scores closely associated with the property of the iid distribution (identically and independent distribution) governing all of the examples. When classifying a new instance, CP assigns a p-value for each given artificial label to approximate the confidence level of prediction. CP is more than a reliable classifier of which the most novel and valuable feature is hedging prediction, i.e., the performance can be set prior to classification and the prediction is well-calibrated that the accurate rate is exactly equal to the predefined confidence level. It is impressive to see its superiority over the Bayesian approach which often relies on strong underlying assumptions. In this paper, we use a random forest outlier measure to design the nonconformity score and develop a modified random forest classifier.</p><p>Since reports from both academia and practice indicate that the default assumption of equal misclassification costs is most likely violated [<xref ref-type="bibr" rid="B6">6</xref>], the natural desiderata is extending CP to label-wise CP, which takes into account different costs for misclassification errors of different class and allows different confidence level to be specified for different classification of an instance. In this paper, we investigate the method to extend CP to label-conditional CP, which can solve the non-uniform costs of errors in classification.</p><p>Consider a classification problem E: The reality outputs examples Z<sup>(n-1) </sup>= {(x<sub>1</sub>, y<sub>1</sub>),..., (x<sub>n-1</sub>, y<sub>n-1</sub>)} &#x02208; X &#x000d7; Y and an unlabeled test instance x<sub>n</sub>, where X denotes a measurable space of possible instances x<sub>i</sub>&#x02208; X, i = 1, 2,... n - 1,...; Y denotes a measurable space of possible labels, y<sub>i</sub>&#x02208; Y, i = 1,2,... n - 1,...; the example space is represented as Z = X &#x000d7; Y. We assume that each instance is generated by the same unknown probability distribution P over Z, which satisfies the exchangeability assumption.</p><sec><title>Conformal predictor (CP)</title><p>CP is designed to introduce confidence estimation to the machine learning algorithms. It generalizes its framework from the iid assumption to exchangeability which omits the information about examples order.</p><p>To construct a prediction set for an unlabeled instance x<sub>n</sub>, CP operates in a transductive manner and online setting. Each possible label is tried as a label for x<sub>n</sub>. In each try we form an artificial sequence (x<sub>1</sub>, y<sub>1</sub>),..., (x<sub>n</sub>, y), then we measure how likely it is that the resulting sequence is generated by the unknown distribution P and how nonconforming x<sub>n </sub>is with respect to other available examples.</p><p>Given the classification problem E, The function A<sub>n</sub>: Z<sup>(n-1) </sup>&#x000d7; z<sub>n </sub>&#x02192; R is a nonconformity measure if, for any n &#x02208; N,</p><p><disp-formula><italic>&#x003b1;</italic><sub>i </sub>:= A<sub>n</sub>(z<sub>i</sub>, &#x027e6;<sub>1</sub>,..., z<sub>i-1</sub>, z<sub>i+1</sub>,..., z<sub>n</sub>&#x027e7;)</disp-formula></p><p><disp-formula>i = 1,..., n - 1</disp-formula></p><p><disp-formula id="bmcM1"><label>(1)</label><italic>&#x003b1;</italic><sub>n </sub>:= A<sub>n</sub>(z<sub>n</sub>, &#x027e6;z<sub>1</sub>,..., z<sub>n-1</sub>&#x027e7;)</disp-formula></p><p>where [&#x000b7;] is a "bag" in which the elements are irrelevant according to their order. The symbol <italic>&#x003b1; </italic>denotes sample nonconformity score: the larger <italic>&#x003b1;</italic><sub>i </sub>is, the stranger z<sub>i </sub>is corresponding to the distribution. In short, a nonconformity measure is characterized as a measurable kernel that maps Z to R while the value of <italic>&#x003b1;</italic><sub>i </sub>is irrelevant with the order of z<sub>i </sub>in sequence.</p><p>For confidence level 1 - <italic>&#x003b5; </italic>(<italic>&#x003b5; </italic>is the significance level) and any n &#x02208; N, a conformal predictor is defined as:</p><p><disp-formula id="bmcM2"><label>(2)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" name="1471-2105-10-S1-S22-i1" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:msup>                              <mml:mi>&#x00393;</mml:mi>                              <mml:mi>&#x003b5;</mml:mi>                           </mml:msup>                           <mml:mo stretchy="false">(</mml:mo>                           <mml:msub>                              <mml:mtext>z</mml:mtext>                              <mml:mn>1</mml:mn>                           </mml:msub>                           <mml:mo>,</mml:mo>                           <mml:mn>...</mml:mn>                           <mml:mo>,</mml:mo>                           <mml:msub>                              <mml:mtext>z</mml:mtext>                              <mml:mrow>                                 <mml:mtext>n</mml:mtext>                                 <mml:mo>&#x02212;</mml:mo>                                 <mml:mn>1</mml:mn>                              </mml:mrow>                           </mml:msub>                           <mml:mo>,</mml:mo>                           <mml:msub>                              <mml:mtext>x</mml:mtext>                              <mml:mtext>n</mml:mtext>                           </mml:msub>                           <mml:mo stretchy="false">)</mml:mo>                           <mml:mo>=</mml:mo>                           <mml:mrow>                              <mml:mo>{</mml:mo>                              <mml:mrow>                                 <mml:mtext>y</mml:mtext>                                 <mml:mo>&#x02208;</mml:mo>                                 <mml:mtext>Y</mml:mtext>                                 <mml:mo>:</mml:mo>                                 <mml:msub>                                    <mml:mtext>p</mml:mtext>                                    <mml:mtext>y</mml:mtext>                                 </mml:msub>                                 <mml:mo>=</mml:mo>                                 <mml:mfrac>                                    <mml:mrow>                                       <mml:mo>|</mml:mo>                                       <mml:mo>{</mml:mo>                                       <mml:mtext>i</mml:mtext>                                       <mml:mo>=</mml:mo>                                       <mml:mn>1</mml:mn>                                       <mml:mo>,</mml:mo>                                       <mml:mn>...</mml:mn>                                       <mml:mo>,</mml:mo>                                       <mml:mtext>n</mml:mtext>                                       <mml:mo>:</mml:mo>                                       <mml:msub>                                          <mml:mi>&#x003b1;</mml:mi>                                          <mml:mtext>i</mml:mtext>                                       </mml:msub>                                       <mml:mo>&#x02265;</mml:mo>                                       <mml:msub>                                          <mml:mi>&#x003b1;</mml:mi>                                          <mml:mtext>n</mml:mtext>                                       </mml:msub>                                       <mml:mo>}</mml:mo>                                       <mml:mo>|</mml:mo>                                    </mml:mrow>                                    <mml:mtext>n</mml:mtext>                                 </mml:mfrac>                                 <mml:mo>&#x0003e;</mml:mo>                                 <mml:mi>&#x003b5;</mml:mi>                              </mml:mrow>                              <mml:mo>}</mml:mo>                           </mml:mrow>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>A smoothed conformal predictor (smoothed CP) is defined as:</p><p><disp-formula id="bmcM3"><label>(3)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M2" name="1471-2105-10-S1-S22-i2" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:msup>                              <mml:mi>&#x00393;</mml:mi>                              <mml:mrow>                                 <mml:mi>&#x003b5;</mml:mi>                                 <mml:mo>,</mml:mo>                                 <mml:msub>                                    <mml:mi>&#x003c4;</mml:mi>                                    <mml:mtext>n</mml:mtext>                                 </mml:msub>                              </mml:mrow>                           </mml:msup>                           <mml:mo stretchy="false">(</mml:mo>                           <mml:msub>                              <mml:mtext>z</mml:mtext>                              <mml:mn>1</mml:mn>                           </mml:msub>                           <mml:mo>,</mml:mo>                           <mml:mn>...</mml:mn>                           <mml:mo>,</mml:mo>                           <mml:msub>                              <mml:mtext>z</mml:mtext>                              <mml:mrow>                                 <mml:mtext>n</mml:mtext>                                 <mml:mo>&#x02212;</mml:mo>                                 <mml:mn>1</mml:mn>                              </mml:mrow>                           </mml:msub>                           <mml:mo>,</mml:mo>                           <mml:msub>                              <mml:mtext>x</mml:mtext>                              <mml:mtext>n</mml:mtext>                           </mml:msub>                           <mml:mo>,</mml:mo>                           <mml:msub>                              <mml:mi>&#x003c4;</mml:mi>                              <mml:mtext>n</mml:mtext>                           </mml:msub>                           <mml:mo stretchy="false">)</mml:mo>                           <mml:mo>=</mml:mo>                           <mml:mrow>                              <mml:mo>{</mml:mo>                              <mml:mrow>                                 <mml:mtext>y</mml:mtext>                                 <mml:mo>&#x02208;</mml:mo>                                 <mml:mtext>Y</mml:mtext>                                 <mml:mo>:</mml:mo>                                 <mml:msub>                                    <mml:mtext>p</mml:mtext>                                    <mml:mtext>y</mml:mtext>                                 </mml:msub>                                 <mml:mo>=</mml:mo>                                 <mml:mfrac>                                    <mml:mrow>                                       <mml:mo>|</mml:mo>                                       <mml:mo>{</mml:mo>                                       <mml:mtext>i</mml:mtext>                                       <mml:mo>=</mml:mo>                                       <mml:mn>1</mml:mn>                                       <mml:mo>,</mml:mo>                                       <mml:mn>...</mml:mn>                                       <mml:mo>,</mml:mo>                                       <mml:mtext>n</mml:mtext>                                       <mml:mo>:</mml:mo>                                       <mml:msub>                                          <mml:mi>&#x003b1;</mml:mi>                                          <mml:mtext>i</mml:mtext>                                       </mml:msub>                                       <mml:mo>&#x0003e;</mml:mo>                                       <mml:msub>                                          <mml:mi>&#x003b1;</mml:mi>                                          <mml:mtext>n</mml:mtext>                                       </mml:msub>                                       <mml:mo>}</mml:mo>                                       <mml:mo>|</mml:mo>                                       <mml:mo>+</mml:mo>                                       <mml:msub>                                          <mml:mi>&#x003c4;</mml:mi>                                          <mml:mtext>n</mml:mtext>                                       </mml:msub>                                       <mml:mo>|</mml:mo>                                       <mml:mo>{</mml:mo>                                       <mml:mtext>i</mml:mtext>                                       <mml:mo>=</mml:mo>                                       <mml:mn>1</mml:mn>                                       <mml:mo>,</mml:mo>                                       <mml:mn>...</mml:mn>                                       <mml:mo>,</mml:mo>                                       <mml:mtext>n</mml:mtext>                                       <mml:mo>:</mml:mo>                                       <mml:msub>                                          <mml:mi>&#x003b1;</mml:mi>                                          <mml:mn>1</mml:mn>                                       </mml:msub>                                       <mml:mo>=</mml:mo>                                       <mml:msub>                                          <mml:mi>&#x003b1;</mml:mi>                                          <mml:mtext>n</mml:mtext>                                       </mml:msub>                                       <mml:mo>}</mml:mo>                                       <mml:mo>|</mml:mo>                                    </mml:mrow>                                    <mml:mtext>n</mml:mtext>                                 </mml:mfrac>                                 <mml:mo>&#x0003e;</mml:mo>                                 <mml:mi>&#x003b5;</mml:mi>                              </mml:mrow>                              <mml:mo>}</mml:mo>                           </mml:mrow>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>where y is a possible label for x<sub>n</sub>; P<sub>y </sub>is called p value, which is the randomness level of z<sub>n </sub>= (x<sub>n</sub>, y) and also the confidence level of y being the true label; <italic>&#x003c4;</italic><sub>n</sub>, n &#x02208; N is a random variables that distributed uniformly in [0, 1]. Smoothed CP is a power version of CP, which benefits from p distributing uniformly in [0, 1].</p><p>Let &#x00393;<sup><italic>&#x003b5; </italic></sup>= {y &#x02208; Y: P<sub>y </sub>&#x0003e; <italic>&#x003b5;</italic>}, and the true label of x<sub>n </sub>is denoted as y<sub>n</sub>,</p><p>If |&#x00393;<sup><italic>&#x003b5;</italic></sup>| = 1, we define it as a certain prediction.</p><p>If |&#x00393;<sup><italic>&#x003b5;</italic></sup>| &#x0003e; 1, it is an uncertain prediction.</p><p>If |&#x00393;<sup><italic>&#x003b5;</italic></sup>| = &#x02205; , it is an empty prediction.</p><p>If y<sub>n </sub>&#x02208; &#x00393;<sup><italic>&#x003b5;</italic></sup>, we define it as a corrective prediction with confidence level 1 - <italic>&#x003b5;</italic>. Otherwise, it is defined as an error.</p><p>When it comes to forced point prediction, CP selects the label with maximum p value as the prediction.</p><p>CP is originally proposed for online learning and Vovk [<xref ref-type="bibr" rid="B7">7</xref>] offered the theoretical proof that in the online setting in the long run the prediction set &#x00393; contains the true label with probability 1 - <italic>&#x003b5; </italic>and the rate of wrong prediction is bounded by <italic>&#x003b5;</italic>. Especially the smoothed CP is exactly valid, i.e., the rate of wrong prediction is exactly equal to <italic>&#x003b5;</italic>. This is summarized as the proposition of well-calibrated.</p><p><disp-formula id="bmcM4"><label>(4)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M3" name="1471-2105-10-S1-S22-i3" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:munder>                              <mml:mrow>                                 <mml:mi>l</mml:mi>                                 <mml:mi>i</mml:mi>                                 <mml:mi>m</mml:mi>                              </mml:mrow>                              <mml:mrow>                                 <mml:mi>n</mml:mi>                                 <mml:mo>&#x02192;</mml:mo>                                 <mml:mi>&#x0221e;</mml:mi>                              </mml:mrow>                           </mml:munder>                           <mml:mfrac>                              <mml:mrow>                                 <mml:mi>sup</mml:mi>                                 <mml:mo>&#x02061;</mml:mo>                                 <mml:mo stretchy="false">(</mml:mo>                                 <mml:mi>E</mml:mi>                                 <mml:mi>r</mml:mi>                                 <mml:msubsup>                                    <mml:mi>r</mml:mi>                                    <mml:mi>n</mml:mi>                                    <mml:mi>&#x003b5;</mml:mi>                                 </mml:msubsup>                                 <mml:mo stretchy="false">)</mml:mo>                              </mml:mrow>                              <mml:mi>n</mml:mi>                           </mml:mfrac>                           <mml:mo>=</mml:mo>                           <mml:mi>&#x003b5;</mml:mi>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>with <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M4" name="1471-2105-10-S1-S22-i4" overflow="scroll"><mml:semantics><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>E</mml:mi><mml:mi>r</mml:mi></mml:mstyle><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle><mml:mi>&#x003b5;</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> the number of error predictions at the confidence level 1 - <italic>&#x003b5; </italic>(See [<xref ref-type="bibr" rid="B7">7</xref>] for detailed proof). Extensive experiments demonstrated that CP is also applicable to offline learning, which enlarge its applications.</p><p>Different nonconformity measures have been developed from existing algorithms, such as SVM, KNN and so on [<xref ref-type="bibr" rid="B9">9</xref>-<xref ref-type="bibr" rid="B11">11</xref>]. All the CPs have the calibration property, but the efficiency of CP largely depends on the designing of nonconformity measure [<xref ref-type="bibr" rid="B8">8</xref>]. Efficiency means the certain and empty prediction ratio in all predictions. Certain prediction is favourable because it is more informative than uncertain predictions. CP is successfully employed to hedge these popular machine learning methods, and this paper shows that CP-RF is more efficient than others.</p></sec><sec><title>Random forest (RF)</title><p>Breiman's random forest applies Bagging [<xref ref-type="bibr" rid="B12">12</xref>] and Randomization [<xref ref-type="bibr" rid="B13">13</xref>] technique to grow many classification trees with the largest extent possible without pruning. Random Forest is especially attractive in the following cases [<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B15">15</xref>]:</p><p>(1) First, the real world data is noisy and contains many missing values, some of the attributes are categorical, or semi-continuous.</p><p>(2) Furthermore, there are needs to integrate different data sources which face the issue of weighting them.</p><p>(3) RF show high predictive accuracy and are applicable in high-dimensional problems with highly correlated features, especially in the situation which often occurs in bioinformatics, like medical diagnosis.</p><p>In this paper, the random forest outlier measure is used to design a nonconformity measure in order to incorporate random forest into the CP and label conditional CP scheme. Our method can be used in both online and offline settings.</p></sec><sec><title>Cost-sensitive learning problem</title><p>In medical diagnosis, the default assumption of equal misclassification costs underlying machine learning techniques is most likely violated. A false negative prediction may have more serious consequences than a false positive prediction. To address this problem, cost-sensitive classification is developed, which considers the varying costs of different misclassification types [<xref ref-type="bibr" rid="B16">16</xref>]. Usually a cost matrix is defined or learned to reflect the penalty of classifying samples from one class as another. A cost-sensitive classification method takes a cost matrix into consideration during the model building process   [<xref ref-type="bibr" rid="B17">17</xref>]. However, how to get a proper cost matrix remains an open question [<xref ref-type="bibr" rid="B18">18</xref>]. The definition or learning of a cost matrix is quite subjective. In this paper, we extend our method to label conditional CP to address the cost sensitive problem, and the risk of misclassification of each class is well controlled.</p></sec></sec><sec><title>Results</title><sec><title>Experiments setup</title><p>The experiments are divided into two parts: First, to show the calibration property and efficiency of our method, we demonstrate our method CP-RF on 8 benchmark datasets and a real-world gene expression dataset. Second, to cope with the cost-sensitive problem, we extend CP-RF to label conditional CP-RF, and test its performance on two public application datasets.</p></sec><sec><title>Part I Performance of CP-RF</title><p>We employ 8 UCI datasets [<xref ref-type="bibr" rid="B19">19</xref>], including satellite, isolet, soybean, and covertype, etc. Some details are included in Table <xref ref-type="table" rid="T1">1</xref>, which contains information of the number of instances (<italic>n</italic>), number of class (<italic>c</italic>), number of attributes (<italic>a</italic>), and number of numeric (<italic>num</italic>) and nominal (<italic>nom</italic>).</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Datasets used in the experiments</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Dataset</td><td align="left">n</td><td align="left">c</td><td align="left">a</td><td align="left">num</td><td align="left">nom</td></tr></thead><tbody><tr><td align="left">liver</td><td align="left">345</td><td align="left">2</td><td align="left">7</td><td align="left">7</td><td align="left">0</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="left">pima</td><td align="left">768</td><td align="left">2</td><td align="left">8</td><td align="left">8</td><td align="left">0</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="left">sonar</td><td align="left">208</td><td align="left">2</td><td align="left">60</td><td align="left">60</td><td align="left">0</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="left">house votes</td><td align="left">435</td><td align="left">2</td><td align="left">16</td><td align="left">0</td><td align="left">16</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="left">satellite</td><td align="left">6435</td><td align="left">6</td><td align="left">60</td><td align="left">60</td><td align="left">0</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="left">isolet</td><td align="left">300</td><td align="left">26</td><td align="left">618</td><td align="left">618</td><td align="left">0</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="left">soybean</td><td align="left">683</td><td align="left">19</td><td align="left">35</td><td align="left">0</td><td align="left">35</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="left">covertype</td><td align="left">500</td><td align="left">3</td><td align="left">54</td><td align="left">10</td><td align="left">44</td></tr></tbody></table></table-wrap><p>We perform CP-RF in a 10-fold cross validation in an online fashion and report the average performance and compare it with TCM-SVM and TCM-KNN. We use the following key indices at each predefined significance level: (1) Percentage of certain predictions. (2) Percentage of uncertain predictions. (3) Percentage of empty predictions. (4) Percentage of corrective predictions. These terms distinguish with traditional accuracy rate given by RF, SVM and other traditional classifiers.</p><p>Given a significance level <italic>&#x003b5;</italic>, the calibration and efficiency can be laid out. Let the number of trees (denoted as <italic>ntrees</italic>) equal to 1000 and the number of variables to split on at each node (denoted as <italic>ntry</italic>) be the default value <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M5" name="1471-2105-10-S1-S22-i5" overflow="scroll"><mml:semantics><mml:mrow><mml:msqrt><mml:mi>a</mml:mi></mml:msqrt></mml:mrow></mml:semantics></mml:math></inline-formula> (<italic>a </italic>is the number of attributes). In Figures <xref ref-type="fig" rid="F1">1</xref>, <xref ref-type="fig" rid="F2">2</xref>, <xref ref-type="fig" rid="F3">3</xref>, <xref ref-type="fig" rid="F4">4</xref>, <xref ref-type="fig" rid="F5">5</xref>, <xref ref-type="fig" rid="F6">6</xref>, <xref ref-type="fig" rid="F7">7</xref>, <xref ref-type="fig" rid="F8">8</xref>, we demonstrate performance curves according with the significance level <italic>&#x003b5; </italic>ranging from 0.01 to 1, and show the average experimental results on pima (continuous variables), soybean (categorical variables), covertype (mixed variables) and liver (poor data quality), etc.</p><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold>Performance of CP-RF on pima</bold>. We perform CP-RF by applying a ten-fold cross validation in online learning setting and report the average performance. To apperceive how accurate and effective the prediction region is, we use 4 evaluation indices at each predefined significance level: (1) Percentage of certain predictions. (2) Percentage of uncertain predictions with two or more labels which indicates that all these labels are likely to be correct. (3) Percentage of empty predictions. (4) Percentage of corrective predictions which give the proportion of test examples classified correctly. These terms are extended by conformal predictor and distinguish with traditional accuracy rate given by RF, SVM and other traditional classifiers.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-1"/></fig><fig position="float" id="F2"><label>Figure 2</label><caption><p>Performance of CP-RF on soybean.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-2"/></fig><fig position="float" id="F3"><label>Figure 3</label><caption><p>Performance of CP-RF on covertype.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-3"/></fig><fig position="float" id="F4"><label>Figure 4</label><caption><p>Performance of CP-RF on liver.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-4"/></fig><fig position="float" id="F5"><label>Figure 5</label><caption><p>Performance of CP-RF on isolate.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-5"/></fig><fig position="float" id="F6"><label>Figure 6</label><caption><p>Performance of CP-RF on satellite.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-6"/></fig><fig position="float" id="F7"><label>Figure 7</label><caption><p>Performance of CP-RF on sonar.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-7"/></fig><fig position="float" id="F8"><label>Figure 8</label><caption><p>Performance of CP-RF on vote.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-8"/></fig><p>Figures <xref ref-type="fig" rid="F1">1</xref>, <xref ref-type="fig" rid="F2">2</xref>, <xref ref-type="fig" rid="F3">3</xref>, <xref ref-type="fig" rid="F4">4</xref>, <xref ref-type="fig" rid="F5">5</xref>, <xref ref-type="fig" rid="F6">6</xref>, <xref ref-type="fig" rid="F7">7</xref>, <xref ref-type="fig" rid="F8">8</xref> show that the empirical error line is well-calibrated with neglectable statistical fluctuations. It allows controlling the number of errors prior to classification. Percentages of corrective predictions with a predefined level of confidence illustrate the calibration of the new algorithm. Figures <xref ref-type="fig" rid="F1">1</xref>, <xref ref-type="fig" rid="F2">2</xref>, <xref ref-type="fig" rid="F3">3</xref>, <xref ref-type="fig" rid="F4">4</xref>, <xref ref-type="fig" rid="F5">5</xref>, <xref ref-type="fig" rid="F6">6</xref>, <xref ref-type="fig" rid="F7">7</xref>, <xref ref-type="fig" rid="F8">8</xref> also show high accuracy with series of significance level and some interest points are extracted in Table <xref ref-type="table" rid="T2">2</xref> for impressive purpose.</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Corrective predictions at 5 confidence level</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Confidence level</td><td align="left">liver</td><td align="left">pima</td><td align="left">sonar</td><td align="left">vote</td></tr></thead><tbody><tr><td align="left">0.95</td><td align="left">94%</td><td align="left">96%</td><td align="left">96%</td><td align="left">98%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">0.90</td><td align="left">80%</td><td align="left">87%</td><td align="left">93%</td><td align="left">90%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">0.85</td><td align="left">78%</td><td align="left">81%</td><td align="left">82%</td><td align="left">84%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">0.80</td><td align="left">66%</td><td align="left">82%</td><td align="left">76%</td><td align="left">80%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">Confidence level</td><td align="left">satellite</td><td align="left">isolet</td><td align="left">soy-bean</td><td align="left">cover-type</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">0.95</td><td align="left">92%</td><td align="left">94%</td><td align="left">96%</td><td align="left">96%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">0.90</td><td align="left">90%</td><td align="left">88%</td><td align="left">92%</td><td align="left">82%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">0.85</td><td align="left">84%</td><td align="left">85%</td><td align="left">88%</td><td align="left">78%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">0.80</td><td align="left">81%</td><td align="left">79%</td><td align="left">79%</td><td align="left">76%</td></tr></tbody></table></table-wrap><p>Table <xref ref-type="table" rid="T2">2</xref> demonstrates that CP-RF ensures relative high accuracy when controlling a low risk of error. It is important in many domains to measure the risk of misclassification, and if possible, to ensure low risk of error.</p><p>The percentage of certain predictions reflects the efficiency of prediction. Notice that the percentage of uncertain predictions monotonically decreases with higher significance levels. How fast this decline goes to zero depends on the performance of the classifier plugged into the CP framework. Figures <xref ref-type="fig" rid="F1">1</xref>, <xref ref-type="fig" rid="F2">2</xref>, <xref ref-type="fig" rid="F3">3</xref>, <xref ref-type="fig" rid="F4">4</xref>, <xref ref-type="fig" rid="F5">5</xref>, <xref ref-type="fig" rid="F6">6</xref>, <xref ref-type="fig" rid="F7">7</xref>, <xref ref-type="fig" rid="F8">8</xref> show that CP-RF performs significantly well, and it is applicable at the significance levels of 0.20, 0.15, 0.10, and 0.05. For the convenience of comparison, we apply standard TCM-KNN and TCM-SVM algorithm provided by Gammerman and Vovk. The ratios of certain predictions on the 8 datasets are given for comparison in Table <xref ref-type="table" rid="T3">3</xref> and we can find that the efficiency of CP-RF is much better than the others, which indicates its superiority. Then we compare their performance in the occasion of forced point prediction in Table <xref ref-type="table" rid="T4">4</xref>.</p><table-wrap position="float" id="T3"><label>Table 3</label><caption><p>Comparison of certain prediction</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Dataset</td><td align="left">Confidence level</td><td align="left">CP-RF</td><td align="left">TCM-KNN</td><td align="left">TCM-SVM</td></tr></thead><tbody><tr><td align="left">Sonar</td><td align="left">99%</td><td align="left">53.15%</td><td align="left">44.23%</td><td align="left">23.07%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">95%</td><td align="left">77.89%</td><td align="left">73.07%</td><td align="left">48.07%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">90%</td><td align="left">86.74%</td><td align="left">80.76%</td><td align="left">71.15%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td></td><td align="left">Confidence level</td><td align="left">CP-RF</td><td align="left">TCM-KNN</td><td align="left">TCM-SVM</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">Liver</td><td align="left">99%</td><td align="left">36.28%</td><td align="left">17.56%</td><td align="left">25.31%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">95%</td><td align="left">67.21%</td><td align="left">20.56%</td><td align="left">31.45%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">90%</td><td align="left">79.77%</td><td align="left">36.08%</td><td align="left">58.01%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td></td><td align="left">Confidence level</td><td align="left">CP-RF</td><td align="left">TCM-KNN</td><td align="left">TCM-SVM</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">Pima</td><td align="left">99%</td><td align="left">42.36%</td><td align="left">39.46%</td><td align="left">24.05%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">95%</td><td align="left">69.56%</td><td align="left">44.56%</td><td align="left">28.35%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">90%</td><td align="left">74.71%</td><td align="left">61.97%</td><td align="left">45.12%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td></td><td align="left">Confidence level</td><td align="left">CP-RF</td><td align="left">TCM-KNN</td><td align="left">TCM-SVM</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">Vote</td><td align="left">99%</td><td align="left">87.76%</td><td align="left">83.49%</td><td align="left">80.21%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">95%</td><td align="left">92.87%</td><td align="left">91.75%</td><td align="left">90.72%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">90%</td><td align="left">94.89%</td><td align="left">92.56%</td><td align="left">91.05%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td></td><td align="left">Confidence level</td><td align="left">CP-RF</td><td align="left">TCM-KNN</td><td align="left">TCM-SVM</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">Satellite</td><td align="left">99%</td><td align="left">73.07%</td><td align="left">64.47%</td><td align="left">68.07%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">95%</td><td align="left">87.92%</td><td align="left">85.97%</td><td align="left">88.41%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">90%</td><td align="left">92.28%</td><td align="left">90.72%</td><td align="left">91.76%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td></td><td align="left">Confidence level</td><td align="left">CP-RF</td><td align="left">TCM-KNN</td><td align="left">TCM-SVM</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">Isolet</td><td align="left">99%</td><td align="left">57.95%</td><td align="left">54.87%</td><td align="left">56.95%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">95%</td><td align="left">74.72%</td><td align="left">70.37%</td><td align="left">72.07%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">90%</td><td align="left">87.51%</td><td align="left">81.75%</td><td align="left">82.40%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td></td><td align="left">Confidence level</td><td align="left">CP-RF</td><td align="left">TCM-KNN</td><td align="left">TCM-SVM</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">Soybean</td><td align="left">99%</td><td align="left">73.85%</td><td align="left">54.57%</td><td align="left">63.97%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">95%</td><td align="left">79.32%</td><td align="left">73.45%</td><td align="left">78.83%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">90%</td><td align="left">90.49%</td><td align="left">80.29%</td><td align="left">81.72%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td></td><td align="left">Confidence level</td><td align="left">CP-RF</td><td align="left">TCM-KNN</td><td align="left">TCM-SVM</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">Covertype</td><td align="left">99%</td><td align="left">52.74%</td><td align="left">47.82%</td><td align="left">50.42%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">95%</td><td align="left">68.73%</td><td align="left">63.77%</td><td align="left">65.07%</td></tr><tr><td></td><td colspan="4"><hr></hr></td></tr><tr><td></td><td align="left">90%</td><td align="left">76.45%</td><td align="left">72.16%</td><td align="left">73.75%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T4"><label>Table 4</label><caption><p>Comparisons of accuracy</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Model</td><td align="left">liver</td><td align="left">pima</td><td align="left">sonar</td><td align="left">vote</td></tr></thead><tbody><tr><td align="left">CP-RF</td><td align="left">66%</td><td align="left">86%</td><td align="left">84%</td><td align="left">95%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">TCM-KNN</td><td align="left">61%</td><td align="left">85%</td><td align="left">83%</td><td align="left">91%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">TCM-SVM</td><td align="left">51%</td><td align="left">77%</td><td align="left">96%</td><td align="left">84%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">level</td><td align="left">satellite</td><td align="left">isolet</td><td align="left">soybean</td><td align="left">covertype</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">CP-RF</td><td align="left">84%</td><td align="left">82%</td><td align="left">93%</td><td align="left">83%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">TCM-KNN</td><td align="left">82%</td><td align="left">70%</td><td align="left">89%</td><td align="left">74%</td></tr><tr><td colspan="5"><hr></hr></td></tr><tr><td align="left">TCM-SVM</td><td align="left">74%</td><td align="left">89%</td><td align="left">77%</td><td align="left">67%</td></tr></tbody></table></table-wrap><p>It is clear that CP-RF performs well at most of the datasets, especially on the datasets with categorical and mixed variable. CP-RF especially outperforms TCM-KNN for high-dimension dataset (isolet), and outperforms TCM-SVM for noisy data (covertype).</p><p>To compare with the most popular machine learning methods, we consider Acute Lymphoblastic Leukemia (ALL) data which was previously analyzed with traditional machine learning methods. We choose an ALL dataset from [<xref ref-type="bibr" rid="B20">20</xref>] for comparison (see Table <xref ref-type="table" rid="T5">5</xref>). There are 327 cases with attributes of 12558 genes. The data has been divided into six diagnostic groups and one that contains diagnostic samples that did not fit into any one of the above groups (labeled as "Others"). Each group of samples has been randomized into training and testing parts.</p><table-wrap position="float" id="T5"><label>Table 5</label><caption><p>The characteristic of ALL data</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">Group (Class)</td><td align="center">Size of training set</td><td align="center">Size of testing set</td></tr></thead><tbody><tr><td align="center">(1)BCR-ABL</td><td align="center">9</td><td align="center">6</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="center">(2)E2A-PBX1</td><td align="center">18</td><td align="center">9</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="center">(3)Hyperdiploid&#x0003e;50</td><td align="center">42</td><td align="center">22</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="center">(4)MLL</td><td align="center">14</td><td align="center">6</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="center">(5)T-ALL</td><td align="center">28</td><td align="center">15</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="center">(6)TEL-AML1</td><td align="center">52</td><td align="center">27</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="center">(7)Others</td><td align="center">52</td><td align="center">27</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="center">Total</td><td align="center">215</td><td align="center">112</td></tr></tbody></table></table-wrap><p>We report results using CP-RF without discriminating gene selections, i.e. using all of the genes. In order to compare with traditional machine learning method, we apply CP-RF in an offline fashion, and use results of forced prediction. Table <xref ref-type="table" rid="T6">6</xref> demonstrates the detailed classification performance per class in confusion matrix, and it shows that CP-RF makes only 2 misclassifications. The comparison with a fine-tuned Support Vector Machine is laid out in Table <xref ref-type="table" rid="T7">7</xref>.</p><table-wrap position="float" id="T6"><label>Table 6</label><caption><p>Confusion matrix of CP-RF</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Real\Predicted</td><td align="left">1</td><td align="left">2</td><td align="left">3</td><td align="left">4</td><td align="left">5</td><td align="left">6</td><td align="left">7</td></tr></thead><tbody><tr><td align="left">1</td><td align="left">4</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">2</td><td align="left">0</td><td align="left">9</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">3</td><td align="left">0</td><td align="left">0</td><td align="left">22</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">4</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">6</td><td align="left">0</td><td align="left">0</td><td align="left">0</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">5</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">15</td><td align="left">0</td><td align="left">0</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">6</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">27</td><td align="left">0</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">7</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">27</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T7"><label>Table 7</label><caption><p>Comparison of accuracy per class</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Subgroups</td><td align="left">CP-RF</td><td align="left">SVM*</td></tr></thead><tbody><tr><td align="left">1</td><td align="left">66.7%</td><td align="left">100%</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">2</td><td align="left">100%</td><td align="left">100%</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">3</td><td align="left">100%</td><td align="left">99%</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">4</td><td align="left">100%</td><td align="left">97%</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">5</td><td align="left">100%</td><td align="left">100%</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">6</td><td align="left">100%</td><td align="left">96%</td></tr></tbody></table><table-wrap-foot><p>*The results of SVM are cited from [<xref ref-type="bibr" rid="B20">20</xref>]</p></table-wrap-foot></table-wrap><p>Tables <xref ref-type="table" rid="T6">6</xref> and <xref ref-type="table" rid="T7">7</xref> show CP-RF outperforms SVM in subgroup 3, 4 and 6, and they are well-matched in subgroup 2 and 5. All of the two misclassifications happen in subgroup 1, because this subgroup only has six cases, the error rate seems very large.</p><p>Due to the low sample size, the reliability of classification is not guaranteed[<xref ref-type="bibr" rid="B21">21</xref>]. We show the distinct advantage of CP-RF with two measures, corrective predictions and certain predictions under 5 confidence levels in Table <xref ref-type="table" rid="T8">8</xref>. The results show that our method is well-calibrated and make reliable predictions, even in an offline fashion.</p><table-wrap position="float" id="T8"><label>Table 8</label><caption><p>Corrective and certain prediction at 5 confidence levels</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">level</td><td align="left">Corrective prediction</td><td align="left">Certain prediction</td></tr></thead><tbody><tr><td align="left">99%</td><td align="left">97.64%</td><td align="left">100%</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">95%</td><td align="left">93.24%</td><td align="left">98.41%</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">90%</td><td align="left">88.05%</td><td align="left">98.32%</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">85%</td><td align="left">83.90%</td><td align="left">87.64%</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">80%</td><td align="left">77.65%</td><td align="left">82.94%</td></tr></tbody></table></table-wrap></sec><sec><title>Part II: Performance of label conditional CP-RF</title><p>In this part, we choose two multi-class and unbalanced real-world data sets as examples for cost sensitive learning. The objective is to control risk of misclassification within each class for different misclassifications may have different penalty in medical diagnostics. The first data set is the Thyroid disease records [<xref ref-type="bibr" rid="B22">22</xref>], and the problem is to determine whether a patient referred to the clinic is hypothyroid. Each record has 21 attributes in total (15 Boolean and 6 continuous) corresponding to various symptoms and measurements taken from each patient. The data set contains 7200 examples in total and is highly unbalanced in its representation of the 3 possible classes corresponding to diagnoses. Some details are included in Table <xref ref-type="table" rid="T9">9</xref>, which contains information on the name, index and size of each class.</p><table-wrap position="float" id="T9"><label>Table 9</label><caption><p>Datasets used in the experiments</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">Name of class</td><td align="center">Index</td><td align="center">Size</td></tr></thead><tbody><tr><td align="left" colspan="3">1. Thyroid dataset</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">primary hyperthyroid</td><td align="center">1</td><td align="center">166</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">compensated hyperthyroid</td><td align="center">2</td><td align="center">368</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">normal</td><td align="center">3</td><td align="center">6666</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left" colspan="3">2. Chronic gastritis dataset</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">incoordination between liver and stomach</td><td align="center">1</td><td align="center">240</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">dampness-heat of spleen and stomach</td><td align="center">2</td><td align="center">77</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">deficiency of spleen and stomach</td><td align="center">3</td><td align="center">151</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">blood stasis in stomach</td><td align="center">4</td><td align="center">84</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">yin deficiency of stomach</td><td align="center">5</td><td align="center">157</td></tr></tbody></table></table-wrap><p>Another dataset is the Chronic Gastritis Dataset [<xref ref-type="bibr" rid="B23">23</xref>], which is a common disease of the digestive system with gastric inflammation being its notable features. Compare to Western medicine, Chinese medicine have many advantages in its treatment [<xref ref-type="bibr" rid="B24">24</xref>]. According to "<italic>Diagnostic criteria for the diagnosis of chronic gastritis combining traditional and western medicine</italic>" set by the Integrated Traditional and Western Medicine Digest Special Committee, Chronic Gastritis is divided into five subtypes (see table <xref ref-type="table" rid="T9">9</xref>). In our application, we collected 709 cases from the digestion outpatient department of the Affiliated Shuguang Hospital during February and October, 2006. All cases are inspected by both gastroscopy and pathology. Each case is correlated with 55 kinds of symptoms listed in table <xref ref-type="table" rid="T10">10</xref>.</p><table-wrap position="float" id="T10"><label>Table 10</label><caption><p>ID of the symptoms of chronic gastritis</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">ID</td><td align="left">Symptom</td><td align="left">ID</td><td align="left">Symptom</td><td align="left">ID</td><td align="left">Symptom</td><td align="left">ID</td><td align="left">Symptom</td></tr></thead><tbody><tr><td align="left">1</td><td align="left">distending pain</td><td align="left">2</td><td align="left">hunger pain</td><td align="left">3</td><td align="left">dull pain of stomach</td><td align="left">4</td><td align="left">stabbing pain of stomach</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">5</td><td align="left">burning pain of stomach</td><td align="left">6</td><td align="left">abdominal distention</td><td align="left">7</td><td align="left">aggravated after eating</td><td align="left">8</td><td align="left">likeness of being warmed and pressed</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">9</td><td align="left">aggravated in the night</td><td align="left">10</td><td align="left">distention and fullness</td><td align="left">11</td><td align="left">poor appetite</td><td align="left">12</td><td align="left">nausea</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">13</td><td align="left">vomiting</td><td align="left">14</td><td align="left">vomiting</td><td align="left">15</td><td align="left">vomiting of water</td><td align="left">16</td><td align="left">belching</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">17</td><td align="left">gastric upset</td><td align="left">18</td><td align="left">acid regurgitation</td><td align="left">19</td><td align="left">heartburn</td><td align="left">20</td><td align="left">blockage in deglutition</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">21</td><td align="left">emaciation</td><td align="left">22</td><td align="left">dysphoria</td><td align="left">23</td><td align="left">sallow complexion</td><td align="left">24</td><td align="left">dim complexion</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">25</td><td align="left">less lustrous complexion</td><td align="left">26</td><td align="left">cold limbs</td><td align="left">27</td><td align="left">dizziness</td><td align="left">28</td><td align="left">weakness</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">29</td><td align="left">spontaneous sweating</td><td align="left">30</td><td align="left">night sweating</td><td align="left">31</td><td align="left">insomnia</td><td align="left">32</td><td align="left">dry mouth,</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">33</td><td align="left">bitter taste in mouth</td><td align="left">34</td><td align="left">halitosis</td><td align="left">35</td><td align="left">loose stool</td><td align="left">36</td><td align="left">constipation</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">37</td><td align="left">alternate dry and loose stool</td><td align="left">38</td><td align="left">hemafecia</td><td align="left">39</td><td align="left">yellowish urine</td><td align="left">40</td><td align="left">pale tongue</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">41</td><td align="left">pink tongue</td><td align="left">42</td><td align="left">red tongue</td><td align="left">43</td><td align="left">purplish tongue</td><td align="left">44</td><td align="left">fissured tongue</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">45</td><td align="left">teeth-print tongue</td><td align="left">46</td><td align="left">ecchymosis on tongue</td><td align="left">47</td><td align="left">thin-white fur</td><td align="left">48</td><td align="left">white and greasy fur</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">49</td><td align="left">yellow and greasy fur</td><td align="left">50</td><td align="left">little fur</td><td align="left">51</td><td align="left">thready and unsmooth pulse</td><td align="left">52</td><td align="left">stringy and thready pulse</td></tr><tr><td colspan="8"><hr></hr></td></tr><tr><td align="left">53</td><td align="left">stringy and slippery pulse</td><td align="left">54</td><td align="left">deep and weak pulse</td><td align="left">55</td><td align="left">stringy and slippery pulse</td><td></td><td></td></tr></tbody></table></table-wrap><p>When constructing RF, we let the number of trees equal to 1000 and the number of variables to split on at each node be <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M6" name="1471-2105-10-S1-S22-i6" overflow="scroll"><mml:semantics><mml:mrow><mml:mrow><mml:mo>&#x0230a;</mml:mo><mml:mrow><mml:msqrt><mml:mrow><mml:mn>55</mml:mn></mml:mrow></mml:msqrt></mml:mrow><mml:mo>&#x0230b;</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> (Parameter sensitivity analysis of CP-RF is laid out in the next section). For experiments on Thyroid disease dataset, the original dataset is randomly divided into a training set (3772 samples) and a test set (3428 samples). For Chronic Gastritis dataset, we perform our method in a 10-fold cross validation. Average performances are reported.</p><p>We are interested in performances comparison of the label conditional CP-RF with the CP-RF. Limited by space, we only show parts of results: Figures <xref ref-type="fig" rid="F9">9</xref>, <xref ref-type="fig" rid="F10">10</xref>, <xref ref-type="fig" rid="F11">11</xref>, <xref ref-type="fig" rid="F12">12</xref>, <xref ref-type="fig" rid="F13">13</xref>, <xref ref-type="fig" rid="F14">14</xref>, <xref ref-type="fig" rid="F15">15</xref>, <xref ref-type="fig" rid="F16">16</xref> show experiment results on two classes with relatively small samples of Thyroid and Chronic Gastritis datasets. Figure <xref ref-type="fig" rid="F9">9</xref> shows that the CP-RF is not well-calibrated at most of confidence levels within class "primary hyperthyroid" on Thyroid, and meanwhile Figure <xref ref-type="fig" rid="F10">10</xref> shows that the efficiency of CP-RF is very low with confidences levels ranging from 0.01 to 1. In contrast, as is shown in Figure <xref ref-type="fig" rid="F11">11</xref>, label conditional CP-RF is well-calibrated up to neglectable statistical fluctuations and the empirical corrective prediction line can hardly be distinguished from the exact calibration line. Aside from the property of calibration, label conditional CP-RF show improvements on predictive efficiency in Figure <xref ref-type="fig" rid="F12">12</xref>, compared with CP-RF. These contrasts can also be observed in experiments on class "deficiency of spleen and stomach "of Chronic Gastritis datasets (See Figures <xref ref-type="fig" rid="F13">13</xref>, <xref ref-type="fig" rid="F14">14</xref>, <xref ref-type="fig" rid="F15">15</xref>, <xref ref-type="fig" rid="F16">16</xref>).</p><fig position="float" id="F9"><label>Figure 9</label><caption><p><bold>Calibration Performance of CP-RF within class "primary hyperthyroid" on thyroid</bold>. In this experiment, we apply CP-RF and label conditional CP-RF in an online learning fashion. The X axis represents the number of test samples within class "primary hyperthyroid", and Y axis represents the number of error predictions at 3 confidence levels (80%, 95%, and 99%).</p></caption><graphic xlink:href="1471-2105-10-S1-S22-9"/></fig><fig position="float" id="F10"><label>Figure 10</label><caption><p><bold>Efficiency Performance of CP-RF within class "primary hyperthyroid" on thyroid</bold>. To evaluate the performance of CP-RF within class "primary hyperthyroid", we show 3 indices in the experiment: certain prediction, empty prediction and certain&#x00026;correct prediction.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-10"/></fig><fig position="float" id="F11"><label>Figure 11</label><caption><p>Calibration Performance of label conditional CP-RF within class "primary hyperthyroid" on thyroid.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-11"/></fig><fig position="float" id="F12"><label>Figure 12</label><caption><p>Efficiency Performance of label conditional CP-RF within class "primary hyperthyroid" on thyroid.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-12"/></fig><fig position="float" id="F13"><label>Figure 13</label><caption><p>Calibration Performance of CP-RF within class "deficiency of spleen and stomach" on chronic gastritis.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-13"/></fig><fig position="float" id="F14"><label>Figure 14</label><caption><p>Efficiency Performance of CP-RF within class "deficiency of spleen and stomach" on chronic gastritis.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-14"/></fig><fig position="float" id="F15"><label>Figure 15</label><caption><p>Calibration Performance of label conditional CP-RF within class "deficiency of spleen and stomach" on chronic gastritis.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-15"/></fig><fig position="float" id="F16"><label>Figure 16</label><caption><p>Efficiency Performance of label conditional CP-RF within class "deficiency of spleen and stomach" on chronic gastritis.</p></caption><graphic xlink:href="1471-2105-10-S1-S22-16"/></fig><p>It is noticeable that the percentage of certain predictions and certain &#x00026; correct ratios monotonically increase with significance levels. How fast the decline of uncertain prediction goes to zero also depends on the superiority of calculation of <italic>p value</italic>.</p><p>Some interest points are extracted in Tables <xref ref-type="table" rid="T11">11</xref> and <xref ref-type="table" rid="T12">12</xref>. It demonstrates that label conditional CP-RF can be used to control the risk of misclassification within each class, so that it can be considered as an alternative approach for cost sensitive learning for unbalanced data.</p><table-wrap position="float" id="T11"><label>Table 11</label><caption><p>label conditional empirical corrective prediction at 5 confidence level within each class on thyroid data</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">class</td><td align="center">99(%)</td><td align="center">95(%)</td><td align="center">90(%)</td><td align="center">85(%)</td><td align="center">80(%)</td></tr></thead><tbody><tr><td align="center">1</td><td align="center">100</td><td align="center">97.26</td><td align="center">90.41</td><td align="center">87.67</td><td align="center">80.82</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="center">2</td><td align="center">99.44</td><td align="center">96.79</td><td align="center">91.13</td><td align="center">85.49</td><td align="center">81.04</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="center">3</td><td align="center">98.87</td><td align="center">95.37</td><td align="center">90.12</td><td align="center">85.06</td><td align="center">80.49</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T12"><label>Table 12</label><caption><p>label conditional empirical corrective prediction at 5 confidence level within each class on chronic gastritis data</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">class</td><td align="center">99(%)</td><td align="center">95(%)</td><td align="center">90(%)</td><td align="center">85(%)</td><td align="center">80(%)</td></tr></thead><tbody><tr><td align="center">1</td><td align="center">99.14</td><td align="center">95.35</td><td align="center">89.77</td><td align="center">87.44</td><td align="center">82.33</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="center">2</td><td align="center">100</td><td align="center">93.67</td><td align="center">88.72</td><td align="center">82.78</td><td align="center">77.83</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="center">3</td><td align="center">100</td><td align="center">96.32</td><td align="center">89.71</td><td align="center">83.64</td><td align="center">77.94</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="center">4</td><td align="center">100</td><td align="center">96.05</td><td align="center">92.11</td><td align="center">86.84</td><td align="center">81.58</td></tr><tr><td colspan="6"><hr></hr></td></tr><tr><td align="center">5</td><td align="center">100</td><td align="center">95.12</td><td align="center">89.92</td><td align="center">85.34</td><td align="center">80.94</td></tr></tbody></table></table-wrap></sec></sec><sec><title>Discussion</title><sec><title>Part I: CP-RF</title><sec><title>Parameter sensitivity analysis</title><p>A common way to validate an approach is to ensure robustness, that is, the approach must produce consistent results independent of the initial parameter settings. Empirical studies show the parameters adjustments have great impacts on CPs. Normalization of examples affects TCM-KNN greatly. As for TCM-SVM, not only the normalization but the type and parameters of kernel functions are important. Thus, the empirical and non-theoretically alteration hints a potential instability.</p><p>To demonstrate the parameter insensitivity of CP-RF, we set up different parameters for CP-RF, with ntrees = 500,1000,5000 and ntry = 1,..., <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M7" name="1471-2105-10-S1-S22-i7" overflow="scroll"><mml:semantics><mml:mrow><mml:msqrt><mml:mtext>a</mml:mtext></mml:msqrt></mml:mrow></mml:semantics></mml:math></inline-formula>. Mean and standard deviation of forced accuracy on <italic>sonar </italic>are reported.</p><p>For TCM-KNN, We compare the fluctuation of forced accuracy with or without normalization; For TCM-SVM, the affection of different types of kernel are illustrated. The results on <italic>sonar </italic>are summed up in table <xref ref-type="table" rid="T13">13</xref>. CP-RF shows a comparatively trivial fluctuation with the change of parameter settings. The advantage comes from the nature of RF and will benefit medical diagnosis.</p><table-wrap position="float" id="T13"><label>Table 13</label><caption><p>Comparison of Parameter Sensitivity</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">TCM-KNN</td><td align="left">Without normalization</td><td align="left">Attributes normalization</td><td align="left">Examples normalization</td></tr></thead><tbody><tr><td align="left">accuracy</td><td align="left">82.69%</td><td align="left">88.46%</td><td align="left">86.54%</td></tr><tr><td colspan="4"><hr></hr></td></tr><tr><td align="left">TCM-SVM</td><td align="left">Simple dot product</td><td align="left">Radial basis function</td><td align="left">Binomial coefficient polynomial</td></tr><tr><td colspan="4"><hr></hr></td></tr><tr><td align="left">accuracy</td><td align="left">63.46%</td><td align="left">48.08%</td><td align="left">96.15%</td></tr><tr><td colspan="4"><hr></hr></td></tr><tr><td align="left">CP-RF</td><td align="left">Mean</td><td align="left" colspan="2">standard deviation</td></tr><tr><td colspan="4"><hr></hr></td></tr><tr><td align="left">accuracy</td><td align="left">84.92%</td><td align="left" colspan="2">3.52%</td></tr></tbody></table></table-wrap></sec><sec><title>Feature selection</title><p>The problem of feature selection is an open question in many applications. In our method, there is no feature selection. Take gene expression analysis for example, gene selection is a crucial study and remains unsolved. In Yeoh's study, gene expression profiling can accurately identify the known prognostically important leukemia subtypes, by the means of classification using SVM, KNN, and ANN when various selected genes were used. Unfortunately, classifications were performed following a process of discriminating gene selections by a correlation-based feature selection. This process is also labor intensive and requiring experiential knowledge. It is better that automated classification should be made with a level of confidence. Moreover, due to the low sample size, although their research has yielded high predictive accuracies that are comparable with or better than traditional clinical techniques, it remains uncertain how well the selected genes results will extrapolate to practice in the future [<xref ref-type="bibr" rid="B25">25</xref>]. CP-RF is especially suitable for this situation, without discriminating gene selections, i.e. using all of the genes, and this may meet the need of an automated classification. Moreover, no selection bias is introduced.</p></sec></sec><sec><title>Part II: Label conditional CP-RF</title><p>From experiments in Part I, we can see that though CP-RF is well calibrated globally, i.e. the error predictions equal to the predefined confidence level on the whole test data, it cannot guarantee the reliability of classification for each class especially for unbalanced datasets. Different from CP-RF, label conditional CP-RF is label-wise well calibrated while the former may not satisfy the calibration property in some classes. Because the latter uses only partial information from the whole data set, so the computational efficiency is better.</p></sec></sec><sec><title>Conclusion</title><p>Most of state-of-the-art machine learning algorithms cannot provide a reliable measure of their classifications and predictions. This paper addresses the importance of reliability and confidence for classification, and presents a novel method based on a combination of random forest, and conformal predictor. The new algorithm hedges the predictions of RF and gives a well-calibrated region prediction by using the proximity matrix generated with RF as a nonconformity measure of examples. For medical diagnosis, the most important advantage of CP-RF is its calibration: the risk of error can be well controlled. The new method takes advantage of RF and possesses a more precise and stable nonconformity measure. It can deal with redundant and noisy data with mixed types of variables, and is less sensitive to parameter settings. Furthermore, we extend CP-RF to a label conditional version, so that it can control the risk of prediction for each class independently rather than globally. This modified version can provide an alternative way for cost sensitive learning. Experiments on benchmark datasets and real world applications show the usability and superiority of our method.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>CP-RF algorithm</title><p>Executed by transductive inference learning, CP is able to hedge the predictions of any popular machine learning method, which constructs a nonconformity measure for CPs [<xref ref-type="bibr" rid="B3">3</xref>,<xref ref-type="bibr" rid="B4">4</xref>]. It is a remarkable fact that error calibration is guaranteed regardless of the particular classifier plugged into CP and nonconformity measure constructed. However, the quality of region predictions and CP's efficiency accordingly, depends on the nonconformity measure. This issue has been discussed and several types of classifiers have been used, such as support vector machine, k-nearest neighbors, nearest centroid, kernel perceptron, naive Bayes and linear discriminant analysis [<xref ref-type="bibr" rid="B9">9</xref>-<xref ref-type="bibr" rid="B11">11</xref>]. The implementations of these methods are determined by the nature of these classifiers. So TCM-SVM and TCM-KP mainly consider binary classification tasks, TCM-KNN and TCM-KNC is the simplest mathematical realization, and TCM-NB and TCM-LDC is suitable for transductive regression. Indeed, the above methods have demonstrated their applicability and advantages over inductive learning, but there is still much infeasibility. For non-linear datasets, it is especially challenging to TCM-LDC. TCM-KNN and TCM-NC have difficulties with dispersed datasets. TCM-SVM is so processing intensive that it suffers from large datasets. TCM-KP is only practicable to relatively noise-free data. In short, there are many restrictions on data qualities when applying them to real world data. The difficulties in essence lie in the nonconformity measure, which remains an unanswered question.</p><p>Taking above into account, we propose a new algorithm called CP-RF. Random forest classifier naturally leads to a dissimilarity measure between examples in a "strange" space rather than a Euclidean measure. After a RF is grown, since an individual tree is unpruned, the terminal nodes will contain only a small number of observations. Given a random forest of size <italic>k</italic>: <italic>f </italic>= {<italic>T</italic><sub>1</sub>,..., <italic>T</italic><sub><italic>k</italic></sub>} and two examples <italic>x</italic><sub><italic>i </italic></sub>and <italic>x</italic><sub><italic>j</italic></sub>, we propagate them down all the trees within <italic>f</italic>. Let <italic>D</italic><sub><italic>i </italic></sub>= {<italic>T</italic><sub>1<italic>i</italic></sub>,... <italic>T</italic><sub><italic>ki</italic></sub>} and <italic>D</italic><sub><italic>j </italic></sub>= {<italic>T</italic><sub>1<italic>j</italic></sub>,... <italic>T</italic><sub>kj</sub>} be tree node positions for <italic>x</italic><sub><italic>i </italic></sub>and <italic>x</italic><sub><italic>j </italic></sub>on all the <italic>k </italic>trees respectively, a random forest similarity between the two examples is defined as:</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M8" name="1471-2105-10-S1-S22-i8" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:mi>p</mml:mi>                           <mml:mi>r</mml:mi>                           <mml:mi>o</mml:mi>                           <mml:mi>x</mml:mi>                           <mml:mo stretchy="false">(</mml:mo>                           <mml:mi>i</mml:mi>                           <mml:mo>,</mml:mo>                           <mml:mi>j</mml:mi>                           <mml:mo stretchy="false">)</mml:mo>                           <mml:mo>=</mml:mo>                           <mml:mfrac>                              <mml:mn>1</mml:mn>                              <mml:mi>k</mml:mi>                           </mml:mfrac>                           <mml:mstyle displaystyle="true">                              <mml:munderover>                                 <mml:mo>&#x02211;</mml:mo>                                 <mml:mrow>                                    <mml:mi>t</mml:mi>                                    <mml:mo>=</mml:mo>                                    <mml:mn>1</mml:mn>                                 </mml:mrow>                                 <mml:mi>k</mml:mi>                              </mml:munderover>                              <mml:mrow>                                 <mml:mi>I</mml:mi>                                 <mml:mo stretchy="false">(</mml:mo>                                 <mml:msub>                                    <mml:mi>T</mml:mi>                                    <mml:mrow>                                       <mml:mi>t</mml:mi>                                       <mml:mi>i</mml:mi>                                    </mml:mrow>                                 </mml:msub>                                 <mml:mo>,</mml:mo>                                 <mml:msub>                                    <mml:mi>T</mml:mi>                                    <mml:mrow>                                       <mml:mi>t</mml:mi>                                       <mml:mi>j</mml:mi>                                    </mml:mrow>                                 </mml:msub>                                 <mml:mo stretchy="false">)</mml:mo>                              </mml:mrow>                           </mml:mstyle>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>where</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M9" name="1471-2105-10-S1-S22-i9" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:mi>I</mml:mi>                           <mml:mo stretchy="false">(</mml:mo>                           <mml:msub>                              <mml:mi>T</mml:mi>                              <mml:mrow>                                 <mml:mi>t</mml:mi>                                 <mml:mi>i</mml:mi>                              </mml:mrow>                           </mml:msub>                           <mml:mo>,</mml:mo>                           <mml:msub>                              <mml:mi>T</mml:mi>                              <mml:mrow>                                 <mml:mi>t</mml:mi>                                 <mml:mi>j</mml:mi>                              </mml:mrow>                           </mml:msub>                           <mml:mo stretchy="false">)</mml:mo>                           <mml:mo>=</mml:mo>                           <mml:mrow>                              <mml:mo>{</mml:mo>                              <mml:mrow>                                 <mml:mtable columnalign="left">                                    <mml:mtr columnalign="left">                                       <mml:mtd columnalign="left">                                          <mml:mn>1</mml:mn>                                       </mml:mtd>                                       <mml:mtd columnalign="left">                                          <mml:mrow>                                             <mml:mi>i</mml:mi>                                             <mml:mi>f</mml:mi>                                             <mml:mtext>&#x000a0;</mml:mtext>                                             <mml:msub>                                                <mml:mi>T</mml:mi>                                                <mml:mrow>                                                   <mml:mi>t</mml:mi>                                                   <mml:mi>i</mml:mi>                                                </mml:mrow>                                             </mml:msub>                                             <mml:mo>=</mml:mo>                                             <mml:msub>                                                <mml:mi>T</mml:mi>                                                <mml:mrow>                                                   <mml:mi>t</mml:mi>                                                   <mml:mi>j</mml:mi>                                                </mml:mrow>                                             </mml:msub>                                          </mml:mrow>                                       </mml:mtd>                                    </mml:mtr>                                    <mml:mtr columnalign="left">                                       <mml:mtd columnalign="left">                                          <mml:mn>0</mml:mn>                                       </mml:mtd>                                       <mml:mtd columnalign="left">                                          <mml:mrow>                                             <mml:mi>e</mml:mi>                                             <mml:mi>l</mml:mi>                                             <mml:mi>s</mml:mi>                                             <mml:mi>e</mml:mi>                                          </mml:mrow>                                       </mml:mtd>                                    </mml:mtr>                                 </mml:mtable>                              </mml:mrow>                           </mml:mrow>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>i.e., if instance <italic>i </italic>and <italic>j </italic>both land in the same terminal node, the proximity between <italic>i </italic>and <italic>j </italic>is increased by one, this forms a <italic>N </italic>&#x000d7; <italic>N </italic>matrix (&#x027e6;<italic>prox</italic>(<italic>i</italic>, <italic>j</italic>))&#x027e7;<sub><italic>N </italic>&#x000d7; <italic>N</italic></sub>, which is symmetric, positive definite and bounded above by 1, with the diagonal elements equal to 1, and <italic>N </italic>is the total number of cases [<xref ref-type="bibr" rid="B26">26</xref>].</p><p>Outliers are generally defined as cases that are removed from the main body of the data. In the framework of random forest, outliers are cases whose proximities to all other cases in the data are generally small. A useful revision is to define outliers relative to their class. Thus, an outlier in class <italic>j </italic>is a case whose proximities to all other class <italic>j </italic>cases are small. The raw outlier measure for case n in class <italic>j </italic>to the rest of the training data class <italic>j </italic>is defined as</p><p><disp-formula id="bmcM5"><label>(5)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M10" name="1471-2105-10-S1-S22-i10" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:mi>o</mml:mi>                           <mml:mi>u</mml:mi>                           <mml:msub>                              <mml:mi>t</mml:mi>                              <mml:mrow>                                 <mml:mi>r</mml:mi>                                 <mml:mi>a</mml:mi>                                 <mml:mi>w</mml:mi>                              </mml:mrow>                           </mml:msub>                           <mml:mo stretchy="false">(</mml:mo>                           <mml:mi>i</mml:mi>                           <mml:mo stretchy="false">)</mml:mo>                           <mml:mo>=</mml:mo>                           <mml:mfrac>                              <mml:mrow>                                 <mml:mi>n</mml:mi>                                 <mml:mi>s</mml:mi>                                 <mml:mi>a</mml:mi>                                 <mml:mi>m</mml:mi>                                 <mml:mi>p</mml:mi>                                 <mml:mi>l</mml:mi>                                 <mml:mi>e</mml:mi>                              </mml:mrow>                              <mml:mrow>                                 <mml:mover accent="true">                                    <mml:mrow>                                       <mml:mi>p</mml:mi>                                       <mml:mo stretchy="false">(</mml:mo>                                       <mml:mi>i</mml:mi>                                       <mml:mo stretchy="false">)</mml:mo>                                    </mml:mrow>                                    <mml:mo stretchy="true">&#x000af;</mml:mo>                                 </mml:mover>                              </mml:mrow>                           </mml:mfrac>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>where <italic>nsample </italic>denotes the number of samples in class <italic>j and </italic><inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M11" name="1471-2105-10-S1-S22-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow></mml:semantics></mml:math></inline-formula> is the average proximity from case <italic>i </italic>to the rest of the training data within class <italic>j</italic>:</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M12" name="1471-2105-10-S1-S22-i12" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:mover accent="true">                              <mml:mrow>                                 <mml:mi>p</mml:mi>                                 <mml:mo stretchy="false">(</mml:mo>                                 <mml:mi>i</mml:mi>                                 <mml:mo stretchy="false">)</mml:mo>                              </mml:mrow>                              <mml:mo stretchy="true">&#x000af;</mml:mo>                           </mml:mover>                           <mml:mo>=</mml:mo>                           <mml:mstyle displaystyle="true">                              <mml:munder>                                 <mml:mo>&#x02211;</mml:mo>                                 <mml:mi>j</mml:mi>                              </mml:munder>                              <mml:mrow>                                 <mml:msup>                                    <mml:mrow>                                       <mml:mo stretchy="false">[</mml:mo>                                       <mml:mi>p</mml:mi>                                       <mml:mi>r</mml:mi>                                       <mml:mi>o</mml:mi>                                       <mml:mi>x</mml:mi>                                       <mml:mo stretchy="false">(</mml:mo>                                       <mml:mi>i</mml:mi>                                       <mml:mo>,</mml:mo>                                       <mml:mi>j</mml:mi>                                       <mml:mo stretchy="false">)</mml:mo>                                       <mml:mo stretchy="false">]</mml:mo>                                    </mml:mrow>                                    <mml:mn>2</mml:mn>                                 </mml:msup>                              </mml:mrow>                           </mml:mstyle>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>The value of <italic>out</italic><sub><italic>raw</italic></sub>(<italic>i</italic>) will be large if the average proximity is small. Within each class find the median of these raw measures <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M13" name="1471-2105-10-S1-S22-i13" overflow="scroll"><mml:semantics><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#x000af;</mml:mo></mml:mover></mml:mrow></mml:semantics></mml:math></inline-formula>, and their absolute deviation <italic>&#x003c3; </italic>from the median. The raw measure is scaled to arrive at the final outlier measure by the following:</p><p><disp-formula id="bmcM6"><label>(6)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M14" name="1471-2105-10-S1-S22-i14" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:mi>o</mml:mi>                           <mml:mi>u</mml:mi>                           <mml:mi>t</mml:mi>                           <mml:mo stretchy="false">(</mml:mo>                           <mml:mi>i</mml:mi>                           <mml:mo stretchy="false">)</mml:mo>                           <mml:mo>=</mml:mo>                           <mml:mfrac>                              <mml:mrow>                                 <mml:mstyle mathvariant="bold" mathsize="normal">                                    <mml:mi>o</mml:mi>                                    <mml:mi>u</mml:mi>                                 </mml:mstyle>                                 <mml:msub>                                    <mml:mstyle mathvariant="bold" mathsize="normal">                                       <mml:mi>t</mml:mi>                                    </mml:mstyle>                                    <mml:mrow>                                       <mml:mstyle mathvariant="bold" mathsize="normal">                                          <mml:mi>r</mml:mi>                                          <mml:mi>a</mml:mi>                                          <mml:mi>w</mml:mi>                                       </mml:mstyle>                                    </mml:mrow>                                 </mml:msub>                                 <mml:mo stretchy="false">(</mml:mo>                                 <mml:mstyle mathvariant="bold" mathsize="normal">                                    <mml:mi>i</mml:mi>                                 </mml:mstyle>                                 <mml:mo stretchy="false">)</mml:mo>                                 <mml:mo>&#x02212;</mml:mo>                                 <mml:mover accent="true">                                    <mml:mrow>                                       <mml:mi>o</mml:mi>                                       <mml:mi>u</mml:mi>                                       <mml:msub>                                          <mml:mi>t</mml:mi>                                          <mml:mrow>                                             <mml:mi>r</mml:mi>                                             <mml:mi>a</mml:mi>                                             <mml:mi>w</mml:mi>                                          </mml:mrow>                                       </mml:msub>                                    </mml:mrow>                                    <mml:mo stretchy="true">&#x000af;</mml:mo>                                 </mml:mover>                              </mml:mrow>                              <mml:mi>&#x003c3;</mml:mi>                           </mml:mfrac>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>After a random forest is constructed, the proximity matrix of training dataset and a given test example remains the same regardless of changing the order of input data sequence, so random forest outlier measure can be used as a nonconformity measure.</p><p>In our method CP-RF, we define a new nonconformity measure <italic>&#x003b1;</italic><sub><italic>i </italic></sub>= <italic>out</italic>(<italic>i</italic>), and then predict each test sample with Eq. (3). The detailed CP-RF algorithm is summarized in pseudo codes below.</p><p>Algorithm: CP-RF</p><p><bold>Input</bold>: Training set <italic>T </italic>= {(<italic>x</italic><sub>1</sub>, <italic>y</italic><sub>1</sub>),..., (<italic>x</italic><sub><italic>l</italic></sub>, <italic>y</italic><sub>1</sub>)} and a new unlabeled example <italic>x</italic><sub><italic>l</italic>+1</sub>.</p><p><bold>Output</bold>: The set of <italic>p value</italic>s <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M15" name="1471-2105-10-S1-S22-i15" overflow="scroll"><mml:semantics><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> when <italic>T </italic>is an <italic>m</italic>-class dataset.</p><p>1. for <italic>i </italic>= 1 to <italic>m </italic>do</p><p>2. Assign label <italic>i </italic>to <italic>x</italic><sub><italic>l</italic>+1</sub></p><p>3. Construct a RF classifier with training set <italic>T</italic>, put the test example <italic>x</italic><sub><italic>l</italic>+1 </sub>to the forest and output the sample proximity matrix (&#x027e6;<italic>prox</italic>(<italic>i</italic>, <italic>j</italic>))&#x027e7;<sub>(<italic>l</italic>+1) &#x000d7; (<italic>l</italic>+1)</sub>;</p><p>4. Compute nonconformity scores <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M16" name="1471-2105-10-S1-S22-i16" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> of all examples using Eq.(6) (<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M17" name="1471-2105-10-S1-S22-i17" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> is the nonconformity measure of <italic>x</italic><sub><italic>l</italic>+1 </sub>when assigned label <italic>i</italic>);</p><p>5. Compute the <italic>p value </italic><inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M18" name="1471-2105-10-S1-S22-i18" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> of <italic>x</italic><sub><italic>l</italic>+1 </sub>with Eq. (3).</p><p>6. End for</p></sec><sec><title>Label conditional CP-RF algorithm</title><p>Given a significance level <italic>&#x003b5; </italic>&#x0003e; 0 and the goal is to compute predictive regions, ideally consisting of just one label, containing the true label with probability 1 - <italic>&#x003b5;</italic>. But in some situations our predictions are well-calibrated globally, but not within each class. In cost-sensitive learning problem, we must allow different significance levels to be specified for each possible classification of an object because the penalty of misclassification is not the same among all classes [<xref ref-type="bibr" rid="B27">27</xref>,<xref ref-type="bibr" rid="B28">28</xref>]. This problem can be viewed as a conditional inference. We extend our method to label conditional CP to address it, which can also be seen as one version of Mondrian CP (MCP) [<xref ref-type="bibr" rid="B3">3</xref>,<xref ref-type="bibr" rid="B29">29</xref>].</p><p>An important aspect of MCP is the method of calculating <italic>p values</italic>. For example, calculating the p-values in standard CP, the nonconformity score of a new example against the nonconformity scores of all examples observed up to that point are compared. In contrast, label conditional CPs compare the nonconformity score of a new example with the previously observed examples within each class. In detail, this method applies a function called Mondrian taxonomy to effectively partition the example space <italic>Z </italic>into rectangular groups. Given a division of the Cartesian product <italic>N </italic>&#x000d7; <italic>Z </italic>into categories: a function <italic>k</italic>: <italic>N </italic>&#x000d7; <italic>Z </italic>&#x02192; <italic>K </italic>maps each pair (<italic>n</italic>, <italic>z</italic>) (<italic>z </italic>is an example and <italic>n </italic>is the ordinal number of this example in the data sequence &#x027e6;(<italic>Z</italic>)&#x027e7; &#x02193; 1, <italic>z </italic>&#x02193; 2,...) to its category; a label conditional nonconformity measure based on <italic>k </italic>is defined as:</p><p><disp-formula><italic>A</italic><sub><italic>n</italic></sub>: <italic>K</italic><sup><italic>n</italic>-1 </sup>&#x000d7; ((<italic>Z</italic><sup>&#x02022;</sup>))<sup><italic>K </italic></sup>&#x000d7; <italic>K </italic>&#x000d7; <italic>Z </italic>&#x02192; R</disp-formula></p><p>The smoothed Mondrian conformal predictor (smoothed MCP) determined by the Mondrian nonconformity measure <italic>A</italic><sub><italic>n </italic></sub>produces <italic>p values </italic>as:</p><p><disp-formula id="bmcM7"><label>(7)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M19" name="1471-2105-10-S1-S22-i19" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:mtable>                              <mml:mtr>                                 <mml:mtd>                                    <mml:mrow>                                       <mml:msup>                                          <mml:mi>&#x00393;</mml:mi>                                          <mml:mrow>                                             <mml:mi>&#x003b5;</mml:mi>                                             <mml:mo>,</mml:mo>                                             <mml:msub>                                                <mml:mi>&#x003c4;</mml:mi>                                                <mml:mi>n</mml:mi>                                             </mml:msub>                                             <mml:mo>,</mml:mo>                                             <mml:mi>k</mml:mi>                                          </mml:mrow>                                       </mml:msup>                                       <mml:mo stretchy="false">(</mml:mo>                                       <mml:msub>                                          <mml:mi>z</mml:mi>                                          <mml:mn>1</mml:mn>                                       </mml:msub>                                       <mml:mo>,</mml:mo>                                       <mml:mn>...</mml:mn>                                       <mml:mo>,</mml:mo>                                       <mml:msub>                                          <mml:mi>z</mml:mi>                                          <mml:mrow>                                             <mml:mi>n</mml:mi>                                             <mml:mo>&#x02212;</mml:mo>                                             <mml:mn>1</mml:mn>                                          </mml:mrow>                                       </mml:msub>                                       <mml:mo>,</mml:mo>                                       <mml:msub>                                          <mml:mi>x</mml:mi>                                          <mml:mi>n</mml:mi>                                       </mml:msub>                                       <mml:mo>,</mml:mo>                                       <mml:msub>                                          <mml:mi>&#x003c4;</mml:mi>                                          <mml:mi>n</mml:mi>                                       </mml:msub>                                       <mml:mo>,</mml:mo>                                       <mml:mi>k</mml:mi>                                       <mml:mo stretchy="false">)</mml:mo>                                       <mml:mo>=</mml:mo>                                    </mml:mrow>                                 </mml:mtd>                              </mml:mtr>                              <mml:mtr>                                 <mml:mtd>                                    <mml:mrow>                                       <mml:mrow>                                          <mml:mo>{</mml:mo>                                          <mml:mrow>                                             <mml:mi>y</mml:mi>                                             <mml:mo>&#x02208;</mml:mo>                                             <mml:mi>Y</mml:mi>                                             <mml:mo>:</mml:mo>                                             <mml:msub>                                                <mml:mi>p</mml:mi>                                                <mml:mi>y</mml:mi>                                             </mml:msub>                                             <mml:mo>=</mml:mo>                                             <mml:mfrac>                                                <mml:mrow>                                                   <mml:mo>|</mml:mo>                                                   <mml:mo>{</mml:mo>                                                   <mml:mi>i</mml:mi>                                                   <mml:mo>:</mml:mo>                                                   <mml:msub>                                                      <mml:mi>k</mml:mi>                                                      <mml:mi>i</mml:mi>                                                   </mml:msub>                                                   <mml:mo>=</mml:mo>                                                   <mml:msub>                                                      <mml:mi>k</mml:mi>                                                      <mml:mi>n</mml:mi>                                                   </mml:msub>                                                   <mml:mo>&#x00026;</mml:mo>                                                   <mml:msub>                                                      <mml:mi>&#x003b1;</mml:mi>                                                      <mml:mi>i</mml:mi>                                                   </mml:msub>                                                   <mml:mo>&#x0003e;</mml:mo>                                                   <mml:msub>                                                      <mml:mi>&#x003b1;</mml:mi>                                                      <mml:mi>n</mml:mi>                                                   </mml:msub>                                                   <mml:mo>}</mml:mo>                                                   <mml:mo>|</mml:mo>                                                   <mml:mo>+</mml:mo>                                                   <mml:msub>                                                      <mml:mi>&#x003c4;</mml:mi>                                                      <mml:mi>n</mml:mi>                                                   </mml:msub>                                                   <mml:mo>|</mml:mo>                                                   <mml:mo>{</mml:mo>                                                   <mml:mi>i</mml:mi>                                                   <mml:mo>:</mml:mo>                                                   <mml:msub>                                                      <mml:mi>k</mml:mi>                                                      <mml:mi>i</mml:mi>                                                   </mml:msub>                                                   <mml:mo>=</mml:mo>                                                   <mml:msub>                                                      <mml:mi>k</mml:mi>                                                      <mml:mi>n</mml:mi>                                                   </mml:msub>                                                   <mml:mo>&#x00026;</mml:mo>                                                   <mml:msub>                                                      <mml:mi>&#x003b1;</mml:mi>                                                      <mml:mi>i</mml:mi>                                                   </mml:msub>                                                   <mml:mo>=</mml:mo>                                                   <mml:msub>                                                      <mml:mi>&#x003b1;</mml:mi>                                                      <mml:mi>n</mml:mi>                                                   </mml:msub>                                                   <mml:mo>}</mml:mo>                                                   <mml:mo>|</mml:mo>                                                </mml:mrow>                                                <mml:mrow>                                                   <mml:mo>|</mml:mo>                                                   <mml:mo>{</mml:mo>                                                   <mml:mi>i</mml:mi>                                                   <mml:mo>:</mml:mo>                                                   <mml:msub>                                                      <mml:mi>k</mml:mi>                                                      <mml:mi>i</mml:mi>                                                   </mml:msub>                                                   <mml:mo>=</mml:mo>                                                   <mml:msub>                                                      <mml:mi>k</mml:mi>                                                      <mml:mi>n</mml:mi>                                                   </mml:msub>                                                   <mml:mo>}</mml:mo>                                                   <mml:mo>|</mml:mo>                                                </mml:mrow>                                             </mml:mfrac>                                             <mml:mo>&#x0003e;</mml:mo>                                             <mml:mi>&#x003b5;</mml:mi>                                          </mml:mrow>                                          <mml:mo>}</mml:mo>                                       </mml:mrow>                                    </mml:mrow>                                 </mml:mtd>                              </mml:mtr>                           </mml:mtable>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>with <italic>&#x003b1;</italic><sub><italic>i </italic></sub>denotes a nonconformity score.</p><p>In label conditional CP-RF, <italic>&#x003b1;</italic><sub><italic>i </italic></sub>= <italic>out</italic>(<italic>i</italic>) and compared with CP-RF, the small difference is computing <italic>p values </italic>with part of training examples, with Eq. (7). So we get higher computational efficiency. Limited by space, the detailed label conditional CP-RF algorithm is omitted here.</p></sec></sec><sec><title>Availability</title><p>The chronic gastritis dataset, the core source codes of CP-RF and label conditional CP-RF are available at <ext-link ext-link-type="uri" xlink:href="http://59.77.15.238/APBC_paper"/> or <ext-link ext-link-type="uri" xlink:href="http://www.penna.cn/cp-rf"/></p></sec><sec><title>List of abbreviations used</title><p>CP: Conformal predictor; RF: Random forests; KNN: K nearest neighbour classifier; SVM: Support vector machine; KP: Kernel perceptron; NB: Na&#x000ef;ve Bayes; NC: Nearest centroid; LDC: Linear discriminant classifier; KNC: Kernel nearest centroid; ANN: Artificial neural network.</p></sec><sec><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec><sec><title>Authors' contributions</title><p>FY, HM and HZW conceived the study and research question. FY and HZW designed and implemented the algorithms, set up and performed the experiments and drafted the manuscript. HM, CDL and WWC contributed to the theoretical understanding and presentation of the problem.</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>The authors are grateful to Prof. Vladimir Vovk and Dima Devetyarov for valuable suggestions and essential help on conformal predictors. The authors would also like to thank Prof. Chang-le Zhou for assistance in data collection and support. The work is based upon work supported by the 985 Innovation Project on Information Technique of Xiamen University under Grant No.0000-x07204 and the National High Technology Research and Development Program of China (863 Program) under Grant No.2006AA01Z129.</p><p>This article has been published as part of <italic>BMC Bioinformatics </italic>Volume 10 Supplement 1, 2009: Proceedings of The Seventh Asia Pacific Bioinformatics Conference (APBC) 2009. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10?issue=S1"/></p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pirooznia</surname><given-names>M</given-names></name><name><surname>Yang</surname><given-names>JY</given-names></name><name><surname>Yang</surname><given-names>MQ</given-names></name><name><surname>Deng</surname><given-names>YP</given-names></name></person-group><article-title>A comparative study of different machine learning methods on microarray gene expression data</article-title><source>BMC Genomics</source><year>2008</year><volume>9</volume><fpage>S13</fpage><pub-id pub-id-type="pmid">18366602</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gammerman</surname><given-names>A</given-names></name><name><surname>Vovk</surname><given-names>V</given-names></name></person-group><article-title>Prediction algorithms and confidence measures based on algorithmic randomness theory</article-title><source>Theoretical Computer Science</source><year>2002</year><volume>287</volume><fpage>209</fpage><lpage>217</lpage></citation></ref><ref id="B3"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Vovk</surname><given-names>V</given-names></name><name><surname>Gammerman</surname><given-names>A</given-names></name><name><surname>Shafer</surname><given-names>G</given-names></name></person-group><source>Algorithmic learning in a random world</source><year>2005</year><publisher-name>Springer, New York</publisher-name></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gammerman</surname><given-names>A</given-names></name><name><surname>Vovk</surname><given-names>V</given-names></name></person-group><article-title>Hedging predictions in machine learning</article-title><source>Computer Journal</source><year>2007</year><volume>50</volume><fpage>151</fpage><lpage>177</lpage></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shafer</surname><given-names>G</given-names></name><name><surname>Vovk</surname><given-names>V</given-names></name></person-group><article-title>A tutorial on conformal prediction</article-title><source>J Mach Learn Res</source><year>2007</year><volume>9</volume><fpage>371</fpage><lpage>421</lpage></citation></ref><ref id="B6"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Elkan</surname><given-names>C</given-names></name></person-group><article-title>The foundations of cost-sensitive learning</article-title><source>Proceedings of the Seventeenth International Joint Conference of Artificial Intelligence</source><year>2001</year><publisher-name>Morgan Kaufmann, Seattle, Washington</publisher-name><fpage>973</fpage><lpage>978</lpage></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Vovk</surname><given-names>V</given-names></name></person-group><article-title>A Universal Well-Calibrated Algorithm for On-line Classification</article-title><source>J Mach Learn Res</source><year>2004</year><volume>5</volume><fpage>575</fpage><lpage>604</lpage></citation></ref><ref id="B8"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Stijn</surname><given-names>V</given-names></name><name><surname>Laurens</surname><given-names>VDM</given-names></name><name><surname>Ida</surname><given-names>SK</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Petra Perner, LNAI</surname></name></person-group><article-title>Off-line learning with transductive confidence machines: an empirical evaluation</article-title><source>Proceedings of the 5th International Conference on Machine Learning and Data Mining in Pattern Recognition</source><year>2007</year><volume>4571</volume><publisher-name>Leipzig, Germany. Springer Press</publisher-name><fpage>310</fpage><lpage>323</lpage></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tony</surname><given-names>B</given-names></name><name><surname>Zhiyuan</surname><given-names>L</given-names></name><name><surname>Gammerman</surname><given-names>A</given-names></name><name><surname>Frederick</surname><given-names>VD</given-names></name><name><surname>Vaskar</surname><given-names>S</given-names></name></person-group><article-title>Qualified predictions for microarray and proteomics pattern diagnostics with confidence machines</article-title><source>International Journal of Neural Systems</source><year>2005</year><volume>15</volume><fpage>247</fpage><lpage>258</lpage><pub-id pub-id-type="pmid">16187401</pub-id></citation></ref><ref id="B10"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Bellotti</surname><given-names>T</given-names></name><name><surname>Zhiyuan</surname><given-names>L</given-names></name><name><surname>Gammerman</surname><given-names>A</given-names></name></person-group><article-title>Reliable classification of childhood acute leukaemia from gene expression data using Confidence Machines</article-title><source>Proceedings of IEEE International Conference on Granular Computing, Atlanta, USA</source><year>2006</year><fpage>148</fpage><lpage>153</lpage></citation></ref><ref id="B11"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Proedrou</surname><given-names>K</given-names></name><name><surname>Nouretdinov</surname><given-names>I</given-names></name><name><surname>Vovk</surname><given-names>V</given-names></name><name><surname>Gammerman</surname><given-names>A</given-names></name></person-group><article-title>Transductive confidence machines for pattern recognition</article-title><source>Proceedings of the 13th European Conference on Machine Learning</source><year>2002</year><fpage>381</fpage><lpage>390</lpage></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Bagging Predictors</article-title><source>Mach Learn</source><year>1996</year><volume>24</volume><fpage>123</fpage><lpage>140</lpage></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Random forests</article-title><source>Mach Learn</source><year>2001</year><volume>45</volume><fpage>5</fpage><lpage>32</lpage></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Diaz</surname><given-names>UR</given-names></name><name><surname>Alvarez</surname><given-names>AS</given-names></name></person-group><article-title>Gene Selection and Classification of Microarray Data Using Random Forest</article-title><source>BMC Bioinformatics</source><year>2006</year><volume>7</volume><fpage>3</fpage><pub-id pub-id-type="pmid">16398926</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Strobl</surname><given-names>C</given-names></name><name><surname>Boulesteix</surname><given-names>AL</given-names></name><name><surname>Kneib</surname><given-names>T</given-names></name><name><surname>Augustin</surname><given-names>T</given-names></name><name><surname>Zeileis</surname><given-names>A</given-names></name></person-group><article-title>Conditional variable importance for random forests</article-title><source>BMC Bioinformatics</source><year>2008</year><volume>9</volume><fpage>307</fpage><pub-id pub-id-type="pmid">18620558</pub-id></citation></ref><ref id="B16"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Turney</surname><given-names>P</given-names></name></person-group><article-title>Types of cost in inductive concept learning</article-title><source>Workshop on Cost-Sensitive Learning at ICML</source><year>2000</year><publisher-name>Stanford University, California</publisher-name><fpage>15</fpage><lpage>21</lpage></citation></ref><ref id="B17"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>ZH</given-names></name><name><surname>Liu</surname><given-names>XY</given-names></name></person-group><article-title>On multi-class cost-sensitive learning</article-title><source>Proceedings of the 21st National Conference on Artificial Intelligence, Boston, MA</source><year>2006</year><fpage>567</fpage><lpage>572</lpage></citation></ref><ref id="B18"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Zadrozny</surname><given-names>B</given-names></name><name><surname>Elkan</surname><given-names>C</given-names></name></person-group><article-title>Learning and making decisions when costs and probabilities are both unknown</article-title><source>Proceedings of the Seventh International Conference on Knowledge Discovery and Data Mining</source><year>2001</year><publisher-name>ACM Press</publisher-name><fpage>204</fpage><lpage>213</lpage></citation></ref><ref id="B19"><citation citation-type="other"><article-title>UCI Machine Learning Repository</article-title><ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml/"/></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yeoh</surname><given-names>EJ</given-names></name><name><surname>Ross</surname><given-names>ME</given-names></name><name><surname>Shurtleff</surname><given-names>SA</given-names></name><etal></etal></person-group><article-title>Classification subtype discovery and prediction of outcome in pediatric acute lymphoblastic leukemia by gene expression profiling</article-title><source>Cancer Cell</source><year>2002</year><volume>1</volume><fpage>133</fpage><lpage>143</lpage><pub-id pub-id-type="pmid">12086872</pub-id></citation></ref><ref id="B21"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Sorin</surname><given-names>D</given-names></name></person-group><source>Data Analysis Tools for DNA Microarrays</source><year>2003</year><publisher-name>Chapman&#x00026;Hall/CRC, London</publisher-name></citation></ref><ref id="B22"><citation citation-type="other"><article-title>Thyroid Disease Database</article-title><ext-link ext-link-type="uri" xlink:href="ftp://ftp.ics.uci.edu/pub/machine-learning-databases/thyroid-disease/"/></citation></ref><ref id="B23"><citation citation-type="other"><article-title>Chronic Gastritis Dataset</article-title><ext-link ext-link-type="uri" xlink:href="http://59.77.15.238/APBC_paper"/></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Niu</surname><given-names>HZ</given-names></name><name><surname>Wang</surname><given-names>RX</given-names></name><name><surname>Lan</surname><given-names>SM</given-names></name><name><surname>Xu</surname><given-names>WL</given-names></name></person-group><article-title>hinking and approaches on treatment of chronic gastritis with integration of traditional Chinese and western medicine</article-title><source>Shandong Journal of Traditional Chinese Medicine</source><year>2001</year><volume>20</volume><fpage>70</fpage><lpage>72</lpage></citation></ref><ref id="B25"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Boulesteix</surname><given-names>AL</given-names></name><name><surname>Strobl</surname><given-names>C</given-names></name><name><surname>Augustin</surname><given-names>T</given-names></name><name><surname>Daumer</surname><given-names>M</given-names></name></person-group><article-title>Evaluating microarray-based classifiers: an overview</article-title><source>Cancer Informatics</source><year>2008</year><volume>6</volume><fpage>77</fpage><lpage>97</lpage></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Qi</surname><given-names>Y</given-names></name><name><surname>Klein</surname><given-names>SJ</given-names></name><name><surname>Bar</surname><given-names>JZ</given-names></name></person-group><article-title>Random forest similarity for protein-protein interaction prediction from multiple sources</article-title><source>Pacific Symposium on Biocomputing</source><year>2005</year><volume>10</volume><fpage>531</fpage><lpage>542</lpage><pub-id pub-id-type="pmid">15759657</pub-id></citation></ref><ref id="B27"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Domingos</surname><given-names>P</given-names></name></person-group><article-title>MetaCost: A general method for making classifiers cost-sensitive</article-title><source>Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining</source><year>1999</year><publisher-name>New York. ACM Press</publisher-name><fpage>155</fpage><lpage>164</lpage></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chris</surname><given-names>D</given-names></name><name><surname>Robert</surname><given-names>CH</given-names></name></person-group><article-title>Cost curves: An improved method for visualizing classifier performance</article-title><source>Machine Learning</source><year>2006</year><volume>65</volume><fpage>95</fpage><lpage>130</lpage></citation></ref><ref id="B29"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Vovk</surname><given-names>V</given-names></name><name><surname>Lindsay</surname><given-names>D</given-names></name><name><surname>Nouretdinov</surname><given-names>I</given-names></name><name><surname>Gammerman</surname><given-names>A</given-names></name></person-group><article-title>Mondrian Confidence Machine</article-title><source>Technical Report</source><publisher-name>Computer Learning Research Centre, Royal Holloway, University of London</publisher-name></citation></ref></ref-list></back></article>