<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article" xml:lang="EN"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id><journal-id journal-id-type="publisher-id">bioinformatics</journal-id><journal-id journal-id-type="hwp">bioinfo</journal-id><journal-title>Bioinformatics</journal-title><issn pub-type="ppub">1367-4803</issn><issn pub-type="epub">1460-2059</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">18826957</article-id><article-id pub-id-type="pmc">2639006</article-id><article-id pub-id-type="doi">10.1093/bioinformatics/btn505</article-id><article-id pub-id-type="publisher-id">btn505</article-id><article-categories><subj-group subj-group-type="heading"><subject>Applications Note</subject><subj-group><subject>Systems Biology</subject></subj-group></subj-group></article-categories><title-group><article-title>BNFinder: exact and efficient method for learning Bayesian networks</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wilczy&#x00144;ski</surname><given-names>Bartek</given-names></name></contrib><contrib contrib-type="author"><name><surname>Dojer</surname><given-names>Norbert</given-names></name><xref ref-type="corresp" rid="COR1">*</xref></contrib></contrib-group><aff>Institute of Informatics, University of Warsaw, Poland</aff><author-notes><corresp id="COR1">*To whom correspondence should be addressed.</corresp><fn><p>Associate Editor: Thomas Lengauer</p></fn></author-notes><pub-date pub-type="ppub"><day>15</day><month>1</month><year>2009</year></pub-date><pub-date pub-type="epub"><day>30</day><month>9</month><year>2008</year></pub-date><pub-date pub-type="pmc-release"><day>30</day><month>9</month><year>2008</year></pub-date><volume>25</volume><issue>2</issue><fpage>286</fpage><lpage>287</lpage><history><date date-type="received"><day>4</day><month>3</month><year>2008</year></date><date date-type="rev-recd"><day>3</day><month>9</month><year>2008</year></date><date date-type="accepted"><day>22</day><month>9</month><year>2008</year></date></history><permissions><copyright-statement>&#x000a9; 2008 The Author(s)</copyright-statement><copyright-year>2008</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc/2.0/uk/"><p><!--CREATIVE COMMONS-->This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/2.0/uk/">http://creativecommons.org/licenses/by-nc/2.0/uk/</ext-link>) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.</p></license></permissions><abstract><p><bold>Motivation:</bold> Bayesian methods are widely used in many different areas of research. Recently, it has become a very popular tool for biological network reconstruction, due to its ability to handle noisy data. Even though there are many software packages allowing for Bayesian network reconstruction, only few of them are freely available to researchers. Moreover, they usually require at least basic programming abilities, which restricts their potential user base. Our goal was to provide software which would be freely available, efficient and usable to non-programmers.</p><p><bold>Results:</bold> We present a BNFinder software, which allows for Bayesian network reconstruction from experimental data. It supports dynamic Bayesian networks and, if the variables are partially ordered, also static Bayesian networks. The main advantage of BNFinder is the use exact algorithm, which is at the same time very efficient (polynomial with respect to the number of observations).</p><p><bold>Availability:</bold> The software, supplementary information and manual is available at <ext-link ext-link-type="uri" xlink:href="http://bioputer.mimuw.edu.pl/software/bnf/">http://bioputer.mimuw.edu.pl/software/bnf/</ext-link>. Besides the availability of the standalone application and the source code, we have developed a web interface to BNFinder application running on our servers. A web tutorial on different options of BNFinder is also available.</p><p><bold>Contact:</bold> <email>dojer@mimuw.edu.pl</email></p></abstract></article-meta></front><body><sec sec-type="intro" id="SEC1"><title>1 INTRODUCTION</title><p>Computational methods of Bayesian network inference are very popular in many different areas of bioinformatics and other fields of science. Examples include: regulatory network reconstruction (Dojer <italic>et al</italic>., <xref ref-type="bibr" rid="B5">2006</xref>; Husmeier, <xref ref-type="bibr" rid="B7">2003</xref>) where nodes represent genes and edges represent statistical dependencies which may indicate regulatory interactions; predicting gene expression from promoter sequence (Beer and Tavazoie, <xref ref-type="bibr" rid="B1">2004</xref>; Segal <italic>et al</italic>., <xref ref-type="bibr" rid="B13">2003</xref>) where edges lead from promoter features (motif occurrences, their positions, etc.) to expression patterns (affinity to overlapping expression clusters); neural signal transduction analysis (Smith <italic>et al</italic>., <xref ref-type="bibr" rid="B14">2006</xref>) where network topology mimics the topology of connections between different parts of the brain and many others. Despite differences in the interpretation of network structure, the methodology of these studies is remarkably similar [see, Needham <italic>et al</italic>. (<xref ref-type="bibr" rid="B11">2007</xref>) for overview and further examples]. We aim to provide new software which could be used for different applications of Bayesian network reconstruction.</p><p>Most programs learning Bayesian networks from data are based on heuristic search techniques of identifying good models. This is due to a number of discouraging complexity results (Chickering, <xref ref-type="bibr" rid="B2">1996</xref>; Chickering <italic>et al</italic>., <xref ref-type="bibr" rid="B3">2004</xref>; Meek, <xref ref-type="bibr" rid="B8">2001</xref>) showing that, without restrictive assumptions, learning Bayesian networks from data is NP-hard with respect to the number of network vertices. On the other hand, the known exact algorithms learn the structure of optimal networks having up to 20&#x02013;40 vertices (Ott <italic>et al</italic>., <xref ref-type="bibr" rid="B12">2004</xref>).</p><p>In an extensive comparison, Murphy (<xref ref-type="bibr" rid="B9">2007</xref>) lists over 50 software packages available for different applications of Bayesian networks. However, if one is searching for a free software able to infer the structure of static and dynamic Bayesian networks from data there are only two such applications:<list list-type="bullet"><list-item><p>Banjo package (Smith <italic>et al</italic>., <xref ref-type="bibr" rid="B14">2006</xref>): Bayesian ANalysis with Java Objects,</p></list-item><list-item><p>Bayes Net Toolbox (Murphy, <xref ref-type="bibr" rid="B10">2002</xref>) for Matlab with an extension for dynamic Bayesian networks inference using MCMC (Husmeier, <xref ref-type="bibr" rid="B7">2003</xref>).</p></list-item></list></p><p>Both of these software packages use heuristic search algorithms to find the best scoring network topology in a vast space of possible directed graphs, usually with some constraints on the maximal vertex in-degree.</p></sec><sec sec-type="methods" id="SEC2"><title>2 METHODS</title><p>For a thorough treatment of the contents of the present section, we refer the reader to <ext-link ext-link-type="uri" xlink:href="http://bioputer.mimuw.edu.pl/software/bnf/">Supplementary Materials</ext-link>.</p><p>The BNFinder program is based on a novel polynomial-time algorithm for learning an optimal Bayesian network structure (Dojer, <xref ref-type="bibr" rid="B4">2006</xref>). The algorithm was designed to save reasonable speed and perfect quality of learning in a wide class of problems occurring in the computational molecular biology. It works under the assumption that there is no need to examine the acyclicity of the graph, which is satisfied in the following cases:<list list-type="bullet"><list-item><p>When dealing with dynamic Bayesian networks, a dynamic Bayesian network describes stochastic evolution of a set of random variables over discretized time. Therefore, conditional distributions refer to random variables in neighboring time points and the graph is always acyclic.</p></list-item><list-item><p>In case of static Bayesian networks, the set of possible network structures must be restricted. BNFinder lets the user divide the set of variables into an ordered set of disjoint subsets of variables, where edges can only lead from upstream to downstream subsets. If such ordering is not known beforehand, one can try to run BNFinder with different orderings and choose a network with the best overall score.</p></list-item></list></p><p>BNFinder learns optimal networks with respect to two generally used scoring criteria: Bayesian&#x02013;Dirichlet equivalence (BDe) and minimal description length (MDL). The (default) BDe score originates from Bayesian statistics and corresponds to the <italic>posterior</italic> probability of a network-given data. The MDL score originates from information theory and corresponds to the length of the data compressed with the compression model derived from the network structure. It also has a statistical interpretation as an approximation of the posterior probability. The algorithm works in polynomial time for both scores, but computations with the MDL are faster, especially for large datasets. However, we recommend using the BDe score due to its exactness in the statistical interpretation.</p><p>Both MDL and BDe scores were originally designed for discrete variables. Continuous variables are handled with corresponding scores, derived under the assumption that conditional distributions belong to a family of Gaussian mixtures.</p><p>BNFinder may learn either dynamic Bayesian networks (from time series data) or static ones (from independent experiment data). In the second case it is necessary to specify constraints on the network's structure forcing its acyclicity.</p><p>A special treatment is required for experiments, in which the values of some variables were perturbed (e.g. knockout experiments). Since perturbations change the structure of interactions, learning procedures have to use data selectively. BNFinder handles perturbations in the way following Dojer <italic>et al</italic>. (<xref ref-type="bibr" rid="B5">2006</xref>), i.e. for scoring sets of parents of a variable <italic>v</italic>, it takes into account only the experiments where <italic>v</italic> was not perturbed.</p><p>A prior distribution on the network structure may be specified through assigning weights to potential variable interactions in the way following Tamada <italic>et al</italic>. (<xref ref-type="bibr" rid="B15">2003</xref>). Moreover, the size of regulator sets of each variable may be bounded to a given number and the spaces of possible conditional probability distributions of selected variables may be restricted to <italic>noisy-and</italic> or <italic>noisy-or</italic> distributions.</p><p>There are important biological applications of Bayesian networks, in which usually the amount of learning data is small relative to the network size (e.g. reconstruction of gene regulatory networks from microarray data). Typically in such cases suboptimal models explain the data nearly as well as the optimal (highest scoring) one. For this reason, Friedman and Koller (<xref ref-type="bibr" rid="B6">2003</xref>) propose to pay attention for network <italic>features</italic> frequently appearing in suboptimal networks. Following this idea, BNFinder splits a potential network structure into independently learned features, each one composed of a vertex and its parent set. For each vertex BNFinder returns as an output a user-specified number of suboptimal parents features with their relative posterior probabilities. Setting this parameter to 1 causes BNFinder to learn the optimal network structure composed of the highest scoring features. Otherwise returned features constitute a class of suboptimal networks.</p><p>Output may be written in a few formats, supported in various graph and Bayesian network applications.</p></sec><sec id="SEC3"><title>3 IMPLEMENTATION</title><p>The BNFinder software is implemented in the Python programming language so it can be installed and run on all popular operating systems. The only requirement is the availability of a recent version (&#x0003e;2.4) of the Python interpreter. Detailed installation instructions can be found on the <ext-link ext-link-type="uri" xlink:href="http://bioputer.mimuw.edu.pl/software/bnf/">Supplementary Web Page</ext-link>.</p><p>Besides of the stand-alone version of BNFinder we have made a publicly available web server which allows for using BNFinder running on our servers on users' data. The server uses a very simple web form for input and sends the results to the e-mail address provided. To save the resources, we have limited the web version to handle at most 20 variables and 500 observations.</p><p>In order to judge the performance of our software, we have compared it to the Banjo library (Smith <italic>et al</italic>., <xref ref-type="bibr" rid="B14">2006</xref>). As a realistic dataset, we have chosen the dataset attached as an example to the Banjo package, consisting of 20 variables and 2000 observations, published by Smith <italic>et al</italic>. (<xref ref-type="bibr" rid="B14">2006</xref>). The authors search for a dynamic Bayesian network with an in-degree of all vertices not larger than 5. It should be noted, that the number of such networks is extremely large (((20&#x000d7; 19&#x000d7; 18&#x000d7; 17&#x000d7; 16)/(1&#x000d7; 2&#x000d7; 3&#x000d7; 4&#x000d7; 5))<sup>20</sup>&#x0223c;6.4&#x000d7; 10<sup>84</sup>). Even though Banjo is able to analyze approximately 1 million networks per minute on a single CPU it would take it more than 10<sup>70</sup> years to search through all possible networks. Thanks to the new algorithm (Dojer, <xref ref-type="bibr" rid="B4">2006</xref>) our method is able to find the correct topology for the same dataset in a few hours on the same computer.</p></sec></body><back><ack><title>ACKNOWLEDGEMENTS</title><p>The computational resources were provided by CoE Bio-Exploratorium project: WKP 1/1.4.3/1/2004/44/44/115.</p><p><italic>Funding</italic>: Polish Ministry of Science and Higher Education (No. PBZ-MNiI-2/1/2005 and 3 T11F 021 28, partial); Foundation for Polish Science (to B.W.).</p><p><italic>Conflict of Interest</italic>: none declared.</p></ack><ref-list><title>REFERENCES</title><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Beer</surname><given-names>MA</given-names></name><name><surname>Tavazoie</surname><given-names>S</given-names></name></person-group><article-title>Predicting gene expression from sequence.</article-title><source>Cell</source><year>2004</year><volume>117</volume><fpage>185</fpage><lpage>198</lpage><pub-id pub-id-type="pmid">15084257</pub-id></citation></ref><ref id="B2"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Chickering</surname><given-names>DM</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Fisher</surname><given-names>D</given-names></name><name><surname>Lenz</surname><given-names>H-J</given-names></name></person-group><article-title>Learning Bayesian networks is NP-complete.</article-title><source>Learning from Data: Artificial Inteligence and Statistics V.</source><year>1996</year><publisher-name>Springer-Verlag</publisher-name></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chickering</surname><given-names>DM</given-names></name><etal/></person-group><article-title>Large-sample learning of Bayesian networks is NP-hard.</article-title><source>J. Mach. Learn. Res</source><year>2004</year><volume>5</volume><fpage>1287</fpage><lpage>1330</lpage></citation></ref><ref id="B4"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Dojer</surname><given-names>N</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Kr&#x000e1;lovic</surname><given-names>R</given-names></name><name><surname>Urzyczyn</surname><given-names>P</given-names></name></person-group><article-title>Learning Bayesian networks does not have to be NP-hard</article-title><source>Proceedings of Mathematical Foundations of Computer Science 2006.</source><year>2006</year><publisher-name>Springer-Verlag</publisher-name><fpage>305</fpage><lpage>314</lpage><comment>LNCS 4162</comment></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Dojer</surname><given-names>N</given-names></name><etal/></person-group><article-title>Applying dynamic Bayesian networks to perturbed gene expression data.</article-title><source>BMC Bioinformatics</source><year>2006</year><volume>7</volume><fpage>249</fpage><pub-id pub-id-type="pmid">16681847</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>N</given-names></name><name><surname>Koller</surname><given-names>D</given-names></name></person-group><article-title>Being Bayesian about network structure. a bayesian approach to structure discovery in bayesian networks.</article-title><source>Mach. Learn</source><year>2003</year><volume>50</volume><fpage>95</fpage><lpage>125</lpage></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Husmeier</surname><given-names>D</given-names></name></person-group><article-title>Sensitivity and specificity of inferring genetic regulatory interactions from microarray experiments with dynamic Bayesian networks.</article-title><source>Bioinformatics</source><year>2003</year><volume>19</volume><fpage>2271</fpage><lpage>2282</lpage><pub-id pub-id-type="pmid">14630656</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Meek</surname><given-names>C</given-names></name></person-group><article-title>Finding a path is harder than finding a tree.</article-title><source>J. Artif. Intell. Res.</source><year>2001</year><volume>15</volume><fpage>383</fpage><lpage>389</lpage></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>K</given-names></name></person-group><article-title>Software packages for graphical models &#x02013; Bayesian networks.</article-title><source>Bull. Int. Soc. Bayesian Anal</source><year>2007</year><volume>14</volume></citation></ref><ref id="B10"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>KP</given-names></name></person-group><article-title>Bayes Net Toolbox</article-title><source>Technical report.</source><year>2002</year><publisher-name>MIT Artificial Intelligence Laboratory</publisher-name></citation></ref><ref id="B11"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Needham</surname><given-names>CJ</given-names></name><etal/></person-group><article-title>A primer on learning in Bayesian networks for computational biology.</article-title><source>PLoS Comput. Biol.</source><year>2007</year><volume>3</volume><fpage>e129</fpage></citation></ref><ref id="B12"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Ott</surname><given-names>S</given-names></name><etal/></person-group><article-title>Finding optimal models for small gene networks.</article-title><source>Pac. Symp. Biocomput.</source><year>2004</year><fpage>557</fpage><lpage>567</lpage></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Segal</surname><given-names>E</given-names></name><etal/></person-group><article-title>Genome-wide discovery of transcriptional modules from DNA sequence and gene expression.</article-title><source>Bioinformatics</source><year>2003</year><volume>19</volume><issue>Suppl. 1</issue><fpage>273</fpage><lpage>282</lpage></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>VA</given-names></name><etal/></person-group><article-title>Computational inference of neural information flow networks.</article-title><source>PLoS Comput. Biol</source><year>2006</year><volume>2</volume><fpage>e161</fpage><pub-id pub-id-type="pmid">17121460</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tamada</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Estimating gene networks from gene expression data by combining Bayesian network model with promoter element detection.</article-title><source>Bioinformatics</source><year>2003</year><volume>1</volume><issue>Suppl. 2</issue><fpage>ii227</fpage><lpage>ii236</lpage><pub-id pub-id-type="pmid">14534194</pub-id></citation></ref></ref-list></back></article>