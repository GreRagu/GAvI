<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article" xml:lang="EN"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title>PLoS ONE</journal-title><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19259259</article-id><article-id pub-id-type="pmc">2645675</article-id><article-id pub-id-type="publisher-id">08-PONE-RA-06422R1</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0004638</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience/Cognitive Neuroscience</subject><subject>Neuroscience/Sensory Systems</subject><subject>Neuroscience/Theoretical Neuroscience</subject><subject>Neuroscience/Experimental Psychology</subject></subj-group></article-categories><title-group><article-title>Lip-Reading Aids Word Recognition Most in Moderate Noise: A Bayesian Explanation Using High-Dimensional Feature Space</article-title><alt-title alt-title-type="running-head">Bayesian Speech Recognition</alt-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ma</surname><given-names>Wei Ji</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>&#x0002a;</sup></xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Zhou</surname><given-names>Xiang</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Ross</surname><given-names>Lars A.</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib><contrib contrib-type="author"><name><surname>Foxe</surname><given-names>John J.</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib><contrib contrib-type="author"><name><surname>Parra</surname><given-names>Lucas C.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>Department of Neuroscience, Baylor College of Medicine, Houston, Texas, United States of America</addr-line></aff><aff id="aff2"><label>2</label><addr-line>Department of Biomedical Engineering, The City College of New York, New York, New York, United States of America</addr-line></aff><aff id="aff3"><label>3</label><addr-line>Program in Cognitive Neuroscience, Department of Psychology, The City College of New York, New York, New York, United States of America</addr-line></aff><aff id="aff4"><label>4</label><addr-line>The Cognitive Neuroscience Laboratory, Nathan S. Kline Institute for Psychiatric Research, Program in Cognitive Neuroscience and Schizophrenia, Orangeburg, New York, United States of America</addr-line></aff><aff id="aff5"><label>5</label><addr-line>Program in Neuropsychology, Department of Psychology, Queens College of the City University of New York, Flushing, New York, United States of America</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Whitney</surname><given-names>David</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">University of California Davis, United States of America</aff><author-notes><corresp id="cor1">&#x0002a; E-mail: <email>wjma@bcm.edu</email></corresp><fn fn-type="con"><p>Conceived and designed the experiments: WJM XZ LAR JJF LP. Performed the experiments: XZ LP. Analyzed the data: WJM XZ LP. Wrote the paper: WJM LP. Designed the computational model: WJM.</p></fn></author-notes><pub-date pub-type="collection"><year>2009</year></pub-date><pub-date pub-type="epub"><day>4</day><month>3</month><year>2009</year></pub-date><volume>4</volume><issue>3</issue><elocation-id>e4638</elocation-id><history><date date-type="received"><day>16</day><month>9</month><year>2008</year></date><date date-type="accepted"><day>7</day><month>1</month><year>2009</year></date></history><copyright-statement>Ma et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</copyright-statement><copyright-year>2009</copyright-year><abstract><p>Watching a speaker's facial movements can dramatically enhance our ability to comprehend words, especially in noisy environments. From a general doctrine of combining information from different sensory modalities (the principle of inverse effectiveness), one would expect that the visual signals would be most effective at the highest levels of auditory noise. In contrast, we find, in accord with a recent paper, that visual information improves performance more at intermediate levels of auditory noise than at the highest levels, and we show that a novel visual stimulus containing only temporal information does the same. We present a Bayesian model of optimal cue integration that can explain these conflicts. In this model, words are regarded as points in a multidimensional space and word recognition is a probabilistic inference process. When the dimensionality of the feature space is low, the Bayesian model predicts inverse effectiveness; when the dimensionality is high, the enhancement is maximal at intermediate auditory noise levels. When the auditory and visual stimuli differ slightly in high noise, the model makes a counterintuitive prediction: as sound quality increases, the proportion of reported words corresponding to the visual stimulus should first increase and then decrease. We confirm this prediction in a behavioral experiment. We conclude that auditory-visual speech perception obeys the same notion of optimality previously observed only for simple multisensory stimuli.</p></abstract><counts><page-count count="14"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Vision often plays a crucial role in understanding speech. Watching a speaker's facial movements, especially lip movements, provides input that can supplement the information from the speaker's voice. &#x0201c;Lip-reading&#x0201d; or &#x0201c;speech-reading&#x0201d; allows hearing-impaired individuals to understand speech (e.g. <xref ref-type="bibr" rid="pone.0004638-Campbell1">&#x0005b;1&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Bernstein1">&#x0005b;2&#x0005d;</xref>), and in subjects with intact hearing abilities, substantially facilitates speech perception under noisy environmental conditions <xref ref-type="bibr" rid="pone.0004638-Grant1">&#x0005b;3&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-MacLeod1">&#x0005b;4&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Massaro1">&#x0005b;5&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Bernstein2">&#x0005b;6&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Grant2">&#x0005b;7&#x0005d;</xref>. This benefit has been quantified by measuring performance enhancement due to visual input as a function of auditory noise <xref ref-type="bibr" rid="pone.0004638-Sumby1">&#x0005b;8&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Erber1">&#x0005b;9&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Erber2">&#x0005b;10&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Erber3">&#x0005b;11&#x0005d;</xref>. In these experiments, participants were asked to identify spoken words from a checklist, delivered during an auditory-alone condition and during an auditory-visual condition in which the speaker's face was visible. The benefit from the visual information, measured in percent correct, was found to be greatest when the auditory stimulus was most noisy (but see <xref ref-type="bibr" rid="pone.0004638-Binnie1">&#x0005b;12&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-McCormick1">&#x0005b;13&#x0005d;</xref>). This seems to be evidence for inverse effectiveness, a widely cited concept stating that the largest multisensory enhancement is expected when a unisensory stimulus is weakest <xref ref-type="bibr" rid="pone.0004638-Meredith1">&#x0005b;14&#x0005d;</xref>. However, when multisensory word recognition was tested under more natural conditions (without a checklist), maximal gain was found not at low, but at intermediate signal-to-noise ratios (SNRs) <xref ref-type="bibr" rid="pone.0004638-Ross1">&#x0005b;15&#x0005d;</xref>, in apparent contradiction to inverse effectiveness.</p><p>Here, we first replicate and extend the findings by Ross et al. <xref ref-type="bibr" rid="pone.0004638-Ross1">&#x0005b;15&#x0005d;</xref>. We then examine human performance when veridical visual speech information is replaced by purely temporal visual information and find that a minimum sound quality is required for such visual input to improve performance. This is again inconsistent with inverse effectiveness. We formulate a Bayesian cue integration model that explains these behavioral findings. In Bayesian cue integration, the relative reliabilities of cues are taken into account when inferring the identity of the source stimulus. For simple stimuli, human multisensory integration has been shown to be close to Bayes-optimal (e.g. <xref ref-type="bibr" rid="pone.0004638-Alais1">&#x0005b;16&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Ernst1">&#x0005b;17&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-vanBeers1">&#x0005b;18&#x0005d;</xref>). For identification tasks using multidimensional stimuli such as speech, the Bayesian model predicts visual enhancements that are largest at intermediate auditory SNRs, provided that a sufficiently large vocabulary is used. Inverse effectiveness is predicted only when the underlying feature space is low-dimensional. To further test the Bayesian theory, we generate a prediction for the perceptual integration of slightly incongruent auditory and visual stimuli: at very low SNR, the percentage of reported words that match the visual stimulus should increase as SNR increases, even though the weight to vision decreases. We report behavioral data confirming this counterintuitive prediction. Together, these results suggest that Bayesian optimality of cue integration is not limited to simple stimuli.</p></sec><sec sec-type="methods" id="s2"><title>Methods</title><sec id="s2a"><title>Psychophysics</title><sec id="s2a1"><title>Subjects</title><p>Thirty-three volunteer subjects (14 female) were recruited among the student population at CCNY and gave informed consent (written) in accordance with the guidelines of the IRB at CCNY. Seventeen subjects participated in the first experiment, which only contained matching auditory and visual stimuli (congruent), while 16 subjects participated in the second experiment, which also included conflicting auditory-visual stimuli (incongruent). Subjects were native American-English speakers or learned English when they were young. All participants had normal or corrected-to-normal vision and reported normal hearing.</p></sec><sec id="s2a2"><title>First experiment</title><p>Auditory (A) and auditory-visual (AV) stimuli were the same as in <xref ref-type="bibr" rid="pone.0004638-Ross1">&#x0005b;15&#x0005d;</xref>. 546 Simple monosyllabic English words were selected from a well-characterized normed set based on their written-word frequency <xref ref-type="bibr" rid="pone.0004638-Kucera1">&#x0005b;19&#x0005d;</xref>. These high-frequency words, uttered in isolation by a female American-English native speaker, were recorded as audio and video, and reproduced to subjects as audio alone (A) or as audio and video together (AV). Stationary acoustic noise with a 1/<italic>f</italic>-spectrum and a frequency range of 3 Hz to 16 kHz was presented, extending 1.5 s before and 1 s after the speech sound. Video was presented on a 17-inch LCD monitor with the face extending a visual angle of approximately 15&#x000b0;. Speech sound was played back from a centrally located loudspeaker, and the noise from two lateral speakers, all at 50 cm distance from the subject (see <xref ref-type="fig" rid="pone-0004638-g001">Figure 1</xref>). This configuration was originally chosen to allow for spatial auditory cues that may interact with visual cues. The A condition included a stationary visual stimulus to indicate to the subject the onset and offset of the speech sound (face with mouth closed or mouth open). This controlled for a bias in attention, which may otherwise favor the AV condition, since the video may give the subject a clue as to when to attend to the auditory stimulus (this contrasts the experiment in <xref ref-type="bibr" rid="pone.0004638-Ross1">&#x0005b;15&#x0005d;</xref> which did not indicate speech onset in the A condition). Speech was presented at a constant 50 dB sound pressure level and noise at levels between 50 dB and 74 dB in steps of 4 dB, resulting in an SNR ranging from 0 dB to &#x02212;24 dB. To generate the modified video sequence (V&#x0002a; stimulus, AV&#x0002a; condition) we used a video synthesis program that can generate a face which is similar in appearance to a given natural face <xref ref-type="bibr" rid="pone.0004638-LehnSchioler1">&#x0005b;20&#x0005d;</xref>. We used natural faces instead of artificial visual stimuli as they are known to generate the largest auditory-visual enhancements in speech recognition <xref ref-type="bibr" rid="pone.0004638-Lidestam1">&#x0005b;21&#x0005d;</xref>. The method used features extracted from the clean audio signal to generate articulations of the mouth, eyes, brows, and outline of the face. From these, realistic video frames were generated (for details, see the Supporting Information and <xref ref-type="supplementary-material" rid="pone.0004638.s001">Figure S1</xref>). Here we used the power of the audio (in time frames of 40 ms) as the only feature to generate the video. Hence, the V&#x0002a; stimulus can only represent visual information associated with the overall intensity fluctuations of the signal in time. The video cannot reflect any information associated with the detailed spectral content of the original speech signal. It may, at most, convey broad phonetic characterizations such as vowel versus consonant (vowels tend to have higher energy content).In each of the 3 conditions (A, AV, AV&#x0002a;), 26 words were presented at each of 7 SNR levels. Each word was presented only once, resulting in a total of 546 words (26&#x000d7;7&#x000d7;3). Stimuli were identical for all subjects to reduce cross-subject variability.</p><fig id="pone-0004638-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0004638.g001</object-id><label>Figure 1</label><caption><title>Experimental set-up and timing of audio-visual stimuli.</title></caption><graphic xlink:href="pone.0004638.g001"/></fig></sec><sec id="s2a3"><title>Second experiment</title><p>The second experiment, which included incongruent stimuli, included four conditions: visual-only (V), auditory-only (A), congruent auditory-visual (A&#x0200a;&#x0003d;&#x0200a;V), and incongruent auditory-visual (A&#x02260;V). Auditory and visual stimuli were selected from the same set of words as above. The A&#x02260;V condition presented the sound of one word while showing a face uttering a different but similar-sounding word. To select similar-sounding words, we computed the correlations of spectrograms of all word pairs within a set of 700 words and selected pairs with the highest correlation. As before, the 700 words were selected as the most frequent monosyllabic words, following <xref ref-type="bibr" rid="pone.0004638-Kucera1">&#x0005b;19&#x0005d;</xref>. Words with homophones were excluded. Words were presented only once, either as video or as audio. The V condition was presented with no sound, while the A condition was presented with a static visual stimulus as above, to control for attention. The noise had the same timing and spectral characteristics as above, but SNR was now adjusted in the range of &#x02212;28 dB to &#x02212;8 dB by varying the level of the speech signal and keeping the noise at a constant 50 dB (the intention was to help subjects maintain an equal level of effort despite the low SNR in some of the trials). Fourteen subjects were also tested with pure auditory noise (&#x02212;&#x0221e; dB). No significant behavioral difference was found between &#x02212;28 dB and this pure-noise condition, suggesting that at this lowest noise level, speech is fully masked by the noise. The &#x02212;28 dB condition was included to capture the predicted increase of visual reports at low SNR. Higher SNR conditions were omitted to limit the total duration of the experiment. To prevent subjects from noticing that stimuli were incongruent, the A&#x02260;V condition was tested only up to &#x02212;12 dB.</p><p>Each of the 4 conditions (A, V, A&#x0200a;&#x0003d;&#x0200a;V, A&#x02260;V) included 40 trials at each SNR level. In the A&#x02260;V condition, this made for a total of 400 words used (5 SNR levels&#x000d7;40 trials&#x000d7;2 words per trial). There were no repetitions among these 400 words. For the 6 SNR levels of the A and the A&#x0200a;&#x0003d;&#x0200a;V conditions, as well as the V-alone condition, a total of 520 words were drawn from the same pool (6&#x000d7;2&#x0002b;1&#x0200a;&#x0003d;&#x0200a;13 combinations of condition and SNR level; 40 trials each; 13&#x000d7;40&#x0200a;&#x0003d;&#x0200a;520). There were no repetitions among these 520 words, but overlap with the 400 words in the A&#x02260;V condition could not be avoided. Stimuli were identical for all subjects to reduce cross-subject variability.</p></sec><sec id="s2a4"><title>Procedures</title><p>Except for varying SNR levels, the noise was identical in all conditions in order to reduce variability, while the presentation of all stimuli was fully randomized to control for potential learning effects. A brief instruction was shown to participants before the experiment. Participants were required to write down the words they identified, and asked to note when they did not recognize a word. Subjects had no time constraints to give their response, but answered in 5&#x02013;10 seconds, making each experiment last approximately 90 minutes. For classification as correct, we insisted on correct spelling. After the experiment, participants were presented with the full list of words used in the experiment and asked to indicate any words they did not know. These words were then excluded from the analysis and no longer presented to subsequent subjects.</p></sec></sec><sec id="s2b"><title>Model</title><sec id="s2b1"><title>Background: words as points in a high-dimensional feature space</title><p>Traditionally, words are conceived of as a sequence of phonemes, with phonemes representing the elementary carriers of word identity. Classic phonetic features are grouped in categories such as place of articulation, voicing, and manner. These phonetic features have been derived empirically focusing on auditory stimuli. However, the definition of relevant phonemes depends also on the type of stimulus <xref ref-type="bibr" rid="pone.0004638-Jiang1">&#x0005b;22&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Mattys1">&#x0005b;23&#x0005d;</xref>. For instance in speech-reading, the visual stimulus may not be sufficient to disambiguate among distinct phonemes (e.g. in words such as &#x02018;pad, &#x02018;bat&#x02019;, and &#x02018;mat&#x02019;, the phonemes /p/, /b/, and /m/ are difficult to disambiguate visually and may be considered the same &#x02018;viseme&#x02019; <xref ref-type="bibr" rid="pone.0004638-Fisher1">&#x0005b;24&#x0005d;</xref>). Similarly, an auditory stimulus distorted by noise, or degraded due to hearing loss will no longer communicate some phonetic features <xref ref-type="bibr" rid="pone.0004638-Wang1">&#x0005b;25&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Wang2">&#x0005b;26&#x0005d;</xref>. The specific phonetic identification depends therefore on the specifics of the audio-visual speech stimulus. Given this dependence on the stimulus, there has been an effort to automatically extract relevant auditory and visual features directly from the stimulus in conjunction with behavioral experiments on phoneme identification <xref ref-type="bibr" rid="pone.0004638-Jiang1">&#x0005b;22&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Auer1">&#x0005b;27&#x0005d;</xref>. These experiments, and the associated computational and modeling approaches, by and large have converged on the notion that words can be represented by a conjunction of features, with each phoneme in a word contributing a set of features. This feature space can be generally thought of as a topographic space with well-defined neighborhood relationships <xref ref-type="bibr" rid="pone.0004638-Luce1">&#x0005b;28&#x0005d;</xref>. For instance, words that are &#x0201c;close by&#x0201d; are more likely to be confused when the stimulus is distorted by noise. In this feature space, words are not evenly distributed, and words that are clustered in high-density regions are harder to recognize <xref ref-type="bibr" rid="pone.0004638-Luce1">&#x0005b;28&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Auer2">&#x0005b;29&#x0005d;</xref>. The conjunction of phonetic features of several phonemes can make this space rather high-dimensional. However, not all phonetic combinations occur equally likely, and even fewer combinations represent actual words in a lexicon <xref ref-type="bibr" rid="pone.0004638-MacEachern1">&#x0005b;30&#x0005d;</xref>. Such phonotactic and lexical constraints allow accurate word identification even in a reduced phonetic representation <xref ref-type="bibr" rid="pone.0004638-Mattys1">&#x0005b;23&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Auer2">&#x0005b;29&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Iverson1">&#x0005b;31&#x0005d;</xref>. Essentially, in the high-dimensional joint feature space, many areas have a zero probability of containing a lexically correct word. Empirical evidence also suggests that high-frequency words are easier to recognize, implying that the prior probability of a given word plays a role in correct identification <xref ref-type="bibr" rid="pone.0004638-Luce1">&#x0005b;28&#x0005d;</xref>.</p></sec><sec id="s2b2"><title>Bayes-optimal word recognition in n-dimensional space</title><p>We present a first-principles model for multisensory word recognition that captures the main concepts of a stimulus neighborhood in high-dimensional feature space, where the reliability of the signal affects the size of the neighborhood and lexical information is represented by the distribution of words.</p><p>Let us assume that there are <italic>n</italic> features and that the observer's vocabulary can be represented by points in this <italic>n</italic>-dimensional space, which we will call word prototypes. Different speakers, different articulation, and noise will induce variability in the perceived stimulus for a given word. We assume that these noisy examplars of the word are represented in the observer's brain within some neighborhood of the prototype. We characterize their distribution by a <italic>n</italic>-dimensional normal distribution centered at the prototype. An important distinction from previous models is that we do not differentiate explicitly between visual and auditory features. Both the auditory and visual stimulus contribute to the observer's estimate for each feature dimension. The variance associated with these estimates may differ for the auditory versus the visual stimulus. In this view, a feature that is primarily auditory is characterized by a smaller variance afforded by the auditory than by the visual stimulus. Moreover, we will allow for correlated features.</p><p>The process of word identification is modeled as follows (for details, see the Supporting Information). First, we define the generative model, also called noise model. For a given vocabulary size <italic>N</italic>, word prototypes are denoted by vectors <inline-formula><inline-graphic xlink:href="pone.0004638.e001.jpg" mimetype="image"/></inline-formula> (<italic>i</italic>&#x0200a;&#x0003d;&#x0200a;1&#x02025;<italic>N</italic>) and are randomly drawn from a <italic>n</italic>-dimensional normal distribution. On each trial, a test word <inline-formula><inline-graphic xlink:href="pone.0004638.e002.jpg" mimetype="image"/></inline-formula> is presented to the subject and gives rise to noise-perturbed exemplars <inline-formula><inline-graphic xlink:href="pone.0004638.e003.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0004638.e004.jpg" mimetype="image"/></inline-formula> in the subject's brain. These are sampled from Gaussian distributions with mean at <inline-formula><inline-graphic xlink:href="pone.0004638.e005.jpg" mimetype="image"/></inline-formula> and covariance matrices <inline-formula><inline-graphic xlink:href="pone.0004638.e006.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0004638.e007.jpg" mimetype="image"/></inline-formula>, respectively (which do not depend on <inline-formula><inline-graphic xlink:href="pone.0004638.e008.jpg" mimetype="image"/></inline-formula>). We model the overall level of reliability of the stimuli by non-negative scalars, <italic>r<sub>A</sub></italic> for auditory and <italic>r<sub>V</sub></italic> for visual. These parameters are usually under experimental control &#x02013; for example, increasing the auditory signal-to-noise ratio leads to an increase in <italic>r<sub>A</sub></italic>. The covariance matrices <inline-formula><inline-graphic xlink:href="pone.0004638.e009.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0004638.e010.jpg" mimetype="image"/></inline-formula> are scaled by factors of <inline-formula><inline-graphic xlink:href="pone.0004638.e011.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0004638.e012.jpg" mimetype="image"/></inline-formula>, respectively. The equivalent of such a scaling in one dimension would be that reliability is inversely proportional to the standard deviation of the noise distribution, and therefore closely related to the <inline-formula><inline-graphic xlink:href="pone.0004638.e013.jpg" mimetype="image"/></inline-formula> measure.</p><p>Having specified the generative model, we can now formalize the Bayesian inference process which &#x0201c;inverts&#x0201d; it. To the subject's nervous system, the exemplars <inline-formula><inline-graphic xlink:href="pone.0004638.e014.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0004638.e015.jpg" mimetype="image"/></inline-formula> are known, while <inline-formula><inline-graphic xlink:href="pone.0004638.e016.jpg" mimetype="image"/></inline-formula> is not; for the experimenter, the reverse holds. On each multisensory trial, <inline-formula><inline-graphic xlink:href="pone.0004638.e017.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0004638.e018.jpg" mimetype="image"/></inline-formula> provide the brain with a likelihood function <inline-formula><inline-graphic xlink:href="pone.0004638.e019.jpg" mimetype="image"/></inline-formula>, i.e. a function over (not necessarily lexically correct) utterances <bold>w</bold>, indicating how probable it was that each has given rise to <inline-formula><inline-graphic xlink:href="pone.0004638.e020.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0004638.e021.jpg" mimetype="image"/></inline-formula>. Assuming that auditory and visual noise are independent, this likelihood function is the product of both unisensory likelihood functions, i.e. <inline-formula><inline-graphic xlink:href="pone.0004638.e022.jpg" mimetype="image"/></inline-formula> (see <xref ref-type="fig" rid="pone-0004638-g002">Figure 2a</xref>). The unisensory likelihood functions are defined by the noise model outlined above, and consequently, the multisensory likelihood function will also be an <italic>n</italic>-dimensional Gaussian, with mean at <inline-formula><inline-graphic xlink:href="pone.0004638.e023.jpg" mimetype="image"/></inline-formula> and covariance matrix <inline-formula><inline-graphic xlink:href="pone.0004638.e024.jpg" mimetype="image"/></inline-formula>. The utterance <inline-formula><inline-graphic xlink:href="pone.0004638.e025.jpg" mimetype="image"/></inline-formula> is the multisensory maximum-likelihood estimate. The well-known one-dimensional analogs of these expressions are <inline-formula><inline-graphic xlink:href="pone.0004638.e026.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0004638.e027.jpg" mimetype="image"/></inline-formula> (e.g. <xref ref-type="bibr" rid="pone.0004638-Ernst1">&#x0005b;17&#x0005d;</xref>). The former means that the multisensory likelihood function will have its maximum closest to the peak of the likelihood function corresponding to the most reliable modality; the linear weights of both modalities are determined by their reliabilities. (Interestingly, <bold>&#x003bc;</bold><italic><sub>AV</sub></italic> does not necessarily lie on the line connecting <inline-formula><inline-graphic xlink:href="pone.0004638.e028.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0004638.e029.jpg" mimetype="image"/></inline-formula>.) The latter indicates that the multisensory likelihood function is narrower than both unisensory likelihood functions, indicating the benefit of combining both modalities.</p><fig id="pone-0004638-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0004638.g002</object-id><label>Figure 2</label><caption><title>Bayesian model of auditory-visual word recognition.</title><p>a. Inference process on a single multisensory trial. Word prototypes are points in a high-dimensional space (of which two dimensions are shown). The presented word (in red) gives rise to an auditory (&#x003bc;<italic><sub>A</sub></italic>) and a visual (&#x003bc;<italic><sub>V</sub></italic>) observation (which are the respective unisensory estimates if only one modality is presented). Based on these, the brain constructs likelihood functions over utterances w, indicated by muted-colored discs. The diameter of a disc is proportional to the standard deviation of the Gaussian. The auditory-visual likelihood is the product of the unisensory likelihoods and is centered at &#x003bc;<sub>AV</sub> (see text), which is the multisensory estimate on this trial. b. Across many repetitions of the test word, the estimates will form a distribution centered at the test word. The estimate distributions are shown as bright-colored discs for the auditory-alone (A), visual-alone (V), and auditory-visual (AV) conditions. Since the distributions &#x0201c;cover&#x0201d; many words, errors will be made. Note the different interpretations of the discs in a and b: single-trial likelihood functions, versus estimate distributions across many trials. c. Side view of the estimate distributions in b. The AV estimate distribution is sharper than both the A and the V distribution, leading to fewer errors. This indicates the advantage conferred by multisensory integration.</p></caption><graphic xlink:href="pone.0004638.g002"/></fig><p>Still on a single trial, we account for uneven word frequencies by multiplying the likelihood values of all utterances with prior probabilities. These are taken to be zero for non-lexical utterances and are assigned according to an exponential distribution for lexical words, <inline-formula><inline-graphic xlink:href="pone.0004638.e030.jpg" mimetype="image"/></inline-formula>, with a decay constant <inline-formula><inline-graphic xlink:href="pone.0004638.e031.jpg" mimetype="image"/></inline-formula>. Previous studies do not provide strong guidance on how to choose this prior. It is most likely a combination of frequency knowledge acquired before and during the experiment. Good fits to the data are possible with a variety of priors we have tried. This issue deserves further attention. Posterior probabilities are computed for all words in the vocabulary through <inline-formula><inline-graphic xlink:href="pone.0004638.e032.jpg" mimetype="image"/></inline-formula>. According to the model, the observer then reports the word with maximum posterior probability (for details, see Supporting Information). Trials for which the reported word was equal to the test word were counted as correct. The &#x0201c;correctness regions&#x0201d; for each word typically have heterogeneous and irregular boundaries.</p><p>In the generation of the word prototypes as well as the generation of noisy word exemplars we sampled from normal distributions. The <italic>k</italic>-dimensional correlation structure in the corresponding covariance matrices was generated by adding to the diagonal matrix a product, <inline-formula><inline-graphic xlink:href="pone.0004638.e033.jpg" mimetype="image"/></inline-formula>, of a <italic>n&#x000d7;k</italic>-dimensional matrix <bold>X</bold> with normally distributed coefficients and an adjustable scale.</p><p>Across many trials, the maximum-likelihood estimates (either auditory, visual, or auditory-visual) of a given word form a probability distribution, as illustrated in <xref ref-type="fig" rid="pone-0004638-g002">Figure 2b</xref>. It turns out that when all distributions are Gaussian, the covariance matrix of this distribution is equal to that of a corresponding single-trial likelihood function (A, V, or AV). Therefore, estimation precision is governed by stimulus reliability, and many papers only discuss the estimate distributions. However, it is important to keep in mind that a full likelihood function is encoded on a single trial. This is particularly important when the prior distribution is not uniform.</p></sec><sec id="s2b3"><title>Fitting the models to the behavioral data</title><p>To relate the Bayesian model to the behavioral data we have to identify the relationship between auditory reliability <inline-formula><inline-graphic xlink:href="pone.0004638.e034.jpg" mimetype="image"/></inline-formula> and SNR. As SNR increases, the reliability of the auditory signal increases monotonically. Here, we simply assume a rectified linear relationship between SNR measured in dB (a logarithmic scale) and reliability: <inline-formula><inline-graphic xlink:href="pone.0004638.e035.jpg" mimetype="image"/></inline-formula>, where &#x003b1; and &#x003b2; are constants and &#x0005b;&#x000b7;&#x0005d;<sub>&#x0002b;</sub> sets negative arguments to zero. The data is fit by first optimizing &#x003b1; and &#x003b2; in the A condition. The AV and AV&#x0002a; conditions are then fit by adjusting visual reliability <inline-formula><inline-graphic xlink:href="pone.0004638.e036.jpg" mimetype="image"/></inline-formula> separately for each. Throughout this paper, we plot performance as a function of SNR when behavioral data are fitted, and as a function of <inline-formula><inline-graphic xlink:href="pone.0004638.e037.jpg" mimetype="image"/></inline-formula> otherwise (as this is more general).</p><p>The percentage of correct identification was computed by testing over a large number of test words. Behavioral performance was fit to the model performance by using 1000 test words per data point (not to be confused with the number of vocabulary words <italic>N</italic>). The final performance curves according to the model were computed with 8000 test words per data point, to produce smoother traces.</p></sec></sec></sec><sec id="s3"><title>Results</title><sec id="s3a"><title>Summary of results</title><p>We first present the results of our behavioral experiment, showing that open-set word identification in noise does not follow inverse effectiveness (<xref ref-type="fig" rid="pone-0004638-g003">Figure 3</xref>). In both the AV and the AV&#x0002a; condition, the enhancement due to additional visual information is maximal not at the highest but at an intermediate or low noise level.</p><fig id="pone-0004638-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0004638.g003</object-id><label>Figure 3</label><caption><title>Behavioral performance in open-set word recognition.</title><p>Data consisted of auditory-alone performance (blue) and auditory-visual performance (green). The multisensory enhancement (red) is the difference between auditory-visual and auditory-alone performance. Error bars indicate s.e.m. a: Full visual information (AV). b: Impoverished visual information (AV&#x0002a;). In both cases, maximum enhancement occurs at intermediate values of auditory SNR.</p></caption><graphic xlink:href="pone.0004638.g003"/></fig><p>We then present results of the model that conceives of speech recognition as a Bayesian cue combination process. This model implements the key statistical properties of phonetic features and lexical information that are known to affects human speech recognition performance. The computations show that Bayesian inference produces multisensory enhancements that do not decline monotonically with SNR but have a maximum at intermediate SNR. The resulting performance curves are shown to fit the present behavioral data with high accuracy (<xref ref-type="fig" rid="pone-0004638-g004">Figure 4a</xref>). We next modeled the auditory-visual enhancement when visual reliability is reduced and find that the model effects are consistent with the behavioral results of the impoverished visual condition AV&#x0002a; (<xref ref-type="fig" rid="pone-0004638-g004">Figures 4b</xref> and <xref ref-type="fig" rid="pone-0004638-g005">5a</xref>). We show that words in higher-density regions are harder to recognize (<xref ref-type="fig" rid="pone-0004638-g004">Figure 4c</xref>), consistent with earlier findings. We also show that when vocabulary size is reduced, the enhancements resemble earlier behavioral data on speech perception in noise, which used checklists instead of an open word set (<xref ref-type="fig" rid="pone-0004638-g005">Figure 5b</xref>).</p><fig id="pone-0004638-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0004638.g004</object-id><label>Figure 4</label><caption><title>A Bayesian model of speech perception can describe human identification performance.</title><p>A vocabulary of size <italic>N</italic>&#x0200a;&#x0003d;&#x0200a;2000 was used. Words were distributed in an irregular manner in a space of dimension <italic>n</italic>&#x0200a;&#x0003d;&#x0200a;40. For details of the fits, see the Supplemental Material. a: Data (symbols) and model fits (lines) for A-alone and AV conditions. The red line is the multisensory enhancement obtained from the model. b: Same for impoverished visual information (AV&#x0002a;). c: Words in high-density regions are harder to recognize. In the simulation in a, words were categorized according to their mean distance to other words. When the mean distance is large (sparse, solid lines), recognition performance in both A-alone and AV conditions is higher than when the mean distance is small (dense, dashed lines).</p></caption><graphic xlink:href="pone.0004638.g004"/></fig><fig id="pone-0004638-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0004638.g005</object-id><label>Figure 5</label><caption><title>Predictions of the Bayesian model for auditory-visual enhancement as a function of auditory SNR, for various values of:</title><p>a: visual reliability (from 0.05 to 0.95 in steps of 0.10); b: vocabulary size. For both plots, all other parameters were taken from the fit in <xref ref-type="fig" rid="pone-0004638-g004">Figure 4</xref>. See <xref ref-type="sec" rid="s3">Results</xref> for interpretation.</p></caption><graphic xlink:href="pone.0004638.g005"/></fig><p>We then provide evidence that these numerical results are a robust property of the model and do not depend on specific parameter choices. In particular, we show that the predicted performance curves show the same trends when we compute rigorous analytic expressions for a strongly simplified high-dimensional model (<xref ref-type="fig" rid="pone-0004638-g006">Figure 6</xref>). Moreover, we show rigorously that in 1 and 2 dimensions, optimal cue integration does follow an inverse-effectiveness rule. This suggests that high dimensionality of the feature space is both necessary and sufficient for the results to hold.</p><fig id="pone-0004638-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0004638.g006</object-id><label>Figure 6</label><caption><title>Optimal cue combination in multiple dimensions according to a simple analytical model.</title><p>a. In this simplified model, word prototypes (dots) lie on a rectangular grid, here shown in two dimensions. The green blob indicates an example estimate distribution (compare <xref ref-type="fig" rid="pone-0004638-g002">Fig. 2b</xref>). The dashed lines enclose the correctness region when the central word is presented. b and c. The model was fitted to the data in the AV condition (b) and the AV&#x0002a; condition (c). Data are shown as symbols, lines are model fits. Colors are as in <xref ref-type="fig" rid="pone-0004638-g003">Fig. 3</xref>. d. The same model in 1 dimension, but now allowing word prototypes to be unequally spaced. The green curve is an estimate distribution. The vertical dashed lines are the boundaries of the decision regions. The shaded area corresponds to correct responses when the presented stimulus is the one marked in red. e. Typical identification performance in 1 dimension, for the A (blue) and AV (green) conditions. The multisensory enhancement (red) decreases monotonically with auditory reliability. This is an instance of inverse effectiveness. For details, see the Supporting Information.</p></caption><graphic xlink:href="pone.0004638.g006"/></fig><p>The modeling efforts conclude with predictions for the case of cue conflict (incongruence), i.e. visual and auditory stimuli that do not represent the same word (<xref ref-type="fig" rid="pone-0004638-g007">Figure 7a</xref>). Finally, we present results of a subsequent behavioral experiment which confirm these theoretical predictions (<xref ref-type="fig" rid="pone-0004638-g007">Figure 7b</xref>), lending further support to the hypothesis that human speech identification follows Bayes-optimal cue combination. The congruent trials in this experiment show unambiguously that multisensory integration occurs at all SNR levels.</p><fig id="pone-0004638-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0004638.g007</object-id><label>Figure 7</label><caption><title>Effect of an auditory word on reports of an incongruent visual word.</title><p>a. Illustration of the Bayesian prediction. An experiment was simulated in which pairs of slightly incongruent auditory and visual words are presented. On each trial, the observer integrates the signals and reports a single word. Frequencies of reporting the auditory word (cyan), the visual word (magenta), and other words (brown) are shown as a function of auditory reliability. As auditory reliability increases, the percentage reports of the visual word reaches a maximum before it eventually decreases. This is a small but significant effect. Note that the interpretation of both curves is completely different from that of <xref ref-type="fig" rid="pone-0004638-g003">Figures 3</xref>&#x02013;<xref ref-type="fig" rid="pone-0004638-g004">4</xref> (here, the only condition is multisensory, and there is no notion of correctness). A vocabulary of size <italic>N</italic>&#x0200a;&#x0003d;&#x0200a;2000 and dimension <italic>n</italic>&#x0200a;&#x0003d;&#x0200a;30 were used, and visual reliability was fixed at <italic>r<sub>V</sub></italic>&#x0200a;&#x0003d;&#x0200a;0.5. Robustness of the effect across dimensions and vocabulary sizes is demonstrated in <xref ref-type="supplementary-material" rid="pone.0004638.s005">Figure S5</xref>. b. Experimental test of the Bayesian prediction. The percentage reports of the visual word exhibits a maximum as a function of SNR. The curves in a have not been fitted to those in b. c. Reports of the visual word as a percentage of the total reports of either the auditory or the visual word, computed from the data shown in b. As expected, this declines monotonically with SNR.</p></caption><graphic xlink:href="pone.0004638.g007"/></fig></sec><sec id="s3b"><title>Behavioral performance in an open-set word identification task does not follow inverse effectiveness</title><p>Monosyllabic words were presented in an auditory (A) and auditory-visual (AV) condition under varying noise levels. Subjects responded in writing which word they identified. In addition to the original video, a modified video sequence was generated for each word and presented together with the corresponding original audio (AV&#x0002a;). The goal of this modified video sequence was to represent only temporal information, but not spectral information. The rates of correct identification are shown in <xref ref-type="fig" rid="pone-0004638-g003">Figure 3</xref> and confirm previous literature on the benefits of auditory-visual speech in noise.</p><p>In the AV condition (<xref ref-type="fig" rid="pone-0004638-g003">Figure 3a</xref>), identification performance at all noise levels improves by adding the visual information, with the highest gains occurring at intermediate noise levels. The enhancements are large and statistically significant by any measure. When contrasting these results with the study by Ross et al. as well as our second experiment below, one can see that the specifics of the performance gains depend on the experimental protocol (see <xref ref-type="supplementary-material" rid="pone.0004638.s002">Figure S2</xref>). However, in all instances, the maximum gain for the AV condition is obtained at a SNR of approximately &#x02212;12 dB.</p><p>The enhancements in the AV&#x0002a; condition (<xref ref-type="fig" rid="pone-0004638-g003">Figure 3b</xref>) are smaller and were tested for significance as follows. A repeated-measures 2-way ANOVA shows a significant effect of the stimulus condition (AV&#x0002a; vs A) with <italic>F</italic>(1,32)&#x0200a;&#x0003d;&#x0200a;80.3 and a significant effect of SNR with <italic>F</italic>(6,224)&#x0200a;&#x0003d;&#x0200a;524. This means that adding the V&#x0002a; visual stimulus improves performance significantly with respect to the A-only condition. It also means, trivially, that performance varies with SNR. The ANOVA analysis shows a significant interaction between the two factors (<italic>F</italic>(6,224)&#x0200a;&#x0003d;&#x0200a;10.9), indicating that enhancement across the two conditions changes with SNR. (Compare this to the effect sizes in the AV vs A conditions, where we find <italic>F</italic>(1,224)&#x0200a;&#x0003d;&#x0200a;510 for the difference between conditions, <italic>F</italic>(6,224)&#x0200a;&#x0003d;&#x0200a;495 for the effect of SNR, and <italic>F</italic>(6,224)&#x0200a;&#x0003d;&#x0200a;25.0 for the interaction between the two factors.) A subsequent sequential paired t-test on each SNR (with Holms' correction for multiple comparisons) shows that there is an enhancement at high (less negative) SNR (for &#x02212;8 dB or higher) and no significant enhancement below &#x02212;12 dB. Put differently, for the AV&#x0002a; condition, a minimum auditory SNR is required before the additional visual stimulus can aid word identification. This indicates that performance enhancements follow the opposite trend from what one would expect for inverse effectiveness. Significance in all these tests falls at a <italic>p</italic>-value of 0.001 or less, except for the gain due to V&#x0002a; at &#x02212;12 dB, for which <italic>p</italic>&#x0003c;0.01.</p></sec><sec id="s3c"><title>Bayes-optimal cue combination in high dimensions predicts largest multisensory enhancement at intermediate noise levels</title><p>Speech recognition is a process in which perceived phonetic information is compared to a mental lexicon. Here we use a model that is broadly consistent with results from linguistics which describe how phonetic features are be integrated with lexical information (see <xref ref-type="sec" rid="s2">Methods</xref>). Briefly, the model regards word recognition as a Bayesian inference process in which vocabulary words are prototypes defined by a conjunction of phonetic features. A specific word stimulus corresponds to a point in this space and different instantiations of the same word are distributed in some proximity of the mean prototype (see <xref ref-type="fig" rid="pone-0004638-g002">Figure 2a</xref>). To mimic the varying similarity or distinctiveness of vocabulary words the prototypes were chosen to be unevenly distributed in this feature space, with close-by prototypes representing similar words. Each prototype word is assigned a prior likelihood to be observed thus capturing the uneven frequency of occurrence of different words in natural speech. In addition, we allow features to be correlated, which relaxes restrictions of previous models that often implicitly assume phonemes to be independent <xref ref-type="bibr" rid="pone.0004638-Luce1">&#x0005b;28&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Braida1">&#x0005b;32&#x0005d;</xref>.</p><p>We computed identification performance simulating auditory-only and auditory-visual stimulation for various values of auditory and visual reliability. The results of the model are shown in <xref ref-type="fig" rid="pone-0004638-g004">Figure 4</xref> with suitably chosen parameters. To compare the results to behavioral data, they are plotted here as a function of auditory SNR. A rectified-linear relation between auditory reliability and SNR was assumed, with parameters determined by fitting the performance curve to the auditory-alone conditions. As SNR is increased, the model shows that performance reaches a maximum and ultimately decreases. Indeed, the model replicates the behavioral data with high accuracy (<inline-formula><inline-graphic xlink:href="pone.0004638.e038.jpg" mimetype="image"/></inline-formula> for conditions A, AV, and AV&#x0002a; respectively). For this example, we chose <italic>N</italic>&#x0200a;&#x0003d;&#x0200a;2000 words as an estimate of the number of uniquely spoken monosyllabic words that may be known by our subject population (John Lawler, personal communication), <inline-formula><inline-graphic xlink:href="pone.0004638.e039.jpg" mimetype="image"/></inline-formula> dimensions, and uncorrelated features. This left only 4 free parameters: 2 for the relation between auditory SNR and reliability, and 2 for the visual reliabilities in the AV and AV&#x0002a; conditions. Dimensionalities between 20 and 50 and vocabulary sizes of 800&#x02013;3000 words give equally good results (see Supporting <xref ref-type="supplementary-material" rid="pone.0004638.s003">Figure S3a</xref>). This makes it impossible to reliably determine the parameter values from these data, but it speaks in favor of the generality of the qualitative conclusion that the behavioral data can be explained by a Bayesian model as long as dimension and vocabulary size are sufficiently high. Also note that a small vocabulary size or a low feature space dimension cannot account for the data. Finally, we tested several cases of nonzero correlations of various ranks between features in the auditory or visual noise; these correlations had little or no effect on the reported performance curves.</p></sec><sec id="s3d"><title>Words in higher-density regions are harder to recognize</title><p>In earlier work using related models, it was found that words with more neighbors are harder to recognize <xref ref-type="bibr" rid="pone.0004638-Luce1">&#x0005b;28&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Auer2">&#x0005b;29&#x0005d;</xref>. In order to confirm that this is the case in the Bayesian model, we divided the vocabulary into two subsets according to the density of their neighbors. In the simulation used to fit the behavioral data (<xref ref-type="fig" rid="pone-0004638-g004">Figures 4a and 4b</xref>), each word has a roughly normal distribution of distances to other words. However, the mean of this distribution varies across words, with some words being in high-density and others in low-density regions. We defined the subsets by whether the mean distance of a word to other words is larger or smaller than the median mean distance. We computed performance separately for each subset and found that indeed, for both A and AV conditions, performance is better on words with a higher mean distance to other words (see <xref ref-type="fig" rid="pone-0004638-g004">Figure 4c</xref>).</p></sec><sec id="s3e"><title>Largest multisensory enhancement shifts to higher SNR as visual reliability decreases</title><p>The simulations for the AV and AV&#x0002a; conditions shown in <xref ref-type="fig" rid="pone-0004638-g004">Figures 4a and 4b</xref> are identical except for the values of visual reliability, with <inline-formula><inline-graphic xlink:href="pone.0004638.e040.jpg" mimetype="image"/></inline-formula> and <inline-formula><inline-graphic xlink:href="pone.0004638.e041.jpg" mimetype="image"/></inline-formula>, respectively. These values are consistent with the fact that the V&#x0002a; stimulus provides less reliable information. The auditory reliability at which maximum performance gain is attained depends on the reliability of the secondary modality. <xref ref-type="fig" rid="pone-0004638-g005">Figure 5a</xref> explores this behavior as a function of visual reliability. It shows that the maximum gain shifts to higher SNR as the reliability of the secondary modality increases. When the secondary modality is extremely uninformative, as in the AV&#x0002a; condition, the enhancement is very low at all SNR values and exhibits a maximum at high SNR. Therefore, we predict that subjects with an impaired ability to extract visual information will show their greatest multisensory enhancement at higher SNR than normal-vision controls.</p></sec><sec id="s3f"><title>Largest multisensory enhancement shifts to higher SNR as vocabulary size increases</title><p>The Bayesian model explains why the maximum multisensory enhancement occurs at intermediate values of SNR. This raises the question what was different in earlier behavioral experiments that found the largest enhancement at the lowest values of SNR <xref ref-type="bibr" rid="pone.0004638-Sumby1">&#x0005b;8&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Erber1">&#x0005b;9&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Erber4">&#x0005b;33&#x0005d;</xref>. It was hypothesized before <xref ref-type="bibr" rid="pone.0004638-Ross1">&#x0005b;15&#x0005d;</xref> that the number of words plays a crucial role, since in earlier studies the possible responses were restricted to a relatively short checklist. Therefore we checked in the numerical model the effect of vocabulary size on the multisensory enhancement function (see <xref ref-type="fig" rid="pone-0004638-g005">Figure 5b</xref>). Note that the vocabulary size is not the number of test words (which is kept constant), but the number of all monosyllabic words that the subject may consider in determining her response. All parameters were fixed at the values used in obtaining the fits of <xref ref-type="fig" rid="pone-0004638-g004">Figures 4a and 4b</xref>, except for the number of words in the vocabulary. We find that multisensory enhancement peaks at lower SNR as fewer words are considered. Therefore, with the vocabulary sizes used in earlier studies (e.g. at most 256 words in <xref ref-type="bibr" rid="pone.0004638-Sumby1">&#x0005b;8&#x0005d;</xref>), it is not surprising that inverse effectiveness was observed. When the maximum occurs at low SNR, but even lower levels of SNR are not used in the experiment, enhancement can appear to obey inverse effectiveness while in fact this is only a consequence of the limited SNR range used. The dependence of auditory-alone performance on set size is interesting in its own right <xref ref-type="bibr" rid="pone.0004638-Miller1">&#x0005b;34&#x0005d;</xref> and warrants further attention in the context of the Bayesian model.</p></sec><sec id="s3g"><title>Maximum enhancement at intermediate SNR is a generic property of Bayes-optimal cue combination in higher dimensions</title><p>The numerical modeling results replicate the behavioral data accurately, but does this depend critically on the specific modeling choices or the number of model parameters? Surprisingly, if we drop all the flexibility of the numerical model, and instead assume &#x02013; unrealistically &#x02013; that vocabulary words are uniformly distributed on a regular lattice of <italic>n</italic> independent features (see <xref ref-type="fig" rid="pone-0004638-g006">Figure 6a</xref>) we find that the main conclusions of the numerical model are preserved. This simplified case can be treated analytically. In Supporting <xref ref-type="supplementary-material" rid="pone.0004638.s004">Figures S4a&#x02013;d</xref>, we show examples of the multisensory enhancement computed analytically, for different values of the dimension <italic>n</italic> and the visual reliability <inline-formula><inline-graphic xlink:href="pone.0004638.e042.jpg" mimetype="image"/></inline-formula>. The curves replicate the observation that for higher dimensions the maximum performance gain is at intermediate values of auditory reliability (a mathematical proof is in the Supporting Information). It also confirms the numerical model result that higher auditory reliabilities are required to obtain maximal performance gain if the visual reliability is lower. Indeed, this simplified analytic model can explain the behavioral data with equally high accuracy (<inline-formula><inline-graphic xlink:href="pone.0004638.e043.jpg" mimetype="image"/></inline-formula> for A, AV, and AV&#x0002a; respectively; <xref ref-type="fig" rid="pone-0004638-g006">Figures 6b&#x02013;c</xref>). Moreover, in the analytical model, it can be proven (see Supporting Information) that in 1 and 2 dimensions, maximum performance gain occurs at the lowest value of SNR, consistent with inverse effectiveness (see <xref ref-type="fig" rid="pone-0004638-g006">Figures 6d&#x02013;e</xref>). In 1 dimension, the proof of this statement does not even require equal spacing of possible choices.</p></sec><sec id="s3h"><title>Predictions for behavioral performance in multisensory cue combination</title><p>The value of the present model does not lie merely in explaining existing data, but also in its generality, which permits to make predictions about yet unobserved behavior. In the previous sections, we already discussed predictions regarding the location of the largest multisensory enhancement upon changing the visual reliability or the vocabulary size. These conclusions are consistent with known data but have not yet been fully experimentally tested.</p><p>Assuming Bayes-optimal behavior, we also predict that human performance in multisensory classification tasks will violate inverse effectiveness whenever the space of task-relevant features is high-dimensional. This is not limited to speech. For example, if an observer has to identify complex objects among an unconstrained number of alternatives based on noisy visual and tactile cues, the enhancement induced by the tactile cue should show a peak at intermediate values of image noise.</p><p>Finally, it would be worthwhile to test our prediction of inverse effectiveness for low-dimensional stimuli. Several behavioral studies in cats <xref ref-type="bibr" rid="pone.0004638-Stein1">&#x0005b;35&#x0005d;</xref> and humans (<xref ref-type="bibr" rid="pone.0004638-Gillmeister1">&#x0005b;36&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Diederich1">&#x0005b;37&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Corneil1">&#x0005b;38&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Rach1">&#x0005b;39&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Serino1">&#x0005b;40&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Bolognini1">&#x0005b;41&#x0005d;</xref>; but see <xref ref-type="bibr" rid="pone.0004638-Holmes1">&#x0005b;42&#x0005d;</xref>) have claimed inverse effectiveness, but on different measures and in different conditions than the ones considered here.</p></sec><sec id="s3i"><title>Prediction for incongruent auditory-visual cues</title><p>So far, we have considered the case where visual speech is congruent with auditory speech. Extensive literature exists on human behavior in the presence of an incongruence, or cue conflict, between auditory and visual speech. Massaro studied such conflict stimuli in the context of the McGurk effect <xref ref-type="bibr" rid="pone.0004638-McGurk1">&#x0005b;43&#x0005d;</xref> and found that it was well described by a Bayesian-type rule <xref ref-type="bibr" rid="pone.0004638-Massaro1">&#x0005b;5&#x0005d;</xref>. Many of these experiments were conducted using a factorial design based on nearby phoneme pairs such as /ba/-/da/. The present study raises the question how human performance can be described when the presented words are part of a much larger vocabulary, as in the experiment discussed here. For nearby word pairs, such as &#x0201c;dear&#x0201d;-&#x0201c;tear&#x0201d; or &#x0201c;pay&#x0201d;-&#x0201c;bay&#x0201d;, which in noise may be easily confused, subjects may not realize the incongruence of the auditory and visual stimuli. Hence, they will tend to merge the cues and when that happens, we expect the model of Bayesian cue integration to predict human behavior, without any need for further assumptions. Since there are now two sources (an auditory word and a visual word), there is no longer a notion of correctness, but trials will fall into three groups: those on which the auditory word is reported, those on which the visual word is reported, and those on which a different word (distracter) is reported.</p><p>The Bayesian model predicts (as many other models would) that when one keeps the visual noise level constant and increases auditory SNR, the frequency of reports of the auditory word will increase and the frequency of reports of other words will decrease. However, surprisingly, it also predicts that the frequency of reports of the visual word will first increase and then decrease, despite the fact that the weight to vision decreases throughout. This follows from a numerical simulation similar to those for the congruent case, and is illustrated for specific parameter choices in <xref ref-type="fig" rid="pone-0004638-g007">Figure 7a</xref>. This prediction holds across a wide range of vocabulary sizes and dimensions (see <xref ref-type="supplementary-material" rid="pone.0004638.s005">Figure S5</xref>) and is confirmed by the analytical model (see Supporting Information). It is a counterintuitive prediction, as one might expect the reports of the visual word to decrease monotonically as the weight to vision decreases. The reason that this does not happen is because as auditory reliability increases, two effects occur (<xref ref-type="fig" rid="pone-0004638-g008">Figure 8</xref>): 1) the mean of the distribution of auditory-visual maximum-likelihood estimates shifts towards the auditory word (this is what is meant by a decreasing weight to vision); 2) the estimate distribution narrows, leading to the squashing of a large distracter set. The interaction of both effects determines the frequency of visual reports. At very low SNR, the width of the distribution is large compared to the distance between the auditory and the visual word. Therefore, the stronger effect is the second one: the probability mass accumulates in the neighborhood of both presented words, which benefits both, since they are very close to each other. Only when the distribution becomes narrow compared to the distance between the two words, the enhancement will benefit the auditory word more exclusively. All this assumes that visual reliability is relatively poor, so that there is a strong tendency to integrate, even at the highest auditory SNRs used.</p><fig id="pone-0004638-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0004638.g008</object-id><label>Figure 8</label><caption><title>A large distracter set gets squashed.</title><p>This figure illustrates the Bayesian model for integrating slightly incongruent auditory-visual stimuli. Dots represent word prototypes. The blue and orange dots represent the auditory and visually presented words, respectively. Each disc represents a Gaussian maximum-likelihood estimate distribution (A, V, or AV); its radius is proportional to the standard deviation of the Gaussian. a&#x02013;c differ in auditory reliability but not in visual reliability. In a, auditory reliability is zero, therefore the V and AV distributions are identical. As auditory reliability increases, the AV distribution sharpens (thereby excluding more and more distractors) and shifts more towards the auditory word. These two effects together initially benefit both the auditory and the visual word, since the visual word is close to the auditory word and enjoys some of the increased probability mass (compare a and b). Eventually, the benefit will go more exclusively to the auditory word (compare b and c). This explains why in <xref ref-type="fig" rid="pone-0004638-g007">Figure 7b</xref> the percentage of reports of the visual word in the AV condition first increases and then ultimately decreases. Note that the auditory and the visual word do not have to be nearest neighbors.</p></caption><graphic xlink:href="pone.0004638.g008"/></fig></sec><sec id="s3j"><title>Prediction on incongruent auditory-visual speech is confirmed by behavioral experiment</title><p>We tested the prediction for incongruent stimuli directly using the same set of auditory and visual words as in the first behavioral experiment. We selected words pairs based on the similarity of their spectrograms (see <xref ref-type="sec" rid="s2">Methods</xref>). This resulted in pairs such as &#x0201c;cry-dry&#x0201d;, &#x0201c;smack-snake&#x0201d; and &#x0201c;lost-rust&#x0201d;. For the incongruent stimuli, one of the two words is presented as audio and the other as video. The prediction requires that subjects do not detect this mismatch and instead fuse the auditory-visual information into a common percept. To ensure this, we interleaved unisensory and congruent multisensory trials and limited the SNR on incongruent trials to at most &#x02212;12 dB. Participants were informed of the incongruent condition only after the experiment. None of the subjects reported noticing an explicit mismatch between video and audio. The percentage of reported words that match the visual or auditory stimulus in the incongruent case (A&#x02260;V) are shown in <xref ref-type="fig" rid="pone-0004638-g007">Figure 7b</xref>. Evidently, the auditory reports increase with SNR, as expected. The trend for the visual reports seems to follow the prediction in <xref ref-type="fig" rid="pone-0004638-g007">Figure 7a</xref>. A one-way ANOVA comparing the percentages of visual reports shows that the difference across SNR is significant (<italic>p</italic>&#x0003c;0.02). Subsequent pairwise comparisons of the different SNR conditions confirm that visual reports at &#x02212;28 dB and &#x02212;12 dB are significantly lower than any of the intermediate SNR values (<italic>p</italic>&#x0003c;0.01 with Bonferroni correction). A simple quadratic fit to the data places the maximum at &#x02212;19&#x000b1;7 dB (<italic>R</italic><sup>2</sup>&#x0200a;&#x0003d;&#x0200a;0.2 when including data for individual subjects, <italic>p</italic>&#x0003c;0.005). As sound quality improves further, subjects are more likely to report correctly what they heard and thus the number of visual reports decreases. This obvious expectation is indeed confirmed here at an SNR above &#x02212;19 dB. The surprising prediction of the model, however, is that at the lowest SNR levels the trend should be reversed: the number of correctly reported visual words increases with increasing auditory reliability. This is indeed confirmed by the behavioral performance for SNRs below &#x02212;19 dB.</p><p>To verify that the increasing frequency of visual word reports is due to the suppression of distracters and not to increasing weight to vision, we plotted the frequency of visual word reports conditioned on the observers reporting either the auditory or the visual word (see <xref ref-type="fig" rid="pone-0004638-g007">Figure 7c</xref>). This ignores all distracters and only considers visual relative to auditory word reports. As expected, this shows a monotonic decline with auditory SNR.</p></sec><sec id="s3k"><title>Audio-visual integration occurred at all SNR levels</title><p>In the AV condition of the first experiment, identification performance at all noise levels improves by adding visual information. But at the same time, the AV performance is significantly greater than pure lip-reading performance at all SNR levels (<italic>p</italic>&#x0003c;0.01, corrected for multiple comparisons) if we assume the 7&#x00025; measured for the visual-only condition on this data by Ross et al. <xref ref-type="bibr" rid="pone.0004638-Ross1">&#x0005b;15&#x0005d;</xref>. To confirm this result, the second experiment measured the visual-only condition explicitly, resulting in a recognition performance of 5.3&#x000b1;1.5&#x00025; (see <xref ref-type="supplementary-material" rid="pone.0004638.s002">Figure S2b</xref>). A post-hoc paired <italic>t</italic>-test shows significant improvement over the V condition for the AV condition down to &#x02212;28 dB (<italic>p</italic>&#x0003c;0.0001). Hence, in these experiments, even marginal auditory information seems to aid in lip-reading (compare <xref ref-type="bibr" rid="pone.0004638-Rosen1">&#x0005b;44&#x0005d;</xref>, where voice pitch was used as an auditory cue) and multisensory integration is occurring at all SNR levels.</p></sec></sec><sec id="s4"><title>Discussion</title><sec id="s4a"><title>A case for Bayesian optimality</title><p>The benefits of speech-reading are well-documented (for a review see <xref ref-type="bibr" rid="pone.0004638-Campbell1">&#x0005b;1&#x0005d;</xref>) and have been described with computational models <xref ref-type="bibr" rid="pone.0004638-Auer2">&#x0005b;29&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Braida1">&#x0005b;32&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Massaro2">&#x0005b;45&#x0005d;</xref>. The notion that words form a neighborhood relationship in some high-dimensional features space was captured also by Luce's Neighborhood Activation Model (NAM) <xref ref-type="bibr" rid="pone.0004638-Luce1">&#x0005b;28&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Auer2">&#x0005b;29&#x0005d;</xref>. The model uses performance measures on individual phonemes to estimate the performance of identifying full words. Similar to the present work, it incorporates word frequency (prior likelihood) and expresses lexical information as permissible points in the joint feature space.</p><p>However, the present study is the first that puts the observed gains in the context of optimal inference. This work was based on the recent finding that maximum auditory-visual gain is obtained at intermediate instead of low auditory SNR levels <xref ref-type="bibr" rid="pone.0004638-Ross1">&#x0005b;15&#x0005d;</xref>, which contradicts the well-known principle of inverse effectiveness. We showed that even purely temporal visual information can improve speech understanding. This was remarkable considering that this impoverished information, by itself, did not allow any identification. Only when combined with a minimum of auditory signal was identification improved and the benefits increase with increasing SNR, opposite to what one would expect from inverse effectiveness.</p><p>We then presented a simple, yet rigorous model in which auditory-visual speech perception was treated as an inference process with noisy cues. We took into account the complexity of speech by conceptualizing words as points in a multidimensional space. The behavioral data in both conditions could be fitted very well, and in particular, the largest multisensory enhancement occurred at intermediate auditory SNR. All else being equal, a decrease in the reliability of the secondary modality or an increase in the number of alternatives causes multisensory enhancement to stray further from inverse effectiveness. In spite of this breakdown, performance is completely consistent with a Bayesian model of cue integration.</p><p>Numerous studies have shown that humans are nearly Bayes-optimal in combining simple perceptual cues, even in the presence of a small conflict between the cues <xref ref-type="bibr" rid="pone.0004638-Alais1">&#x0005b;16&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Ernst1">&#x0005b;17&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-vanBeers1">&#x0005b;18&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Roach1">&#x0005b;46&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Knill1">&#x0005b;47&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Shams1">&#x0005b;48&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Hillis1">&#x0005b;49&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Battaglia1">&#x0005b;50&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Rowland1">&#x0005b;51&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Kording1">&#x0005b;52&#x0005d;</xref>, sensorimotor integration <xref ref-type="bibr" rid="pone.0004638-Kording2">&#x0005b;53&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Ghahramani1">&#x0005b;54&#x0005d;</xref>, and other forms of cue combination <xref ref-type="bibr" rid="pone.0004638-Knill2">&#x0005b;55&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Knill3">&#x0005b;56&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Jacobs1">&#x0005b;57&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Knill4">&#x0005b;58&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Landy1">&#x0005b;59&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Brouwer1">&#x0005b;60&#x0005d;</xref>. This suggests that in multisensory integration, Bayesian optimality is a very general principle, much more so than inverse effectiveness. Moreover, it is extremely difficult to attach any intuition to inverse effectiveness (or lack thereof), while Bayesian optimality is naturally interpreted in terms of the sharpening of probability distributions (see <xref ref-type="fig" rid="pone-0004638-g002">Figure 2</xref>).</p><p>The present model of Bayes-optimal cue combination was used to make a series of predictions. The prediction on the perception of incongruent auditory-visual stimuli was indeed confirmed by a subsequent experiment. This demonstrates the power of the model not only to explain existing results but to generalize to new situations.</p></sec><sec id="s4b"><title>Benefits of temporal information</title><p>Previous behavioral experiments show that many forms of synchronous video can improve auditory perception: simultaneous video can reduce detection thresholds of spoken sentences in noise <xref ref-type="bibr" rid="pone.0004638-Grant3">&#x0005b;61&#x0005d;</xref> and just seeing a speaker's head movement can improve word identification <xref ref-type="bibr" rid="pone.0004638-Thomas1">&#x0005b;62&#x0005d;</xref>. Even more strikingly, syllable identification can be improved when an identical visual stimulus is shown for different syllables <xref ref-type="bibr" rid="pone.0004638-Schwartz1">&#x0005b;63&#x0005d;</xref>. The present study uses only temporal visual information and explains the enhancement effects using a probabilistic model. At a mechanistic level, we propose to attribute this set of findings to the coherent modulation of the auditory signal with facial motion. Grant and others have suggested that hearing may be improved by allowing subjects to confirm whether peaks and valleys in a noisy spectrogram belong either to foreground speech (peaks) or background noise (valleys). Coherence masking protection (CMP) and co-modulation masking release (CMR) are similar phenomena purely within the auditory modality. In the case of CMP the target signal is co-modulated across different frequency bands <xref ref-type="bibr" rid="pone.0004638-Gordon1">&#x0005b;64&#x0005d;</xref>; in the case of CMR the noise is co-modulated <xref ref-type="bibr" rid="pone.0004638-Buus1">&#x0005b;65&#x0005d;</xref>. In either case, the co-modulation may facilitate the grouping of information as belonging to the foreground signal or background noise. For this reason the enhancement observed here with a comodulated visual stimulus may be considered a form of bimodal coherence masking protection <xref ref-type="bibr" rid="pone.0004638-Grant4">&#x0005b;66&#x0005d;</xref>.</p></sec><sec id="s4c"><title>Comparison with other models</title><p>The model presented here has similarities to earlier probabilistic models of multisensory speech perception. In studies on the McGurk effect <xref ref-type="bibr" rid="pone.0004638-McGurk1">&#x0005b;43&#x0005d;</xref> by Massaro and colleagues (for a review, see <xref ref-type="bibr" rid="pone.0004638-Massaro1">&#x0005b;5&#x0005d;</xref>), participants had to identify a spoken syllable as, for instance, /ba/ or /da/, while both auditory and visual speech were varied on a continuum between /ba/ and /da/. The behavioral data were described well by the so-called fuzzy-logical model of speech perception (FLMP; <xref ref-type="bibr" rid="pone.0004638-Massaro1">&#x0005b;5&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Massaro2">&#x0005b;45&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Massaro3">&#x0005b;67&#x0005d;</xref>), in which the evidence for an alternative is expressed as a probability and the multisensory probability is obtained as the normalized product of the unisensory probabilities. The FLMP is related to Bayesian inference <xref ref-type="bibr" rid="pone.0004638-Massaro2">&#x0005b;45&#x0005d;</xref>, but not equivalent to it (since it equates amounts of evidence to response frequencies, which is unjustified in a Bayesian model). Moreover, it was not known whether a Bayesian model can describe data collected with a full vocabulary.</p><p>Another predecessor is Braida's prelabeling model <xref ref-type="bibr" rid="pone.0004638-Braida1">&#x0005b;32&#x0005d;</xref>. In this model, stimuli (consonants) are represented in a multidimensional space and &#x0201c;confusion matrices&#x0201d; reflect the uncertainty in extracting syllable identity from auditory and visual cues. Multisensory performance is computed by assuming that this space is the Cartesian product of a visual and an auditory subspace. This is different from the present model, which computes optimal multisensory performance from the product of two probability distributions in the same space. Moreover, the data available at the time were only at a few SNRs and mostly showed inverse effectiveness. The model proposed here most naturally fits with an amodal (or supramodal) word space: neither dimension of this space has a purely auditory or visual character, but instead, each sensory modality contributes some evidence in each of the feature dimensions. One possible way to think about the word space might be as the space spanned by all parameters of the production process of a word, such as the time courses of vocal chord length, lip shape, and tongue position.</p><p>The notion that words form a neighborhood relationship in some high-dimensional feature space was captured also by Luce's Neighborhood Activation Model (NAM) <xref ref-type="bibr" rid="pone.0004638-Luce1">&#x0005b;28&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Auer2">&#x0005b;29&#x0005d;</xref>. The model uses performance measures on individual phonemes to estimate the performance of identifying full words. Similar to the present work, it incorporates word frequency (prior likelihood) and expresses lexical information as permissible points in the joint feature space. However, this model does not derive the probability of correct identification from first principles as we do here, and is based instead on a descriptive quantitative rule. Nevertheless, Auer has used this model successfully to explain performance gains in audio-visual word recognition <xref ref-type="bibr" rid="pone.0004638-Auer2">&#x0005b;29&#x0005d;</xref>. He concludes that neighborhood relationships derived numerically from behavioral confusion matrixes can also be used to quantify audio-visual word identification performance.</p></sec><sec id="s4d"><title>Outlook on causal inference</title><p>In the predictions above, we considered the case of integrating similar, but incongruent words, i.e. the mismatch between auditory and visual utterance is small. When more disparate word pairs are allowed, integration is no longer guaranteed. In perceptual tasks using simple stimuli, it was found that as the discrepancy increases, human subjects believe less that the two stimuli had a common source <xref ref-type="bibr" rid="pone.0004638-Wallace1">&#x0005b;68&#x0005d;</xref> and can make different responses when asked for the auditory and the visual source separately <xref ref-type="bibr" rid="pone.0004638-Shams1">&#x0005b;48&#x0005d;</xref>. A similar effect can occur in speech perception <xref ref-type="bibr" rid="pone.0004638-Sekiyama1">&#x0005b;69&#x0005d;</xref>, as can be experienced when watching poorly dubbed movies. Temporal discrepancy between auditory and visual speech signals also affects one's percept of unity <xref ref-type="bibr" rid="pone.0004638-McGrath1">&#x0005b;70&#x0005d;</xref>. We surmise that these results can all be modeled by a Bayesian causal inference model, in which the brain not only tries to infer stimulus identity (which word was spoken) but also whether the auditory and visual stimulus had a common source <xref ref-type="bibr" rid="pone.0004638-Kording1">&#x0005b;52&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Sato1">&#x0005b;71&#x0005d;</xref>. The present Bayesian model could open the door to causal modeling in speech perception.</p></sec><sec id="s4e"><title>Neural basis</title><p>The notion of inverse effectiveness was first used to describe effects seen during intracranial recordings in multisensory neurons of the superior colliculus (SC). In some of those neurons, an additional visual input was most effective at driving the cell when auditory information was poorest <xref ref-type="bibr" rid="pone.0004638-Meredith1">&#x0005b;14&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Stein1">&#x0005b;35&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Stein2">&#x0005b;72&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Stanford1">&#x0005b;73&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Wallace2">&#x0005b;74&#x0005d;</xref>. This pattern has also been found in multisensory neurons in the neocortex of animals <xref ref-type="bibr" rid="pone.0004638-Kayser1">&#x0005b;75&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Lakatos1">&#x0005b;76&#x0005d;</xref> and has been inferred in brain imaging <xref ref-type="bibr" rid="pone.0004638-Callan1">&#x0005b;77&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Calvert1">&#x0005b;78&#x0005d;</xref>, although imaging data of multisensory areas have to be interpreted with great caution <xref ref-type="bibr" rid="pone.0004638-Laurienti1">&#x0005b;79&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Beauchamp1">&#x0005b;80&#x0005d;</xref>. It is important to note that inverse effectiveness on a neuronal level makes a statement about spike counts observed in a subset of multisensory neurons. Behavioral inverse effectiveness, however, is a statement about the percentage of correct behavioral responses (it has also been applied to other quantities, such as reaction times). Whether there is a connection between these measures is not clear and to our knowledge there is no rigorous work establishing such a link. In contrast, Bayes-optimal cue integration can be linked to physiology in a rigorous way, using the formalism of probabilistic population codes <xref ref-type="bibr" rid="pone.0004638-Deneve1">&#x0005b;81&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Ma1">&#x0005b;82&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Knill5">&#x0005b;83&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Deneve2">&#x0005b;84&#x0005d;</xref>. The implications of this formalism for speech need to be examined in further work.</p><p>The site of multisensory integration in speech is subject of considerable debate. Common-format theories of auditory-visual speech perception suggest that modality-specific stimulus information is transformed into an amodal representation <xref ref-type="bibr" rid="pone.0004638-Bernstein2">&#x0005b;6&#x0005d;</xref>. This may occur by convergence of modality-specific information onto multisensory neurons, for instance in the superior temporal gyrus/sulcus <xref ref-type="bibr" rid="pone.0004638-Reale1">&#x0005b;85&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Hall1">&#x0005b;86&#x0005d;</xref>. This is a known convergence site for visual articulation and auditory features, and has been shown to depend on the comodulation of audiovisual stimuli <xref ref-type="bibr" rid="pone.0004638-Noesselt1">&#x0005b;87&#x0005d;</xref>. Recent evidence also points at early activity (&#x0003c;100 ms) in the supramarginal and angular gyrus (SMG/AG) <xref ref-type="bibr" rid="pone.0004638-Bernstein3">&#x0005b;88&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-Bernstein4">&#x0005b;89&#x0005d;</xref>. Besides behavioral and fMRI data, there is ample evidence from encephalography for an early influence of the visual modality on auditory speech processing. Gamma-band activity (30 Hz or higher) associated with multimodal fusion is enhanced early after onset of congruent auditory-visual speech stimuli (30&#x02013;120 ms) <xref ref-type="bibr" rid="pone.0004638-Kaiser1">&#x0005b;90&#x0005d;</xref>. This effect is only observed if stimuli are simultaneous, indicating that temporal information is important for early fusion in speech. Furthermore, early auditory evoked potentials (at 50 ms) in response to speech are modulated by congruent visual stimuli <xref ref-type="bibr" rid="pone.0004638-Lebib1">&#x0005b;91&#x0005d;</xref>, <xref ref-type="bibr" rid="pone.0004638-vanWassenhove1">&#x0005b;92&#x0005d;</xref>. Taken together, the behavioral and neuro-imaging data support the notion that auditory processing itself may be aided by comodulated visual stimuli during speech perception. On the other hand, according to modality-specific theories, auditory and visual speech information is processed by modality-specific networks and then associated at a post-labeling stage <xref ref-type="bibr" rid="pone.0004638-Bernstein2">&#x0005b;6&#x0005d;</xref>. Indeed, neuroimaging studies of auditory-visual speech perception implicate a variety of brain regions beyond early processing stages <xref ref-type="bibr" rid="pone.0004638-Campbell2">&#x0005b;93&#x0005d;</xref>. In general, the way auditory-visual signals are integrated remains unresolved. Further neurophysiological research is needed to constrain the possibilities on how auditory-visual integration in speech is achieved.</p></sec></sec><sec sec-type="supplementary-material" id="s5"><title>Supporting Information</title><supplementary-material content-type="local-data" id="pone.0004638.s001"><label>Figure S1</label><caption><p>Method for generating modified video from clean audio. For details, see section 1 of the Supporting Information.</p><p>(0.56 MB TIF)</p></caption><media xlink:href="pone.0004638.s001.tif" mimetype="image" mime-subtype="tiff"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0004638.s002"><label>Figure S2</label><caption><p>Variability between experiments. Auditory-visual stimuli are congruent. Visual-only performance was measured in two of these three studies. a. Identical to <xref ref-type="fig" rid="pone-0004638-g003">Figure 3a</xref>. b. Performance on the congruent trials of the second experiment (the incongruent trials were reported in <xref ref-type="fig" rid="pone-0004638-g007">Figure 7</xref>). c. Data from Ross et al., 2007</p><p>(0.11 MB TIF)</p></caption><media xlink:href="pone.0004638.s002.tif" mimetype="image" mime-subtype="tiff"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0004638.s003"><label>Figure S3</label><caption><p>a. Goodness of best fit (R2) of the numerical model to the behavioral data (such as in <xref ref-type="fig" rid="pone-0004638-g004">Figure 4</xref>), for various values of vocabulary size and dimension. Negative values were set to zero for plotting purposes. In <xref ref-type="fig" rid="pone-0004638-g004">Figure 4</xref>, the parameter combination N&#x0200a;&#x0003d;&#x0200a;2000, n&#x0200a;&#x0003d;&#x0200a;40 was used. b. Sum squared error (on a logarithmic axis) of the analytical model as a function of dimension. The minimum is at n&#x0200a;&#x0003d;&#x0200a;55 (fits shown in <xref ref-type="fig" rid="pone-0004638-g006">Figures 6b&#x02013;c</xref>), but any sufficiently large number of dimensions allows for a good fit. A low number of dimensions does not allow for a good description of the data.</p><p>(0.20 MB TIF)</p></caption><media xlink:href="pone.0004638.s003.tif" mimetype="image" mime-subtype="tiff"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0004638.s004"><label>Figure S4</label><caption><p>Optimal word recognition according to the analytical Bayesian model. a&#x02013;d. Recognition performance as a function of auditory reliability, rA, for various combinations of word space dimension, n, and visual reliability, rV. Colors are as in <xref ref-type="fig" rid="pone-0004638-g003">Figure 3</xref>. <xref ref-type="fig" rid="pone-0004638-g006">Figures 6b&#x02013;c</xref> were generated using the same model. Note that vocabulary size is infinite. Naturally, enhancements are larger when visual reliability is larger. e. Auditory reliability at maximum multisensory enhancement as a function of visual reliability, for fixed dimension. Lowering visual reliability causes the maximum to shift to higher values of auditory reliability. The same was shown for the numerical model in <xref ref-type="fig" rid="pone-0004638-g005">Figure 5a</xref>. f. Auditory reliability at maximum multisensory enhancement as a function of word space dimension, for fixed visual reliability.</p><p>(0.16 MB TIF)</p></caption><media xlink:href="pone.0004638.s004.tif" mimetype="image" mime-subtype="tiff"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0004638.s005"><label>Figure S5</label><caption><p>Effect of an auditory word on reports of an incongruent visual word, as predicted by the Bayesian model. Experiments were simulated in which pairs of similar auditory and visual words were presented. On each trial, the observer integrates the uncertain cues and reports a single word. Frequencies of reporting the auditory word (cyan) and the visual word (magenta) are shown as a function of auditory reliability. Each plot corresponds to a given combination of vocabulary size, N, and word space dimension, n. Visual reliability was fixed at rV&#x0200a;&#x0003d;&#x0200a;0.6. The occurrence of a maximum in the visual reports at a nonzero value of auditory reliability is consistent across vocabulary sizes and dimensions.</p><p>(0.19 MB TIF)</p></caption><media xlink:href="pone.0004638.s005.tif" mimetype="image" mime-subtype="tiff"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="pone.0004638.s006"><label>Supporting Information S1</label><caption><p>(0.21 MB DOC)</p></caption><media xlink:href="pone.0004638.s006.doc" mimetype="application" mime-subtype="msword"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><p>We would like to thank Tue Lehn-Schioler for the extensive help he has provided in generating the artificial video sequence for the AV&#x0002a; condition. We also thank John Lawler for educating us on issues related to the vocabulary size of monosyllabic words. We thank Vikranth Bejjanki, Florian Jaeger, Dave Knill, Alex Pouget, and Josh Wallman for useful discussions and helpful suggestions.</p></ack><ref-list><title>References</title><ref id="pone.0004638-Campbell1"><label>1</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>R</given-names></name></person-group><year>1998</year><article-title>Speechreading: advances in understanding its cortical bases and implications for deafness and speech rehabilitation.</article-title><source>Scand Audiol</source><volume>Suppl 49</volume></citation></ref><ref id="pone.0004638-Bernstein1"><label>2</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>LE</given-names></name><name><surname>Demorest</surname><given-names>ME</given-names></name><name><surname>Tucker</surname><given-names>PE</given-names></name></person-group><year>2000</year><article-title>Speech perception without hearing. Perception and Psychophysics.</article-title><source>Perception and Psychophysics</source><volume>62</volume><fpage>233</fpage><lpage>252</lpage><pub-id pub-id-type="pmid">10723205</pub-id></citation></ref><ref id="pone.0004638-Grant1"><label>3</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname><given-names>KW</given-names></name><name><surname>Walden</surname><given-names>BE</given-names></name></person-group><year>1996</year><article-title>Evaluating the articulation index for auditory-visual consonant recognition.</article-title><source>J Acoust Soc Am</source><volume>100</volume><fpage>2415</fpage><lpage>2424</lpage><pub-id pub-id-type="pmid">8865647</pub-id></citation></ref><ref id="pone.0004638-MacLeod1"><label>4</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>MacLeod</surname><given-names>A</given-names></name><name><surname>Summerfield</surname><given-names>Q</given-names></name></person-group><year>1987</year><article-title>Quantifying the contribution of vision to speech perception in noise.</article-title><source>Br J Audiol</source><volume>21</volume><fpage>131</fpage><lpage>141</lpage><pub-id pub-id-type="pmid">3594015</pub-id></citation></ref><ref id="pone.0004638-Massaro1"><label>5</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Massaro</surname><given-names>DW</given-names></name></person-group><year>1987</year><source>Speech perception by ear and eye: A paradigm for psychological inquiry</source><publisher-loc>Hillsdale, , NJ</publisher-loc><publisher-name>Erlbaum</publisher-name></citation></ref><ref id="pone.0004638-Bernstein2"><label>6</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>LE</given-names></name><name><surname>Auer</surname><given-names>ET</given-names></name><name><surname>Moore</surname><given-names>JK</given-names></name></person-group><year>2004</year><article-title>Audiovisual speech binding: convergence or association?</article-title><person-group person-group-type="editor"><name><surname>Calvert</surname><given-names>GA</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><source>The handbook of multisensory processes</source><publisher-loc>Cambridge, , MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>203</fpage><lpage>223</lpage></citation></ref><ref id="pone.0004638-Grant2"><label>7</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname><given-names>KW</given-names></name><name><surname>Walden</surname><given-names>BE</given-names></name><name><surname>Seitz</surname><given-names>PF</given-names></name></person-group><year>1998</year><article-title>Auditory-visual speech recognition by hearing-impaired subject: Consonant recognition, and auditory-visual integration.</article-title><source>J Acoust Soc Am</source><volume>103</volume><fpage>2677</fpage><lpage>2690</lpage><pub-id pub-id-type="pmid">9604361</pub-id></citation></ref><ref id="pone.0004638-Sumby1"><label>8</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sumby</surname><given-names>WH</given-names></name><name><surname>Pollack</surname><given-names>I</given-names></name></person-group><year>1954</year><article-title>Visual contribution to speech intelligibility in noise.</article-title><source>J Acoust Soc Am</source><volume>26</volume><fpage>212</fpage><lpage>215</lpage></citation></ref><ref id="pone.0004638-Erber1"><label>9</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Erber</surname><given-names>NP</given-names></name></person-group><year>1969</year><article-title>Interaction of audition and vision in the recognition of oral speech stimuli.</article-title><source>J Speech Hearing Res</source><volume>12</volume><fpage>423</fpage><lpage>425</lpage><pub-id pub-id-type="pmid">5808871</pub-id></citation></ref><ref id="pone.0004638-Erber2"><label>10</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Erber</surname><given-names>NP</given-names></name></person-group><year>1971</year><article-title>Auditory and audiovisual reception of words in low-frequency noise by children with normal hearing and by children with impaired hearing.</article-title><source>J Speech Hearing Res</source><volume>143</volume><fpage>496</fpage><lpage>512</lpage><pub-id pub-id-type="pmid">5163883</pub-id></citation></ref><ref id="pone.0004638-Erber3"><label>11</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Erber</surname><given-names>NP</given-names></name></person-group><year>1975</year><article-title>Auditory-visual perception in speech.</article-title><source>J Speech and Hearing Disord</source><volume>40</volume><fpage>481</fpage><lpage>492</lpage><pub-id pub-id-type="pmid">1234963</pub-id></citation></ref><ref id="pone.0004638-Binnie1"><label>12</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Binnie</surname><given-names>CA</given-names></name><name><surname>Montgomery</surname><given-names>A</given-names></name><name><surname>Jackson</surname><given-names>PL</given-names></name></person-group><year>1974</year><article-title>Auditory and visual contributions to the perception of consonants.</article-title><source>J Speech and Hearing Res</source><volume>17</volume><fpage>619</fpage><lpage>630</lpage><pub-id pub-id-type="pmid">4444283</pub-id></citation></ref><ref id="pone.0004638-McCormick1"><label>13</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>McCormick</surname><given-names>B</given-names></name></person-group><year>1979</year><article-title>Audio-visual discrimination of speech.</article-title><source>Clin Otolaryngol Allied Sci</source><volume>45</volume><fpage>355</fpage><lpage>361</lpage><pub-id pub-id-type="pmid">487634</pub-id></citation></ref><ref id="pone.0004638-Meredith1"><label>14</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Meredith</surname><given-names>MA</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year>1986</year><article-title>Spatial factors determine the activity of multisensory neurons in cat superior colliculus.</article-title><source>Cogn Brain Res</source><volume>369</volume><fpage>350</fpage><lpage>354</lpage></citation></ref><ref id="pone.0004638-Ross1"><label>15</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname><given-names>LA</given-names></name><name><surname>Saint-Amour</surname><given-names>D</given-names></name><name><surname>Leavitt</surname><given-names>VN</given-names></name><name><surname>Javitt</surname><given-names>DC</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><year>2007</year><article-title>Do you see what i am saying? Exploring visual enhancement of speech comprehension in noisy environments.</article-title><source>Cereb Cortex</source><volume>17</volume><fpage>1147</fpage><lpage>1153</lpage><pub-id pub-id-type="pmid">16785256</pub-id></citation></ref><ref id="pone.0004638-Alais1"><label>16</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Burr</surname><given-names>D</given-names></name></person-group><year>2004</year><article-title>The ventriloquist effect results from near-optimal bimodal integration.</article-title><source>Curr Biol</source><volume>14</volume><fpage>257</fpage><lpage>262</lpage><pub-id pub-id-type="pmid">14761661</pub-id></citation></ref><ref id="pone.0004638-Ernst1"><label>17</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name></person-group><year>2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion.</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="pmid">11807554</pub-id></citation></ref><ref id="pone.0004638-vanBeers1"><label>18</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>van Beers</surname><given-names>RJ</given-names></name><name><surname>Sittig</surname><given-names>AC</given-names></name><name><surname>Gon</surname><given-names>JJ</given-names></name></person-group><year>1999</year><article-title>Integration of proprioceptive and visual position-information: An experimentally supported model.</article-title><source>J Neurophysiol</source><volume>81</volume><fpage>1355</fpage><lpage>1364</lpage><pub-id pub-id-type="pmid">10085361</pub-id></citation></ref><ref id="pone.0004638-Kucera1"><label>19</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Kucera</surname><given-names>H</given-names></name><name><surname>Francis</surname><given-names>WN</given-names></name></person-group><year>1967</year><source>Computational analysis of present-day American English</source><publisher-loc>Providence, , RI</publisher-loc><publisher-name>Brown University Press</publisher-name></citation></ref><ref id="pone.0004638-LehnSchioler1"><label>20</label><citation citation-type="other"><person-group person-group-type="author"><name><surname>Lehn-Schioler</surname><given-names>T</given-names></name></person-group><year>2005</year><comment>Making Faces - State-Space Models Applied to Multi-Modal Signal Processing: Ph.D. Thesis</comment></citation></ref><ref id="pone.0004638-Lidestam1"><label>21</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lidestam</surname><given-names>B</given-names></name><name><surname>Lyxell</surname><given-names>B</given-names></name><name><surname>Lundeberg</surname><given-names>M</given-names></name></person-group><year>2001</year><article-title>Speech-reading of synthetic and natural faces: effects of contextual cueing and mode of presentation.</article-title><source>Scand Audiol</source><volume>30</volume><fpage>89</fpage><lpage>94</lpage><pub-id pub-id-type="pmid">11409792</pub-id></citation></ref><ref id="pone.0004638-Jiang1"><label>22</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>J</given-names></name><name><surname>Auer</surname><given-names>ET</given-names><suffix>Jr</suffix></name><name><surname>Alwan</surname><given-names>A</given-names></name><name><surname>Keating</surname><given-names>PA</given-names></name><name><surname>Bernstein</surname><given-names>LE</given-names></name></person-group><year>2007</year><article-title>Similarity structure in visual speech perception and optical phonetic signals.</article-title><source>Percept Psychophys</source><volume>69</volume><fpage>1070</fpage><lpage>1083</lpage><pub-id pub-id-type="pmid">18038946</pub-id></citation></ref><ref id="pone.0004638-Mattys1"><label>23</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mattys</surname><given-names>SL</given-names></name><name><surname>Bernstein</surname><given-names>LE</given-names></name><name><surname>Auer</surname><given-names>ET</given-names><suffix>Jr</suffix></name></person-group><year>2002</year><article-title>Stimulus-based lexical distinctiveness as a general word-recognition mechanism.</article-title><source>Percept Psychophys</source><volume>64</volume><fpage>667</fpage><lpage>679</lpage><pub-id pub-id-type="pmid">12132766</pub-id></citation></ref><ref id="pone.0004638-Fisher1"><label>24</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>CG</given-names></name></person-group><year>1968</year><article-title>Confusions among visually perceived consonants.</article-title><source>J Speech Hear Res</source><volume>11</volume><fpage>796</fpage><lpage>804</lpage><pub-id pub-id-type="pmid">5719234</pub-id></citation></ref><ref id="pone.0004638-Wang1"><label>25</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>MD</given-names></name><name><surname>Reed</surname><given-names>CM</given-names></name><name><surname>Bilger</surname><given-names>RC</given-names></name></person-group><year>1978</year><article-title>A comparison of the effects of filtering and sensorineural hearing loss on patients of consonant confusions.</article-title><source>J Speech Hear Res</source><volume>21</volume><fpage>5</fpage><lpage>36</lpage><pub-id pub-id-type="pmid">642487</pub-id></citation></ref><ref id="pone.0004638-Wang2"><label>26</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>MD</given-names></name><name><surname>Bilger</surname><given-names>RC</given-names></name></person-group><year>1973</year><article-title>Consonant confusions in noise: a study of perceptual features.</article-title><source>J Acoust Soc Am</source><volume>54</volume><fpage>1248</fpage><lpage>1266</lpage><pub-id pub-id-type="pmid">4765809</pub-id></citation></ref><ref id="pone.0004638-Auer1"><label>27</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Auer</surname><given-names>ET</given-names><suffix>Jr</suffix></name><name><surname>Bernstein</surname><given-names>LE</given-names></name></person-group><year>1997</year><article-title>Speechreading and the structure of the lexicon: computationally modeling the effects of reduced phonetic distinctiveness on lexical uniqueness.</article-title><source>J Acoust Soc Am</source><volume>102</volume><fpage>3704</fpage><lpage>3710</lpage><pub-id pub-id-type="pmid">9407662</pub-id></citation></ref><ref id="pone.0004638-Luce1"><label>28</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Luce</surname><given-names>PA</given-names></name><name><surname>Pisoni</surname><given-names>DB</given-names></name></person-group><year>1998</year><article-title>Recognizing spoken words: the neighborhood activation model.</article-title><source>Ear Hear</source><volume>19</volume><fpage>1</fpage><lpage>36</lpage><pub-id pub-id-type="pmid">9504270</pub-id></citation></ref><ref id="pone.0004638-Auer2"><label>29</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Auer</surname><given-names>ET</given-names><suffix>Jr</suffix></name></person-group><year>2002</year><article-title>The influence of the lexicon on speech read word recognition: contrasting segmental and lexical distinctiveness.</article-title><source>Psychon Bull Rev</source><volume>9</volume><fpage>341</fpage><lpage>347</lpage><pub-id pub-id-type="pmid">12120798</pub-id></citation></ref><ref id="pone.0004638-MacEachern1"><label>30</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>MacEachern</surname><given-names>MR</given-names></name></person-group><year>2000</year><article-title>On the visual distinctivenes of words in the English lexicon.</article-title><source>J Phonetics</source><volume>28</volume><fpage>367</fpage><lpage>376</lpage></citation></ref><ref id="pone.0004638-Iverson1"><label>31</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Iverson</surname><given-names>P</given-names></name><name><surname>Bernstein</surname><given-names>LE</given-names></name><name><surname>Auer</surname><given-names>ET</given-names><suffix>Jr</suffix></name></person-group><year>1998</year><article-title>Modeling the interaction of phonetic intelligibility and lexical structure in audiovisual word recognition.</article-title><source>Speech Communication</source><volume>26</volume><fpage>46</fpage><lpage>63</lpage></citation></ref><ref id="pone.0004638-Braida1"><label>32</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Braida</surname><given-names>LD</given-names></name></person-group><year>1991</year><article-title>Crossmodal integration in the identification of consonant segments.</article-title><source>Q J Exp Psychol</source><volume>43</volume><fpage>647</fpage><lpage>677</lpage></citation></ref><ref id="pone.0004638-Erber4"><label>33</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Erber</surname><given-names>NP</given-names></name></person-group><year>1975</year><article-title>Auditory-visual perception in speech.</article-title><source>J Speech and Hearing Dis</source><volume>40</volume><fpage>481</fpage><lpage>492</lpage><pub-id pub-id-type="pmid">1234963</pub-id></citation></ref><ref id="pone.0004638-Miller1"><label>34</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>GA</given-names></name><name><surname>Heise</surname><given-names>GA</given-names></name><name><surname>Lichten</surname><given-names>W</given-names></name></person-group><year>1951</year><article-title>The intelligibility of speech as a function of the context of the test materials.</article-title><source>J Exp Psychol</source><volume>41</volume><fpage>329</fpage><lpage>335</lpage><pub-id pub-id-type="pmid">14861384</pub-id></citation></ref><ref id="pone.0004638-Stein1"><label>35</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Huneycutt</surname><given-names>WS</given-names></name><name><surname>Meredith</surname><given-names>MA</given-names></name></person-group><year>1988</year><article-title>Neurons and behavior: the same rules of multisensory integration.</article-title><source>Cogn Brain Res</source><volume>448</volume><fpage>355</fpage><lpage>358</lpage></citation></ref><ref id="pone.0004638-Gillmeister1"><label>36</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gillmeister</surname><given-names>H</given-names></name><name><surname>Eimer</surname><given-names>M</given-names></name></person-group><year>2007</year><article-title>Tactile enhancement of auditory detection and perceived loudness.</article-title><source>Brain Res</source><volume>1160</volume><fpage>58</fpage><lpage>68</lpage><pub-id pub-id-type="pmid">17573048</pub-id></citation></ref><ref id="pone.0004638-Diederich1"><label>37</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Diederich</surname><given-names>A</given-names></name><name><surname>Colonius</surname><given-names>H</given-names></name></person-group><year>2004</year><article-title>Bimodal and trimodal multisensory enhancement: effects of stimulus onset and intensity on reaction time.</article-title><source>Percept Psychophys</source><volume>66</volume><fpage>1388</fpage><lpage>1404</lpage><pub-id pub-id-type="pmid">15813202</pub-id></citation></ref><ref id="pone.0004638-Corneil1"><label>38</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Corneil</surname><given-names>BD</given-names></name><name><surname>Van Wanrooij</surname><given-names>M</given-names></name><name><surname>Munoz</surname><given-names>DP</given-names></name><name><surname>Van Opstal</surname><given-names>AJ</given-names></name></person-group><year>2002</year><article-title>Auditory&#x02013;visual interactions subserving goal-directed saccades in a complex scene.</article-title><source>J Neurophysiol</source><volume>88</volume><fpage>438</fpage><lpage>454</lpage><pub-id pub-id-type="pmid">12091566</pub-id></citation></ref><ref id="pone.0004638-Rach1"><label>39</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rach</surname><given-names>S</given-names></name><name><surname>Diederich</surname><given-names>A</given-names></name></person-group><year>2006</year><article-title>Visual-tactile integration: does stimulus duration influence the relative amount of response enhancement?.</article-title><source>Exp Brain Res</source><volume>173</volume><fpage>1246</fpage><lpage>1266</lpage></citation></ref><ref id="pone.0004638-Serino1"><label>40</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Serino</surname><given-names>A</given-names></name><name><surname>Farne</surname><given-names>A</given-names></name><name><surname>Rinaldesi</surname><given-names>ML</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name><name><surname>Ladavas</surname><given-names>E</given-names></name></person-group><year>2007</year><article-title>Can vision of the body ameliorate impaired somatosensory function?</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>1101</fpage><lpage>1107</lpage><pub-id pub-id-type="pmid">17101158</pub-id></citation></ref><ref id="pone.0004638-Bolognini1"><label>41</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bolognini</surname><given-names>N</given-names></name><name><surname>Rasi</surname><given-names>F</given-names></name><name><surname>Ladavas</surname><given-names>E</given-names></name></person-group><year>2005</year><article-title>Visual localization of sounds.</article-title><source>Neuropsychologia</source><volume>43</volume><fpage>1655</fpage><lpage>1661</lpage><pub-id pub-id-type="pmid">16009247</pub-id></citation></ref><ref id="pone.0004638-Holmes1"><label>42</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname><given-names>NP</given-names></name></person-group><year>2007</year><article-title>The law of inverse effectiveness in neurons and behaviour: Multisensory integration versus normal variability.</article-title><source>Neuropsychologia</source></citation></ref><ref id="pone.0004638-McGurk1"><label>43</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>McGurk</surname><given-names>H</given-names></name><name><surname>MacDonald</surname><given-names>J</given-names></name></person-group><year>1976</year><article-title>Hearing lips and seeing voices.</article-title><source>Nature</source><volume>264</volume><fpage>746</fpage><lpage>748</lpage><pub-id pub-id-type="pmid">1012311</pub-id></citation></ref><ref id="pone.0004638-Rosen1"><label>44</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname><given-names>SM</given-names></name><name><surname>Fourcin</surname><given-names>AJ</given-names></name><name><surname>Moore</surname><given-names>BC</given-names></name></person-group><year>1981</year><article-title>Voice pitch as an aid to lipreading.</article-title><source>Nature</source><volume>291</volume><fpage>150</fpage><lpage>152</lpage><pub-id pub-id-type="pmid">7231534</pub-id></citation></ref><ref id="pone.0004638-Massaro2"><label>45</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Massaro</surname><given-names>DW</given-names></name></person-group><year>1998</year><source>Perceiving talking faces: from speech perception to a behavioral principle</source><publisher-loc>Cambridge, , MA</publisher-loc><publisher-name>MIT Press</publisher-name></citation></ref><ref id="pone.0004638-Roach1"><label>46</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Roach</surname><given-names>NW</given-names></name><name><surname>Heron</surname><given-names>J</given-names></name><name><surname>McGraw</surname><given-names>PV</given-names></name></person-group><year>2006</year><article-title>Resolving multisensory conflict: a strategy for balancing the costs and benefits of audio-visual integration.</article-title><source>Proc Biol Sci</source><volume>273</volume><fpage>2159</fpage><lpage>2168</lpage><pub-id pub-id-type="pmid">16901835</pub-id></citation></ref><ref id="pone.0004638-Knill1"><label>47</label><citation citation-type="book"><person-group person-group-type="editor"><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Richards</surname><given-names>W</given-names></name></person-group><year>1996</year><source>Perception as Bayesian Inference</source><publisher-loc>New York</publisher-loc><publisher-name>Cambridge University Press</publisher-name></citation></ref><ref id="pone.0004638-Shams1"><label>48</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Beierholm</surname><given-names>U</given-names></name></person-group><year>2005</year><article-title>Sound-induced flash illusion as an optimal percept.</article-title><source>Neuroreport</source><volume>16</volume><fpage>1923</fpage><lpage>1927</lpage><pub-id pub-id-type="pmid">16272880</pub-id></citation></ref><ref id="pone.0004638-Hillis1"><label>49</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hillis</surname><given-names>JM</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name></person-group><year>2002</year><article-title>Combining sensory information: mandatory fusion within, but not between, senses.</article-title><source>Science</source><volume>298</volume><fpage>1627</fpage><lpage>1630</lpage><pub-id pub-id-type="pmid">12446912</pub-id></citation></ref><ref id="pone.0004638-Battaglia1"><label>50</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Battaglia</surname><given-names>PW</given-names></name><name><surname>Jacobs</surname><given-names>RA</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><year>2003</year><article-title>Bayesian integration of visual and auditory signals for spatial localization.</article-title><source>J Opt Soc Am A Opt Image Sci Vis</source><volume>20</volume><fpage>1391</fpage><lpage>1397</lpage><pub-id pub-id-type="pmid">12868643</pub-id></citation></ref><ref id="pone.0004638-Rowland1"><label>51</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rowland</surname><given-names>BA</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year>2007</year><article-title>A Bayesian model unifies multisensory spatial localization with the physiological properties of the superior colliculus.</article-title><source>Exp Brain Res</source><volume>180</volume><fpage>153</fpage><lpage>161</lpage><pub-id pub-id-type="pmid">17546470</pub-id></citation></ref><ref id="pone.0004638-Kording1"><label>52</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Beierholm</surname><given-names>U</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Quartz</surname><given-names>S</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><etal/></person-group><year>2007</year><article-title>Causal inference in multisensory perception.</article-title><source>PLoS ONE</source><volume>2</volume><fpage>e943</fpage><pub-id pub-id-type="pmid">17895984</pub-id></citation></ref><ref id="pone.0004638-Kording2"><label>53</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year>2004</year><article-title>Bayesian integration in sensorimotor learning.</article-title><source>Nature</source><volume>427</volume><fpage>244</fpage><lpage>247</lpage><pub-id pub-id-type="pmid">14724638</pub-id></citation></ref><ref id="pone.0004638-Ghahramani1"><label>54</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><year>1995</year><source>Computational and psychophysics of sensorimotor integration</source><publisher-loc>Cambridge</publisher-loc><publisher-name>Massachusetts Institute of Technology</publisher-name></citation></ref><ref id="pone.0004638-Knill2"><label>55</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Saunders</surname><given-names>JA</given-names></name></person-group><year>2003</year><article-title>Do humans optimally integrate stereo and texture information for judgments of surface slant?</article-title><source>Vision Research</source><volume>43</volume><fpage>2539</fpage><lpage>2558</lpage><pub-id pub-id-type="pmid">13129541</pub-id></citation></ref><ref id="pone.0004638-Knill3"><label>56</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name></person-group><year>2003</year><article-title>Mixture models and the probabilistic structure of depth cues.</article-title><source>Vision Res</source><volume>43</volume><fpage>831</fpage><lpage>854</lpage><pub-id pub-id-type="pmid">12639607</pub-id></citation></ref><ref id="pone.0004638-Jacobs1"><label>57</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>RA</given-names></name></person-group><year>1999</year><article-title>Optimal integration of texture and motion cues to depth.</article-title><source>Vision Res</source><volume>39</volume><fpage>3621</fpage><lpage>3629</lpage><pub-id pub-id-type="pmid">10746132</pub-id></citation></ref><ref id="pone.0004638-Knill4"><label>58</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name></person-group><year>2007</year><article-title>Robust cue integration: A Bayesian model and evidence from cue conflict slant studies with stereoscopic and figure cues to slant.</article-title><source>Journal of Vision</source><volume>7</volume><fpage>1</fpage><lpage>24</lpage></citation></ref><ref id="pone.0004638-Landy1"><label>59</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Landy</surname><given-names>MS</given-names></name><name><surname>Maloney</surname><given-names>LT</given-names></name><name><surname>Johnston</surname><given-names>EB</given-names></name><name><surname>Young</surname><given-names>M</given-names></name></person-group><year>1995</year><article-title>Measurement and modeling of depth cue combination: in defense of weak fusion.</article-title><source>Vision Res</source><volume>35</volume><fpage>389</fpage><lpage>412</lpage><pub-id pub-id-type="pmid">7892735</pub-id></citation></ref><ref id="pone.0004638-Brouwer1"><label>60</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname><given-names>A-M</given-names></name><name><surname>Knill</surname><given-names>DC</given-names></name></person-group><year>2007</year><article-title>The role of memory in visually guided reaching.</article-title><source>Journal of Vision</source><volume>7</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">18217846</pub-id></citation></ref><ref id="pone.0004638-Grant3"><label>61</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname><given-names>KW</given-names></name><name><surname>Seitz</surname><given-names>PF</given-names></name></person-group><year>2000</year><article-title>The use of visible speech cues for improving auditory detection of spoken sentences.</article-title><source>J Acoust Soc Am</source><volume>108</volume><fpage>1197</fpage><lpage>1208</lpage><pub-id pub-id-type="pmid">11008820</pub-id></citation></ref><ref id="pone.0004638-Thomas1"><label>62</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>SM</given-names></name><name><surname>Jordan</surname><given-names>TR</given-names></name></person-group><year>2004</year><article-title>Contributions of oral and extraoral facial movement to visual and audiovisual speech perception.</article-title><source>J Exp Psychol Hum Percept Perform</source><volume>30</volume><fpage>873</fpage><lpage>888</lpage><pub-id pub-id-type="pmid">15462626</pub-id></citation></ref><ref id="pone.0004638-Schwartz1"><label>63</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>JL</given-names></name><name><surname>Berthommier</surname><given-names>F</given-names></name><name><surname>Savariaux</surname><given-names>C</given-names></name></person-group><year>2004</year><article-title>Seeing to hear better: evidence for early audio-visual interactions in speech identification.</article-title><source>Cognition</source><volume>93</volume><fpage>B69</fpage><lpage>78</lpage><pub-id pub-id-type="pmid">15147940</pub-id></citation></ref><ref id="pone.0004638-Gordon1"><label>64</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>PC</given-names></name></person-group><year>1997</year><article-title>Coherence masking protection in brief noise complexes: effects of temporal patterns.</article-title><source>J Acoust Soc Am</source><volume>102</volume><fpage>2276</fpage><lpage>2283</lpage><pub-id pub-id-type="pmid">9348685</pub-id></citation></ref><ref id="pone.0004638-Buus1"><label>65</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Buus</surname><given-names>S</given-names></name></person-group><year>1985</year><article-title>Release from masking caused by envelope fluctuations.</article-title><source>J Acoust Soc Am</source><volume>78</volume><fpage>1958</fpage><lpage>1965</lpage><pub-id pub-id-type="pmid">4078172</pub-id></citation></ref><ref id="pone.0004638-Grant4"><label>66</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname><given-names>KW</given-names></name></person-group><year>2001</year><article-title>The effect of speechreading on masked detection thresholds for filtered speech.</article-title><source>J Acoust Soc Am</source><volume>109</volume><fpage>2272</fpage><lpage>2275</lpage><pub-id pub-id-type="pmid">11386581</pub-id></citation></ref><ref id="pone.0004638-Massaro3"><label>67</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Massaro</surname><given-names>DW</given-names></name><name><surname>Cohen</surname><given-names>MM</given-names></name><name><surname>Gesi</surname><given-names>A</given-names></name><name><surname>Heredia</surname><given-names>R</given-names></name><name><surname>Tsuzaki</surname><given-names>M</given-names></name></person-group><year>1993</year><article-title>Bimodal speech perception: an examination across languages.</article-title><source>J Phonetics</source><volume>21</volume><fpage>445</fpage><lpage>478</lpage></citation></ref><ref id="pone.0004638-Wallace1"><label>68</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>MT</given-names></name><name><surname>Roberson</surname><given-names>GE</given-names></name><name><surname>Hairston</surname><given-names>WD</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Vaughan</surname><given-names>JW</given-names></name><etal/></person-group><year>2004</year><article-title>Unifying multisensory signals across time and space.</article-title><source>Exp Brain Res</source><volume>158</volume><fpage>252</fpage><lpage>258</lpage><pub-id pub-id-type="pmid">15112119</pub-id></citation></ref><ref id="pone.0004638-Sekiyama1"><label>69</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sekiyama</surname><given-names>K</given-names></name></person-group><year>1997</year><article-title>Cultural and linguistic factors in audiovisual speech processing: the McGurk effect in Chinese subjects.</article-title><source>Percept Psychophys</source><volume>59</volume><fpage>73</fpage><lpage>80</lpage><pub-id pub-id-type="pmid">9038409</pub-id></citation></ref><ref id="pone.0004638-McGrath1"><label>70</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>McGrath</surname><given-names>M</given-names></name><name><surname>Summerfield</surname><given-names>Q</given-names></name></person-group><year>1985</year><article-title>Intermodal timing relations and audio-visual speech recognition by normal-hearing adults.</article-title><source>J Acoust Soc Am</source><volume>77</volume><fpage>678</fpage><lpage>685</lpage><pub-id pub-id-type="pmid">3973239</pub-id></citation></ref><ref id="pone.0004638-Sato1"><label>71</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>Y</given-names></name><name><surname>Toyoizumi</surname><given-names>T</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name></person-group><year>2007</year><article-title>Bayesian inference explains perception of unity and ventriloquism aftereffect: identification of common sources of audiovisual stimuli.</article-title><source>Neural Computation</source><volume>19</volume><fpage>3335</fpage><lpage>3355</lpage><pub-id pub-id-type="pmid">17970656</pub-id></citation></ref><ref id="pone.0004638-Stein2"><label>72</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Meredith</surname><given-names>MA</given-names></name></person-group><year>1993</year><source>The merging of the senses</source><publisher-loc>Cambridge, , MA</publisher-loc><publisher-name>MIT Press</publisher-name></citation></ref><ref id="pone.0004638-Stanford1"><label>73</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Quessy</surname><given-names>S</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year>2005</year><article-title>Evaluating the operations underlying multisensory integration in the cat superior colliculus.</article-title><source>J Neurosci</source><volume>25</volume><fpage>6499</fpage><lpage>6508</lpage><pub-id pub-id-type="pmid">16014711</pub-id></citation></ref><ref id="pone.0004638-Wallace2"><label>74</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>MT</given-names></name><name><surname>Wilkinson</surname><given-names>LK</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year>1996</year><article-title>Representation and integration of multiple sensory inputs in primate superior colliculus.</article-title><source>J Neurophysiol</source><volume>76</volume><fpage>1246</fpage><lpage>1266</lpage><pub-id pub-id-type="pmid">8871234</pub-id></citation></ref><ref id="pone.0004638-Kayser1"><label>75</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name><name><surname>Augath</surname><given-names>M</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year>2005</year><article-title>Integration of touch and sound in auditory cortex.</article-title><source>Neuron</source><volume>48</volume><fpage>373</fpage><lpage>384</lpage><pub-id pub-id-type="pmid">16242415</pub-id></citation></ref><ref id="pone.0004638-Lakatos1"><label>76</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>CM</given-names></name><name><surname>O'Connell</surname><given-names>MN</given-names></name><name><surname>Mills</surname><given-names>A</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year>2007</year><article-title>Neuronal oscillations and multisensory interaction in primary auditory cortex.</article-title><source>Neuron</source><volume>53</volume><fpage>279</fpage><lpage>292</lpage><pub-id pub-id-type="pmid">17224408</pub-id></citation></ref><ref id="pone.0004638-Callan1"><label>77</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Callan</surname><given-names>DE</given-names></name><name><surname>Jones</surname><given-names>JA</given-names></name><name><surname>Munhall</surname><given-names>K</given-names></name><name><surname>Callan</surname><given-names>AM</given-names></name><name><surname>Kroos</surname><given-names>C</given-names></name><name><surname>Vatikiotis-Bateson</surname><given-names>E</given-names></name></person-group><year>2003</year><article-title>Neural processes underlying perceptual enhancement by visual speech gestures.</article-title><source>Neuroreport</source><volume>14</volume><fpage>2213</fpage><lpage>2218</lpage><pub-id pub-id-type="pmid">14625450</pub-id></citation></ref><ref id="pone.0004638-Calvert1"><label>78</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Calvert</surname><given-names>GA</given-names></name><name><surname>Lewis</surname><given-names>WL</given-names></name></person-group><year>2004</year><source>Hemodynamic studies of audiovisual interactions. The handbook of multisensory processes</source><publisher-loc>Cambridge, , MA</publisher-loc><publisher-name>MIT Press</publisher-name></citation></ref><ref id="pone.0004638-Laurienti1"><label>79</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Laurienti</surname><given-names>PJ</given-names></name><name><surname>Perrault</surname><given-names>TJ</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name></person-group><year>2005</year><article-title>On the use of superadditivity as a metric for characterizing multisensory integration in functional neuroimaging studies.</article-title><source>Exp Brain Res</source><volume>166</volume><fpage>289</fpage><lpage>297</lpage><pub-id pub-id-type="pmid">15988597</pub-id></citation></ref><ref id="pone.0004638-Beauchamp1"><label>80</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname><given-names>MS</given-names></name></person-group><year>2005</year><article-title>Statistical criteria in fMRI studies of multisensory integration.</article-title><source>Neuroinformatics</source><volume>3</volume><fpage>93</fpage><lpage>113</lpage><pub-id pub-id-type="pmid">15988040</pub-id></citation></ref><ref id="pone.0004638-Deneve1"><label>81</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Deneve</surname><given-names>S</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year>2004</year><article-title>Bayesian multisensory integration and cross-modal spatial links.</article-title><source>J Physiol Paris</source><volume>98</volume><fpage>249</fpage><lpage>258</lpage><pub-id pub-id-type="pmid">15477036</pub-id></citation></ref><ref id="pone.0004638-Ma1"><label>82</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year>2006</year><article-title>Bayesian inference with probabilistic population codes.</article-title><source>Nat Neurosci</source><volume>9</volume><fpage>1432</fpage><lpage>1438</lpage><pub-id pub-id-type="pmid">17057707</pub-id></citation></ref><ref id="pone.0004638-Knill5"><label>83</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year>2004</year><article-title>The Bayesian brain: the role of uncertainty in neural coding and computation.</article-title><source>Trends Neurosci</source><volume>27</volume><fpage>712</fpage><lpage>719</lpage><pub-id pub-id-type="pmid">15541511</pub-id></citation></ref><ref id="pone.0004638-Deneve2"><label>84</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Deneve</surname><given-names>S</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year>2001</year><article-title>Efficient computation and cue integration with noisy population codes.</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>826</fpage><lpage>831</lpage></citation></ref><ref id="pone.0004638-Reale1"><label>85</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Reale</surname><given-names>RA</given-names></name><name><surname>Calvert</surname><given-names>GA</given-names></name><name><surname>Thesen</surname><given-names>T</given-names></name><name><surname>Jenison</surname><given-names>RL</given-names></name><name><surname>Kawasaki</surname><given-names>H</given-names></name><etal/></person-group><year>2007</year><article-title>Auditory-visual processing represented in the human superior temporal gyrus.</article-title><source>Neuroscience</source><volume>145</volume><fpage>162</fpage><lpage>184</lpage><pub-id pub-id-type="pmid">17241747</pub-id></citation></ref><ref id="pone.0004638-Hall1"><label>86</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>DA</given-names></name><name><surname>Fussell</surname><given-names>C</given-names></name><name><surname>Summerfield</surname><given-names>AQ</given-names></name></person-group><year>2005</year><article-title>Reading fluent speech from talking faces: typical brain networks and individual differences.</article-title><source>J Cogn Neurosci</source><volume>17</volume><fpage>939</fpage><lpage>953</lpage><pub-id pub-id-type="pmid">15969911</pub-id></citation></ref><ref id="pone.0004638-Noesselt1"><label>87</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Noesselt</surname><given-names>T</given-names></name><name><surname>Rieger</surname><given-names>JW</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Kanowski</surname><given-names>M</given-names></name><name><surname>Hinrichs</surname><given-names>H</given-names></name><etal/></person-group><year>2007</year><article-title>Audiovisual temporal correspondence modulates human multisensory superior temporal sulcus plus primary sensory cortices.</article-title><source>J Neurosci</source><volume>27</volume><fpage>11431</fpage><lpage>11441</lpage><pub-id pub-id-type="pmid">17942738</pub-id></citation></ref><ref id="pone.0004638-Bernstein3"><label>88</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>LE</given-names></name><name><surname>Auer</surname><given-names>ET</given-names><suffix>Jr</suffix></name><name><surname>Wagner</surname><given-names>M</given-names></name><name><surname>Ponton</surname><given-names>CW</given-names></name></person-group><year>2008</year><article-title>Spatiotemporal dynamics of audiovisual speech processing.</article-title><source>Neuroimage</source><volume>39</volume><fpage>423</fpage><lpage>435</lpage><pub-id pub-id-type="pmid">17920933</pub-id></citation></ref><ref id="pone.0004638-Bernstein4"><label>89</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>LE</given-names></name><name><surname>Lu</surname><given-names>ZL</given-names></name><name><surname>Jiang</surname><given-names>J</given-names></name></person-group><year>2008</year><article-title>Quantified acoustic-optical speech signal incongruity identifies cortical sites of audiovisual speech processing.</article-title><source>Brain Res</source></citation></ref><ref id="pone.0004638-Kaiser1"><label>90</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>J</given-names></name><name><surname>Hertrich</surname><given-names>I</given-names></name><name><surname>Ackermann</surname><given-names>H</given-names></name><name><surname>Mathiak</surname><given-names>K</given-names></name><name><surname>Lutzenberger</surname><given-names>W</given-names></name></person-group><year>2005</year><article-title>Hearing lips: gamma-band activity during audiovisual speech perception.</article-title><source>Cereb Cortex</source><volume>15</volume><fpage>646</fpage><lpage>653</lpage><pub-id pub-id-type="pmid">15342432</pub-id></citation></ref><ref id="pone.0004638-Lebib1"><label>91</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lebib</surname><given-names>R</given-names></name><name><surname>Papo</surname><given-names>D</given-names></name><name><surname>de Bode</surname><given-names>S</given-names></name><name><surname>Baudonniere</surname><given-names>PM</given-names></name></person-group><year>2003</year><article-title>Evidence of a visual-to-auditory cross-modal sensory gating phenomenon as reflected by the human P50 event-related brain potential modulation.</article-title><source>Neurosci Lett</source><volume>341</volume><fpage>185</fpage><lpage>188</lpage><pub-id pub-id-type="pmid">12697279</pub-id></citation></ref><ref id="pone.0004638-vanWassenhove1"><label>92</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Grant</surname><given-names>KW</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year>2005</year><article-title>Visual speech speeds up the neural processing of auditory speech.</article-title><source>Proc Natl Acad Sci U S A</source><volume>102</volume><fpage>1181</fpage><lpage>1186</lpage><pub-id pub-id-type="pmid">15647358</pub-id></citation></ref><ref id="pone.0004638-Campbell2"><label>93</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>R</given-names></name></person-group><year>2007</year><article-title>The processing of audio-visual speech: empirical and neural bases.</article-title><source>Philos Trans R Soc Lond B Biol Sci</source></citation></ref></ref-list><fn-group><fn fn-type="conflict"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="financial-disclosure"><p><bold>Funding: </bold>The authors have no support or funding to report.</p></fn></fn-group></back></article>