<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19208178</article-id><article-id pub-id-type="pmc">2648762</article-id><article-id pub-id-type="publisher-id">1471-2105-10-S1-S73</article-id><article-id pub-id-type="doi">10.1186/1471-2105-10-S1-S73</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>PCA-based population structure inference with generic clustering algorithms</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Lee</surname><given-names>Chih</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>chih.lee@uconn.edu</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Abdool</surname><given-names>Ali</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>ali.abdool@uconn.edu</email></contrib><contrib id="A3" corresp="yes" contrib-type="author"><name><surname>Huang</surname><given-names>Chun-Hsi</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>huang@engr.uconn.edu</email></contrib></contrib-group><aff id="I1"><label>1</label>Computer Science and Engineering Department, University of Connecticut, Storrs, CT 06269, USA</aff><pub-date pub-type="collection"><year>2009</year></pub-date><pub-date pub-type="epub"><day>30</day><month>1</month><year>2009</year></pub-date><volume>10</volume><issue>Suppl 1</issue><supplement><named-content content-type="supplement-title">Selected papers from the Seventh Asia-Pacific Bioinformatics Conference (APBC 2009)</named-content><named-content content-type="supplement-editor">Michael Q Zhang, Michael S Waterman and Xuegong Zhang</named-content></supplement><fpage>S73</fpage><lpage>S73</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10/S1/S73"/><permissions><copyright-statement>Copyright &#x000a9; 2009 Lee et al; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2009</copyright-year><copyright-holder>Lee et al; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an open access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>               Lee               Chih                              chih.lee@uconn.edu            </dc:author><dc:title>            PCA-based population structure inference with generic clustering algorithms         </dc:title><dc:date>2009</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 10(Suppl 1): S73-. (2009)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2009)10:Suppl 1&#x0003c;S73&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>Handling genotype data typed at hundreds of thousands of loci is very time-consuming and it is no exception for population structure inference. Therefore, we propose to apply PCA to the genotype data of a population, select the significant principal components using the Tracy-Widom distribution, and assign the individuals to one or more subpopulations using generic clustering algorithms.</p></sec><sec><title>Results</title><p>We investigated K-means, soft K-means and spectral clustering and made comparison to STRUCTURE, a model-based algorithm specifically designed for population structure inference. Moreover, we investigated methods for predicting the number of subpopulations in a population. The results on four simulated datasets and two real datasets indicate that our approach performs comparably well to STRUCTURE. For the simulated datasets, STRUCTURE and soft K-means with BIC produced identical predictions on the number of subpopulations. We also showed that, for real dataset, BIC is a better index than likelihood in predicting the number of subpopulations.</p></sec><sec><title>Conclusion</title><p>Our approach has the advantage of being fast and scalable, while STRUCTURE is very time-consuming because of the nature of MCMC in parameter estimation. Therefore, we suggest choosing the proper algorithm based on the application of population structure inference.</p></sec></abstract><conference><conf-date>13&#x02013;16 January 2009</conf-date><conf-name>The Seventh Asia Pacific Bioinformatics Conference (APBC 2009)</conf-name><conf-loc>Beijing, China</conf-loc></conference></article-meta></front><body><sec><title>Background</title><p>Population structure inference is the problem of assigning each individual in a population to a cluster, given the number of clusters. When admixture is allowed, each individual can be assigned to more than one cluster along with a membership coefficient for each cluster. Population structure inference has many applications in genetic studies. Some obvious applications include grouping individuals, identifying immigrants or admixed individuals, and inferring demographic history. Moreover, it also serves as a preprocessing step in stratified association studies to avoid spurious associations [<xref ref-type="bibr" rid="B1">1</xref>].</p><p>The association between a marker and a locus involved in disease causation has been the object of numerous studies. In a case-control study, it is possible that the samples or patients are drawn from two or more different populations but the population structure is not observed or recorded. Suppose that an allele of a marker appears significantly more frequently in the case than in the control group, we might come to the conclusion that this allele is associated with the disease. However, we have to rule out the possibility that most of the samples in the case group are from a specific population and this allele happens to be the prevalent one at the marker. Therefore, inferring population structure before association studies allow us to avoid this problem, lowering the false positive rate.</p><p>Software STRUCTURE is widely used in population structure inference. It is specifically designed for genotype data and approaches the problem by careful modelling of allele frequencies, origins of alleles of individuals and origins of individual genomes. As described in Section <bold>Methods</bold>, for a genotype dataset of <italic>m </italic>diploid individuals and <italic>n </italic>biallelic markers, STRUCTURE estimates 2<italic>Kn </italic>+ <italic>Km </italic>+ 2<italic>mn </italic>parameters using Markov Chain Monte Carlo (MCMC), where <italic>K </italic>is the number of clusters. Inferring population structure using STRUCTURE is, therefore, very time-consuming since it has to handle large datasets consisting of thousands of individuals genotyped at hundreds of thousands of loci. Therefore, we propose an alternative approach to dealing with this problem.</p><p>From the perspective of machine learning, when dealing with high-dimensional data, it is natural to preprocess the data with dimension reduction and feature selection techniques. Principal component analysis (PCA) is a technique of dimension reduction. The importance of a principal component (PC) is proportional to the corresponding eigenvalue, which is the variance of data projected onto this component. Deciding the number of PCs to be kept for subsequent analyses is not a trivial problem. Fortunately, Johnstone [<xref ref-type="bibr" rid="B2">2</xref>] showed that with suitable normalization, for large <italic>m </italic>and <italic>n</italic>, the distribution of the largest eigenvalue <italic>&#x003bb;</italic><sub>1 </sub>is approximately a Tracy-Widom (TW) distribution [<xref ref-type="bibr" rid="B3">3</xref>]. Patterson <italic>et al</italic>. [<xref ref-type="bibr" rid="B4">4</xref>] applied PCA to real and simulated population genotype data with more than one underlying subpopulation. It is shown that, when the genotype data is projected onto a significant PC, the means of the subpopulations are also significantly different according to an ANOVA test. These empirical results indicate the potential of PCA and the TW distribution in discovery of population structure. Therefore, we propose to perform dimension reduction on genotype data using PCA and apply generic clustering algorithms to infer population structure.</p><p>In this paper, we base our study on PCA and investigate three generic clustering algorithms &#x02013; K-means, soft K-means and spectral clustering algorithms. The results are then compared with those generated by STRUCTURE. We introduce the data, clustering algorithms and evaluation metric in Section <bold>Methods</bold>. Comparisons and analyses of results are given in Section <bold>Results and discussion</bold>. Finally, we give the concluding remarks in Section <bold>Conclusions</bold>.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>Data</title><p>In this study, we use both real and simulated data to evaluate the performance of clustering algorithms. The real data is obtained from the Human Genome Diversity Project-Centre d'Etude du Polymorphisme Humain (HGDP-CEPH) Human Genome Diversity Panel [<xref ref-type="bibr" rid="B5">5</xref>], which contains genotypes of 1,064 individuals sampled from 51 populations. The version 2.0 of the HGDP-CEPH database contains genotypes for 4,991 markers and 4,154 biallelic ones are used in our study. Two subsets of individuals are constructed from the 1,064 ones. One subset encompasses all the 258 individuals in Europe and Middle East, which are geographically close, and we refer to it as the close dataset. The other subset consists of all the 739 individuals in Africa, Central South Asia, East Asia and Europe, which are geographically far apart from each other, and we refer to it as the distant dataset.</p><p>The simulated data is generated using software GENOME, a coalescent-based simulator written by Liang <italic>et al</italic>. [<xref ref-type="bibr" rid="B6">6</xref>]. The parameters are set to mimic the real data from HGDP-CEPH. The number of chromosomes or independent regions is set to 22 since there are 22 autosomal chromosomes in human. Each chromosome has 100 10,000-base fragments, simulating linkage disequilibrium within fragments. The recombination rate between two consecutive fragments is set to 0.01 to simulate the length of human genome. The number of markers per chromosome is set to a fixed number of 250, so the number of markers for each individual is 5,500. We use four simulated datasets in this study. Three of them contain individuals sampled from independent populations. The fourth dataset is generated according to a simple demography shown in Figure <xref ref-type="fig" rid="F1">1</xref>. The details are summarized in Table <xref ref-type="table" rid="T1">1</xref>.</p><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold>The demography used in simulating the fourth dataset</bold>. Generation 0 represents the current generation, while generation <italic>g </italic>represents <italic>g </italic>generations back in time.</p></caption><graphic xlink:href="1471-2105-10-S1-S73-1"/></fig><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Details of the first three simulated datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Set</td><td align="right">#idvs</td><td align="right">#pops</td><td align="right">#idvs from each pop</td></tr></thead><tbody><tr><td align="left">1</td><td align="right">300</td><td align="right">3</td><td align="right">100 100 100</td></tr><tr><td align="left">2</td><td align="right">400</td><td align="right">4</td><td align="right">100 100 100 100</td></tr><tr><td align="left">3</td><td align="right">500</td><td align="right">4</td><td align="right">50 100 150 200</td></tr><tr><td align="left">4</td><td align="right">620</td><td align="right">4</td><td align="right">160 200 160 100</td></tr></tbody></table></table-wrap></sec><sec><title>Principal component analysis</title><p>Principal component analysis (PCA) is a technique of dimension reduction. Given <italic>m </italic>samples and <italic>n </italic>markers or variables, the <italic>m </italic>samples can be represented as a <italic>m </italic>&#x000d7; <italic>n </italic>matrix <bold>X</bold>. We further assume that the sample mean of each marker is 0, i.e., <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" name="1471-2105-10-S1-S73-i1" overflow="scroll"><mml:semantics><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>. Using another basis of <italic>n </italic>vectors or axes, represented as column vectors of <bold>P</bold>, we can project the samples onto the new axes and obtain another <italic>m </italic>&#x000d7; <italic>n </italic>matrix <bold>Y </bold>= <bold>XP</bold>. PCA finds a <bold>P </bold>such that the sample covariance matrix of the <italic>n </italic>new variables is a diagonal matrix. That is,</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M2" name="1471-2105-10-S1-S73-i2" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:msub>                              <mml:mo>&#x02211;</mml:mo>                              <mml:mstyle mathvariant="bold" mathsize="normal">                                 <mml:mi>Y</mml:mi>                              </mml:mstyle>                           </mml:msub>                           <mml:mo>=</mml:mo>                           <mml:mfrac>                              <mml:mn>1</mml:mn>                              <mml:mi>m</mml:mi>                           </mml:mfrac>                           <mml:msup>                              <mml:mstyle mathvariant="bold" mathsize="normal">                                 <mml:mi>Y</mml:mi>                              </mml:mstyle>                              <mml:mi>T</mml:mi>                           </mml:msup>                           <mml:mstyle mathvariant="bold" mathsize="normal">                              <mml:mi>Y</mml:mi>                           </mml:mstyle>                           <mml:mo>=</mml:mo>                           <mml:mfrac>                              <mml:mn>1</mml:mn>                              <mml:mi>m</mml:mi>                           </mml:mfrac>                           <mml:msup>                              <mml:mrow>                                 <mml:mo stretchy="false">(</mml:mo>                                 <mml:mstyle mathvariant="bold" mathsize="normal">                                    <mml:mi>X</mml:mi>                                    <mml:mi>P</mml:mi>                                 </mml:mstyle>                                 <mml:mo stretchy="false">)</mml:mo>                              </mml:mrow>                              <mml:mi>T</mml:mi>                           </mml:msup>                           <mml:mstyle mathvariant="bold" mathsize="normal">                              <mml:mi>X</mml:mi>                              <mml:mi>P</mml:mi>                           </mml:mstyle>                           <mml:mo>=</mml:mo>                           <mml:mfrac>                              <mml:mn>1</mml:mn>                              <mml:mi>m</mml:mi>                           </mml:mfrac>                           <mml:msup>                              <mml:mstyle mathvariant="bold" mathsize="normal">                                 <mml:mi>P</mml:mi>                              </mml:mstyle>                              <mml:mi>T</mml:mi>                           </mml:msup>                           <mml:msup>                              <mml:mstyle mathvariant="bold" mathsize="normal">                                 <mml:mi>X</mml:mi>                              </mml:mstyle>                              <mml:mi>T</mml:mi>                           </mml:msup>                           <mml:mstyle mathvariant="bold" mathsize="normal">                              <mml:mi>X</mml:mi>                              <mml:mi>P</mml:mi>                           </mml:mstyle>                           <mml:mo>=</mml:mo>                           <mml:msup>                              <mml:mstyle mathvariant="bold" mathsize="normal">                                 <mml:mi>P</mml:mi>                              </mml:mstyle>                              <mml:mi>T</mml:mi>                           </mml:msup>                           <mml:msub>                              <mml:mo>&#x02211;</mml:mo>                              <mml:mstyle mathvariant="bold" mathsize="normal">                                 <mml:mi>X</mml:mi>                              </mml:mstyle>                           </mml:msub>                           <mml:mstyle mathvariant="bold" mathsize="normal">                              <mml:mi>P</mml:mi>                           </mml:mstyle>                           <mml:mo>=</mml:mo>                           <mml:mstyle mathvariant="bold" mathsize="normal">                              <mml:mi>D</mml:mi>                           </mml:mstyle>                           <mml:mo>,</mml:mo>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>where <bold>D </bold>is a diagonal matrix, <bold>&#x003a3;</bold><sub><bold>X </bold></sub>and <bold>&#x003a3;</bold><sub><bold>Y </bold></sub>are the sample covariance matrices of the original and new <italic>n </italic>variables, respectively. <bold>P </bold>can be obtained by the eigen decomposition of <bold>&#x003a3;</bold><sub><bold>X</bold></sub>. Therefore, PCA is very simple and easy to implement.</p><p>In this study, we use the software SMARTPCA by Patterson <italic>et al</italic>. [<xref ref-type="bibr" rid="B4">4</xref>]. SMARTPCA is specifically designed for genotype data and it offers options addressing issues such as linkage disequilibrium (LD) in analyzing genotype data. Patterson <italic>et al</italic>. [<xref ref-type="bibr" rid="B4">4</xref>] showed that the presence of LD in data distorts the distribution of eigenvalues, which makes selecting PCs according to the TW statistics meaningless. Therefore, we follow the suggestion and turn on the option to replace the values of each marker with the residuals from a multivariate regression without intercept on the 2 preceding markers. After PCA, we keep those PCs with <italic>p</italic>-values smaller than 5% for subsequent cluster analyses. Since STRUCTURE accepts only genotype data, the input to STRUCTURE is not processed with PCA.</p></sec><sec><title>Clustering algorithms</title><p>In this study, we investigate three generic clustering algorithms &#x02013; K-means, soft K-means and spectral clustering algorithms. In order to compare these generic clustering algorithms to algorithms designed specifically for population structure inference, we also run STRUCTURE on the datasets. We briefly introduce the three generic clustering algorithms and STRUCTURE in the folowing subsections.</p><sec><title>K-means</title><p>The K-means algorithm is an iterative descent algorithm that minimizes the within-cluster sum of squares (WSS) given the number of clusters <italic>K</italic>.</p><p><disp-formula id="bmcM1"><label>(1)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M3" name="1471-2105-10-S1-S73-i3" overflow="scroll">                        <mml:semantics>                           <mml:mrow>                              <mml:msub>                                 <mml:mi>W</mml:mi>                                 <mml:mi>K</mml:mi>                              </mml:msub>                              <mml:mo>=</mml:mo>                              <mml:mstyle displaystyle="true">                                 <mml:munderover>                                    <mml:mo>&#x02211;</mml:mo>                                    <mml:mrow>                                       <mml:mi>i</mml:mi>                                       <mml:mo>=</mml:mo>                                       <mml:mn>1</mml:mn>                                    </mml:mrow>                                    <mml:mi>K</mml:mi>                                 </mml:munderover>                                 <mml:mrow>                                    <mml:mstyle displaystyle="true">                                       <mml:munder>                                          <mml:mo>&#x02211;</mml:mo>                                          <mml:mrow>                                             <mml:mi>j</mml:mi>                                             <mml:mo>&#x02208;</mml:mo>                                             <mml:msub>                                                <mml:mi>C</mml:mi>                                                <mml:mi>i</mml:mi>                                             </mml:msub>                                          </mml:mrow>                                       </mml:munder>                                       <mml:mrow>                                          <mml:msup>                                             <mml:mrow>                                                <mml:mrow>                                                   <mml:mo>&#x02016;</mml:mo>                                                   <mml:mrow>                                                      <mml:msub>                                                         <mml:mstyle mathvariant="bold" mathsize="normal">                                                            <mml:mi>x</mml:mi>                                                         </mml:mstyle>                                                         <mml:mi>j</mml:mi>                                                      </mml:msub>                                                      <mml:mo>&#x02212;</mml:mo>                                                      <mml:msub>                                                         <mml:mi>&#x003bc;</mml:mi>                                                         <mml:mi>i</mml:mi>                                                      </mml:msub>                                                   </mml:mrow>                                                   <mml:mo>&#x02016;</mml:mo>                                                </mml:mrow>                                             </mml:mrow>                                             <mml:mn>2</mml:mn>                                          </mml:msup>                                       </mml:mrow>                                    </mml:mstyle>                                 </mml:mrow>                              </mml:mstyle>                              <mml:mo>,</mml:mo>                           </mml:mrow>                                                   </mml:semantics>                     </mml:math></disp-formula></p><p>where <bold>x</bold><sub><italic>j </italic></sub>is the feature vector representing sample <italic>j</italic>, <italic>&#x003bc;</italic><sub><italic>i </italic></sub>is the center of cluster <italic>i</italic>, and <italic>C</italic><sub><italic>i </italic></sub>is the set of samples in cluster <italic>i</italic>. We use the implementation of a variant by Hartigan and Wong [<xref ref-type="bibr" rid="B7">7</xref>] embedded in the R Language.</p></sec><sec><title>Soft K-means</title><p>The soft K-means algorithm assumes that samples follow a mixture of <italic>K </italic>multivariate Gaussian distributions <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M4" name="1471-2105-10-S1-S73-i4" overflow="scroll"><mml:semantics><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mtext>N</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula>, where <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M5" name="1471-2105-10-S1-S73-i5" overflow="scroll"><mml:semantics><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula>; <italic>&#x003bc;</italic><sub><italic>k </italic></sub>and <bold>&#x003a3;</bold><sub><italic>k </italic></sub>are the mean and covariance matrix for the <italic>k</italic><sup>th </sup>Gaussian distribution. Therefore, given the number of clusters <italic>K</italic>, the algorithm estimates the parameters <italic>&#x003b8; </italic>= (<italic>&#x003b4;</italic><sub>1</sub>,...,<italic>&#x003b4;</italic><sub><italic>K</italic></sub>, <italic>&#x003bc;</italic><sub>1</sub>, <bold>&#x003a3;</bold><sub>1</sub>,...,<italic>&#x003bc;</italic><sub><italic>K</italic></sub>, <bold>&#x003a3;</bold><sub><italic>K</italic></sub>) using the Expectation-Maximization Algorithm, while the unobserved latent variables are the labels of samples. In this study, we use MCLUST Version 3 [<xref ref-type="bibr" rid="B8">8</xref>] for R Language, which offers a wide selection of covariance matrix models.</p></sec><sec><title>Spectral clustering</title><p>The spectral clustering algorithm is based on the weighted graph partitioning problem. Considering a graph of <italic>m </italic>nodes, each node represents a sample and the weight on the edge between two nodes is the similarity between the two samples. We define the total similarity between two clusters <italic>A</italic>, <italic>B </italic>as</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M6" name="1471-2105-10-S1-S73-i6" overflow="scroll">                        <mml:semantics>                           <mml:mrow>                              <mml:mtext>Sim</mml:mtext>                              <mml:mo stretchy="false">(</mml:mo>                              <mml:mi>A</mml:mi>                              <mml:mo>,</mml:mo>                              <mml:mi>B</mml:mi>                              <mml:mo stretchy="false">)</mml:mo>                              <mml:mo>=</mml:mo>                              <mml:mstyle displaystyle="true">                                 <mml:munder>                                    <mml:mo>&#x02211;</mml:mo>                                    <mml:mrow>                                       <mml:mi>i</mml:mi>                                       <mml:mo>&#x02208;</mml:mo>                                       <mml:mi>A</mml:mi>                                    </mml:mrow>                                 </mml:munder>                                 <mml:mrow>                                    <mml:mstyle displaystyle="true">                                       <mml:munder>                                          <mml:mo>&#x02211;</mml:mo>                                          <mml:mrow>                                             <mml:mi>j</mml:mi>                                             <mml:mo>&#x02208;</mml:mo>                                             <mml:mi>B</mml:mi>                                          </mml:mrow>                                       </mml:munder>                                       <mml:mrow>                                          <mml:msub>                                             <mml:mstyle mathvariant="bold" mathsize="normal">                                                <mml:mi>S</mml:mi>                                             </mml:mstyle>                                             <mml:mrow>                                                <mml:mi>i</mml:mi>                                                <mml:mi>j</mml:mi>                                             </mml:mrow>                                          </mml:msub>                                       </mml:mrow>                                    </mml:mstyle>                                 </mml:mrow>                              </mml:mstyle>                              <mml:mo>,</mml:mo>                           </mml:mrow>                                                   </mml:semantics>                     </mml:math></disp-formula></p><p>where <bold>S </bold>is a <italic>m </italic>&#x000d7; <italic>m </italic>similarity matrix. Given the number of clusters <italic>K</italic>, we want to find a partition <italic>C</italic>* such that the following objective function is minimized.</p><p><disp-formula id="bmcM2"><label>(2)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M7" name="1471-2105-10-S1-S73-i7" overflow="scroll">                        <mml:semantics>                           <mml:mrow>                              <mml:mstyle mathvariant="bold" mathsize="normal">                                 <mml:mi>C</mml:mi>                              </mml:mstyle>                              <mml:mo>*</mml:mo>                              <mml:mo>=</mml:mo>                              <mml:mi>arg</mml:mi>                              <mml:mo>&#x02061;</mml:mo>                              <mml:munder>                                 <mml:mrow>                                    <mml:mi>min</mml:mi>                                    <mml:mo>&#x02061;</mml:mo>                                 </mml:mrow>                                 <mml:mstyle mathvariant="bold" mathsize="normal">                                    <mml:mi>C</mml:mi>                                 </mml:mstyle>                              </mml:munder>                              <mml:mstyle displaystyle="true">                                 <mml:munderover>                                    <mml:mo>&#x02211;</mml:mo>                                    <mml:mrow>                                       <mml:mi>k</mml:mi>                                       <mml:mo>=</mml:mo>                                       <mml:mn>1</mml:mn>                                    </mml:mrow>                                    <mml:mi>K</mml:mi>                                 </mml:munderover>                                 <mml:mrow>                                    <mml:mfrac>                                       <mml:mrow>                                          <mml:mtext>Sim</mml:mtext>                                          <mml:mo stretchy="false">(</mml:mo>                                          <mml:msub>                                             <mml:mi>C</mml:mi>                                             <mml:mi>k</mml:mi>                                          </mml:msub>                                          <mml:mo>,</mml:mo>                                          <mml:msubsup>                                             <mml:mo>&#x0222a;</mml:mo>                                             <mml:mrow>                                                <mml:mi>i</mml:mi>                                                <mml:mo>=</mml:mo>                                                <mml:mn>1</mml:mn>                                                <mml:mo>,</mml:mo>                                                <mml:mi>i</mml:mi>                                                <mml:mo>&#x02260;</mml:mo>                                                <mml:mi>k</mml:mi>                                             </mml:mrow>                                             <mml:mi>K</mml:mi>                                          </mml:msubsup>                                          <mml:msub>                                             <mml:mi>C</mml:mi>                                             <mml:mi>i</mml:mi>                                          </mml:msub>                                          <mml:mo stretchy="false">)</mml:mo>                                       </mml:mrow>                                       <mml:mrow>                                          <mml:mtext>Sim</mml:mtext>                                          <mml:mo stretchy="false">(</mml:mo>                                          <mml:msub>                                             <mml:mi>C</mml:mi>                                             <mml:mi>k</mml:mi>                                          </mml:msub>                                          <mml:mo>,</mml:mo>                                          <mml:msubsup>                                             <mml:mo>&#x0222a;</mml:mo>                                             <mml:mrow>                                                <mml:mi>i</mml:mi>                                                <mml:mo>=</mml:mo>                                                <mml:mn>1</mml:mn>                                             </mml:mrow>                                             <mml:mi>K</mml:mi>                                          </mml:msubsup>                                          <mml:msub>                                             <mml:mi>C</mml:mi>                                             <mml:mi>i</mml:mi>                                          </mml:msub>                                          <mml:mo stretchy="false">)</mml:mo>                                       </mml:mrow>                                    </mml:mfrac>                                 </mml:mrow>                              </mml:mstyle>                           </mml:mrow>                                                   </mml:semantics>                     </mml:math></disp-formula></p><p>Equation 2 can be expressed as follows.</p><p><disp-formula id="bmcM3"><label>(3)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M8" name="1471-2105-10-S1-S73-i8" overflow="scroll">                        <mml:semantics>                           <mml:mrow>                              <mml:mstyle mathvariant="bold" mathsize="normal">                                 <mml:mi>E</mml:mi>                              </mml:mstyle>                              <mml:mo>*</mml:mo>                              <mml:mo>=</mml:mo>                              <mml:mi>arg</mml:mi>                              <mml:mo>&#x02061;</mml:mo>                              <mml:munder>                                 <mml:mrow>                                    <mml:mi>min</mml:mi>                                    <mml:mo>&#x02061;</mml:mo>                                 </mml:mrow>                                 <mml:mstyle mathvariant="bold" mathsize="normal">                                    <mml:mi>E</mml:mi>                                 </mml:mstyle>                              </mml:munder>                              <mml:mstyle displaystyle="true">                                 <mml:munderover>                                    <mml:mo>&#x02211;</mml:mo>                                    <mml:mrow>                                       <mml:mi>k</mml:mi>                                       <mml:mo>=</mml:mo>                                       <mml:mn>1</mml:mn>                                    </mml:mrow>                                    <mml:mi>K</mml:mi>                                 </mml:munderover>                                 <mml:mrow>                                    <mml:mfrac>                                       <mml:mrow>                                          <mml:msubsup>                                             <mml:mstyle mathvariant="bold" mathsize="normal">                                                <mml:mi>e</mml:mi>                                             </mml:mstyle>                                             <mml:mi>k</mml:mi>                                             <mml:mi>T</mml:mi>                                          </mml:msubsup>                                          <mml:mo stretchy="false">(</mml:mo>                                          <mml:mstyle mathvariant="bold" mathsize="normal">                                             <mml:mi>D</mml:mi>                                          </mml:mstyle>                                          <mml:mo>&#x02212;</mml:mo>                                          <mml:mstyle mathvariant="bold" mathsize="normal">                                             <mml:mi>W</mml:mi>                                          </mml:mstyle>                                          <mml:mo stretchy="false">)</mml:mo>                                          <mml:msub>                                             <mml:mstyle mathvariant="bold" mathsize="normal">                                                <mml:mi>e</mml:mi>                                             </mml:mstyle>                                             <mml:mi>k</mml:mi>                                          </mml:msub>                                       </mml:mrow>                                       <mml:mrow>                                          <mml:msubsup>                                             <mml:mstyle mathvariant="bold" mathsize="normal">                                                <mml:mi>e</mml:mi>                                             </mml:mstyle>                                             <mml:mi>k</mml:mi>                                             <mml:mi>T</mml:mi>                                          </mml:msubsup>                                          <mml:mstyle mathvariant="bold" mathsize="normal">                                             <mml:mi>D</mml:mi>                                          </mml:mstyle>                                          <mml:msub>                                             <mml:mstyle mathvariant="bold" mathsize="normal">                                                <mml:mi>e</mml:mi>                                             </mml:mstyle>                                             <mml:mi>k</mml:mi>                                          </mml:msub>                                       </mml:mrow>                                    </mml:mfrac>                                 </mml:mrow>                              </mml:mstyle>                              <mml:mo>,</mml:mo>                           </mml:mrow>                                                   </mml:semantics>                     </mml:math></disp-formula></p><p>where <bold>E </bold>= (<bold>e</bold><sub>1</sub>,...,<bold>e</bold><sub><italic>K</italic></sub>) is a <italic>m </italic>&#x000d7; <italic>K </italic>indicator matrix and <bold>D </bold>is a <italic>m </italic>&#x000d7; <italic>m </italic>diagonal degree matrix. The <italic>i</italic><sup>th </sup>element of <bold>e</bold><sub><italic>k </italic></sub>is 1 if sample <italic>i </italic>is in cluster <italic>k</italic>. Otherwise, it is 0. <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M9" name="1471-2105-10-S1-S73-i9" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>D</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>S</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula>. Since finding the optimal <bold>E </bold>is NP-hard, spectral clustering solves the minimization problem by allowing the entries of <bold>E </bold>to have real values. This amounts to finding the <italic>K </italic>eigenvectors of <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M10" name="1471-2105-10-S1-S73-i10" overflow="scroll"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>D</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>D</mml:mi></mml:mstyle><mml:mo>&#x02212;</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>S</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>D</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> with the smallest nonzero eigenvalues. We implemented the algorithm, described in Figure <xref ref-type="fig" rid="F2">2</xref>, proposed by Ng <italic>et al</italic>. [<xref ref-type="bibr" rid="B9">9</xref>] in R. In the last line of the algorithm, one can use any algorithm to perform the clustering. Therefore, we investigate K-means and soft K-means, producing two variants of the spectral clustering algorithm. In this study, we use a radial basis function to calculate the similarity between two samples.</p><p><disp-formula id="bmcM4"><label>(4)</label><bold>S</bold><sub><italic>ij </italic></sub>= exp(-<italic>&#x003b3;</italic>||<bold>x</bold><sub><italic>i </italic></sub>- <bold>x</bold><sub><italic>j</italic></sub>||<sup>2</sup>),</disp-formula></p><p>where <italic>&#x003b3; </italic>is a constant.</p><fig position="float" id="F2"><label>Figure 2</label><caption><p>The spectral clustering algorithm.</p></caption><graphic xlink:href="1471-2105-10-S1-S73-2"/></fig></sec><sec><title>STRUCTURE</title><p>Given the number of clusters <italic>K </italic>and genotype data <bold>X</bold>, STRUCTURE [<xref ref-type="bibr" rid="B10">10</xref>] models the population structure with three vectors of parameters &#x02013; <bold>Q</bold>, <bold>Z </bold>and <bold>P</bold>. The genotype data and parameter vectors contain the following elements.</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M11" name="1471-2105-10-S1-S73-i11" overflow="scroll">                        <mml:semantics>                           <mml:mrow>                              <mml:mtable>                                 <mml:mtr>                                    <mml:mtd>                                       <mml:mrow>                                          <mml:msubsup>                                             <mml:mi>x</mml:mi>                                             <mml:mi>l</mml:mi>                                             <mml:mrow>                                                <mml:mo stretchy="false">(</mml:mo>                                                <mml:mi>i</mml:mi>                                                <mml:mo>,</mml:mo>                                                <mml:mi>a</mml:mi>                                                <mml:mo stretchy="false">)</mml:mo>                                             </mml:mrow>                                          </mml:msubsup>                                       </mml:mrow>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mo>=</mml:mo>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mrow>                                          <mml:mtext>allele&#x000a0;copy&#x000a0;</mml:mtext>                                          <mml:mi>a</mml:mi>                                          <mml:mtext>&#x000a0;of&#x000a0;individual&#x000a0;</mml:mtext>                                          <mml:mi>i</mml:mi>                                          <mml:mtext>&#x000a0;at&#x000a0;locus&#x000a0;</mml:mtext>                                          <mml:mi>l</mml:mi>                                          <mml:mo>;</mml:mo>                                       </mml:mrow>                                    </mml:mtd>                                 </mml:mtr>                                 <mml:mtr>                                    <mml:mtd>                                       <mml:mrow>                                          <mml:msubsup>                                             <mml:mi>q</mml:mi>                                             <mml:mi>k</mml:mi>                                             <mml:mrow>                                                <mml:mo stretchy="false">(</mml:mo>                                                <mml:mi>i</mml:mi>                                                <mml:mo stretchy="false">)</mml:mo>                                             </mml:mrow>                                          </mml:msubsup>                                       </mml:mrow>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mo>=</mml:mo>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mrow>                                          <mml:mtext>proportion&#x000a0;of&#x000a0;individual&#x000a0;</mml:mtext>                                          <mml:mi>i</mml:mi>                                          <mml:mtext>'s&#x000a0;genome</mml:mtext>                                       </mml:mrow>                                    </mml:mtd>                                 </mml:mtr>                                 <mml:mtr>                                    <mml:mtd>                                       <mml:mrow/>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mrow/>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mrow>                                          <mml:mtext>that&#x000a0;originated&#x000a0;from&#x000a0;population&#x000a0;</mml:mtext>                                          <mml:mi>k</mml:mi>                                          <mml:mtext>;</mml:mtext>                                       </mml:mrow>                                    </mml:mtd>                                 </mml:mtr>                                 <mml:mtr>                                    <mml:mtd>                                       <mml:mrow>                                          <mml:msubsup>                                             <mml:mi>z</mml:mi>                                             <mml:mi>l</mml:mi>                                             <mml:mrow>                                                <mml:mo stretchy="false">(</mml:mo>                                                <mml:mi>i</mml:mi>                                                <mml:mo>,</mml:mo>                                                <mml:mi>a</mml:mi>                                                <mml:mo stretchy="false">)</mml:mo>                                             </mml:mrow>                                          </mml:msubsup>                                       </mml:mrow>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mo>=</mml:mo>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mrow>                                          <mml:mtext>population&#x000a0;origin&#x000a0;of&#x000a0;allele&#x000a0;copy&#x000a0;</mml:mtext>                                          <mml:msubsup>                                             <mml:mi>x</mml:mi>                                             <mml:mi>l</mml:mi>                                             <mml:mrow>                                                <mml:mo stretchy="false">(</mml:mo>                                                <mml:mi>i</mml:mi>                                                <mml:mo>,</mml:mo>                                                <mml:mi>a</mml:mi>                                                <mml:mo stretchy="false">)</mml:mo>                                             </mml:mrow>                                          </mml:msubsup>                                          <mml:mtext>;</mml:mtext>                                       </mml:mrow>                                    </mml:mtd>                                 </mml:mtr>                                 <mml:mtr>                                    <mml:mtd>                                       <mml:mrow>                                          <mml:msub>                                             <mml:mi>p</mml:mi>                                             <mml:mrow>                                                <mml:mi>k</mml:mi>                                                <mml:mi>l</mml:mi>                                                <mml:mi>j</mml:mi>                                             </mml:mrow>                                          </mml:msub>                                       </mml:mrow>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mo>=</mml:mo>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mrow>                                          <mml:mtext>frequency&#x000a0;of&#x000a0;allele&#x000a0;</mml:mtext>                                          <mml:mi>j</mml:mi>                                          <mml:mtext>&#x000a0;at&#x000a0;locus&#x000a0;</mml:mtext>                                          <mml:mi>l</mml:mi>                                          <mml:mtext>&#x000a0;in</mml:mtext>                                       </mml:mrow>                                    </mml:mtd>                                 </mml:mtr>                                 <mml:mtr>                                    <mml:mtd>                                       <mml:mrow/>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mrow/>                                    </mml:mtd>                                    <mml:mtd>                                       <mml:mrow>                                          <mml:mtext>population&#x000a0;</mml:mtext>                                          <mml:mi>k</mml:mi>                                          <mml:mo>.</mml:mo>                                       </mml:mrow>                                    </mml:mtd>                                 </mml:mtr>                              </mml:mtable>                           </mml:mrow>                                                   </mml:semantics>                     </mml:math></disp-formula></p><p>In diploid organisms, there are two copies of alleles at each locus on an autosomal chromosome, and hence <italic>a </italic>&#x02208; {1, 2}. The probability model for (<bold>X</bold>, <bold>Z</bold>, <bold>P</bold>, <bold>Q</bold>) is described by the following equations:</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M12" name="1471-2105-10-S1-S73-i12" overflow="scroll">                        <mml:semantics>                           <mml:mtable>                              <mml:mtr>                                 <mml:mtd>                                    <mml:mtext>P</mml:mtext>                                    <mml:mo stretchy="false">(</mml:mo>                                    <mml:msubsup>                                       <mml:mi>x</mml:mi>                                       <mml:mi>l</mml:mi>                                       <mml:mrow>                                          <mml:mo stretchy="false">(</mml:mo>                                          <mml:mi>i</mml:mi>                                          <mml:mo>,</mml:mo>                                          <mml:mi>a</mml:mi>                                          <mml:mo stretchy="false">)</mml:mo>                                       </mml:mrow>                                    </mml:msubsup>                                    <mml:mo>=</mml:mo>                                    <mml:mi>j</mml:mi>                                    <mml:mo>|</mml:mo>                                    <mml:mstyle mathvariant="bold" mathsize="normal">                                       <mml:mi>Z</mml:mi>                                    </mml:mstyle>                                    <mml:mo>,</mml:mo>                                    <mml:mstyle mathvariant="bold" mathsize="normal">                                       <mml:mi>P</mml:mi>                                    </mml:mstyle>                                    <mml:mo>,</mml:mo>                                    <mml:mstyle mathvariant="bold" mathsize="normal">                                       <mml:mi>Q</mml:mi>                                    </mml:mstyle>                                    <mml:mo stretchy="false">)</mml:mo>                                    <mml:mo>=</mml:mo>                                    <mml:msub>                                       <mml:mi>p</mml:mi>                                       <mml:mrow>                                          <mml:msubsup>                                             <mml:mi>z</mml:mi>                                             <mml:mi>l</mml:mi>                                             <mml:mrow>                                                <mml:msub>                                                   <mml:mrow>                                                      <mml:mo stretchy="false">(</mml:mo>                                                      <mml:mi>i</mml:mi>                                                      <mml:mo>,</mml:mo>                                                      <mml:mi>a</mml:mi>                                                      <mml:mo stretchy="false">)</mml:mo>                                                   </mml:mrow>                                                   <mml:mrow>                                                      <mml:mi>l</mml:mi>                                                      <mml:mi>j</mml:mi>                                                   </mml:mrow>                                                </mml:msub>                                             </mml:mrow>                                          </mml:msubsup>                                       </mml:mrow>                                    </mml:msub>                                    <mml:mo>;</mml:mo>                                 </mml:mtd>                              </mml:mtr>                              <mml:mtr>                                 <mml:mtd>                                    <mml:mi>P</mml:mi>                                    <mml:mo stretchy="false">(</mml:mo>                                    <mml:msubsup>                                       <mml:mi>z</mml:mi>                                       <mml:mi>l</mml:mi>                                       <mml:mrow>                                          <mml:mo stretchy="false">(</mml:mo>                                          <mml:mi>i</mml:mi>                                          <mml:mo>,</mml:mo>                                          <mml:mi>a</mml:mi>                                          <mml:mo stretchy="false">)</mml:mo>                                       </mml:mrow>                                    </mml:msubsup>                                    <mml:mo stretchy="false">)</mml:mo>                                    <mml:mo>=</mml:mo>                                    <mml:mi>k</mml:mi>                                    <mml:mo>|</mml:mo>                                    <mml:mstyle mathvariant="bold" mathsize="normal">                                       <mml:mi>P</mml:mi>                                    </mml:mstyle>                                    <mml:mo>,</mml:mo>                                    <mml:mstyle mathvariant="bold" mathsize="normal">                                       <mml:mi>Q</mml:mi>                                    </mml:mstyle>                                    <mml:mo stretchy="false">)</mml:mo>                                    <mml:mo>=</mml:mo>                                    <mml:msubsup>                                       <mml:mi>q</mml:mi>                                       <mml:mi>k</mml:mi>                                       <mml:mrow>                                          <mml:mo stretchy="false">(</mml:mo>                                          <mml:mi>i</mml:mi>                                          <mml:mo stretchy="false">)</mml:mo>                                       </mml:mrow>                                    </mml:msubsup>                                    <mml:mo>;</mml:mo>                                 </mml:mtd>                              </mml:mtr>                              <mml:mtr>                                 <mml:mtd>                                    <mml:msub>                                       <mml:mstyle mathvariant="bold" mathsize="normal">                                          <mml:mi>p</mml:mi>                                       </mml:mstyle>                                       <mml:mrow>                                          <mml:mi>k</mml:mi>                                          <mml:mi>l</mml:mi>                                       </mml:mrow>                                    </mml:msub>                                    <mml:mo>~</mml:mo>                                    <mml:mtext>D</mml:mtext>                                    <mml:mo stretchy="false">(</mml:mo>                                    <mml:msub>                                       <mml:mi>&#x003bb;</mml:mi>                                       <mml:mn>1</mml:mn>                                    </mml:msub>                                    <mml:mo>,</mml:mo>                                    <mml:mo>&#x02026;</mml:mo>                                    <mml:mo>,</mml:mo>                                    <mml:msub>                                       <mml:mi>&#x003bb;</mml:mi>                                       <mml:mrow>                                          <mml:msub>                                             <mml:mi>J</mml:mi>                                             <mml:mi>l</mml:mi>                                          </mml:msub>                                       </mml:mrow>                                    </mml:msub>                                    <mml:mo stretchy="false">)</mml:mo>                                    <mml:mo>,</mml:mo>                                 </mml:mtd>                              </mml:mtr>                           </mml:mtable>                                                   </mml:semantics>                     </mml:math></disp-formula></p><p>where D(&#x000b7;) is the Dirichlet distribution, <italic>J</italic><sub><italic>l </italic></sub>is the number of alleles at locus <italic>l</italic>, and <italic>&#x003bb;</italic><sub>1 </sub>= ... = <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M13" name="1471-2105-10-S1-S73-i13" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> = 1.0, giving a uniform distribution on the allele frequencies;</p><p><disp-formula><bold>q</bold><sup>(<italic>i</italic>) </sup>~ D(<italic>&#x003b1;</italic>,...,<italic>&#x003b1;</italic>),</disp-formula></p><p>where D(&#x000b7;) is again the Dirichlet distribution and <italic>&#x003b1; </italic>&#x02208; [0, 10] is uniformly distributed. The estimates of <bold>Z</bold>, <bold>P</bold>, and <bold>Q </bold>are obtained by sampling <bold>Z</bold>, <bold>P</bold>, <bold>Q </bold>from the posterior distribution P(<bold>Z</bold>, <bold>P</bold>, <bold>Q</bold>|<bold>X</bold>) using a MCMC algorithm. In this study, the burn-in length is set to 5,000 and another 5,000 samples are collected after burn-in for parameter estimation.</p></sec></sec><sec><title>Inferring the number of clusters</title><p>The number of clusters is always an important issue in cluster analysis. As a model-based algorithm, STRUCTURE estimates the number of clusters <italic>K </italic>using the posterior distribution of <italic>K</italic></p><p><disp-formula>P(<italic>K</italic>|<bold>X</bold>) &#x0221d; P(<bold>X</bold>|<italic>K</italic>)P(<italic>K</italic>),</disp-formula></p><p>where <bold>X </bold>denotes the genotype data. In this study, we investigate two methods for selecting the number of clusters. One is a distance-based generic method using the gap statistic proposed by Tibshirani <italic>et al</italic>. [<xref ref-type="bibr" rid="B11">11</xref>]. The other is by using the Bayesian Information Criterion (BIC) [<xref ref-type="bibr" rid="B12">12</xref>] as the model selection criterion with the soft K-means clustering algorithm. We briefly introduce the two methods in the following paragraphs. The gap statistic is a heuristic method based on the WSS given in Equation 1. Given the number of clusters, we expect smaller WSS in a dataset that has clusters than in one that do not. Therefore, the gap statistic is defined as follows.</p><p><disp-formula id="bmcM5"><label>(5)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M14" name="1471-2105-10-S1-S73-i14" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:mtext>Gap</mml:mtext>                           <mml:mo stretchy="false">(</mml:mo>                           <mml:mi>k</mml:mi>                           <mml:mo stretchy="false">)</mml:mo>                           <mml:mo>=</mml:mo>                           <mml:mi>log</mml:mi>                           <mml:mo>&#x02061;</mml:mo>                           <mml:mfrac>                              <mml:mrow>                                 <mml:mtext>E</mml:mtext>                                 <mml:mo stretchy="false">(</mml:mo>                                 <mml:msubsup>                                    <mml:mi>W</mml:mi>                                    <mml:mi>k</mml:mi>                                    <mml:mi>R</mml:mi>                                 </mml:msubsup>                                 <mml:mo stretchy="false">)</mml:mo>                              </mml:mrow>                              <mml:mrow>                                 <mml:mtext>E</mml:mtext>                                 <mml:mo stretchy="false">(</mml:mo>                                 <mml:msubsup>                                    <mml:mi>W</mml:mi>                                    <mml:mn>1</mml:mn>                                    <mml:mi>R</mml:mi>                                 </mml:msubsup>                                 <mml:mo stretchy="false">)</mml:mo>                              </mml:mrow>                           </mml:mfrac>                           <mml:mo>&#x02212;</mml:mo>                           <mml:mi>log</mml:mi>                           <mml:mo>&#x02061;</mml:mo>                           <mml:mfrac>                              <mml:mrow>                                 <mml:msub>                                    <mml:mi>W</mml:mi>                                    <mml:mi>k</mml:mi>                                 </mml:msub>                              </mml:mrow>                              <mml:mrow>                                 <mml:msub>                                    <mml:mi>W</mml:mi>                                    <mml:mn>1</mml:mn>                                 </mml:msub>                              </mml:mrow>                           </mml:mfrac>                           <mml:mo>,</mml:mo>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>where E(<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M15" name="1471-2105-10-S1-S73-i15" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>k</mml:mi><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula>) is the expectation of the WSS for the reference dataset, which has no clusters. Tibshirani <italic>et al</italic>. [<xref ref-type="bibr" rid="B11">11</xref>] suggested using a uniformly distributed reference dataset. E(<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M16" name="1471-2105-10-S1-S73-i5" overflow="scroll"><mml:semantics><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula>) is estimated by randomly generating <italic>B </italic>uniformly distributed datasets.</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M17" name="1471-2105-10-S1-S73-i16" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:mover accent="true">                              <mml:mtext>E</mml:mtext>                              <mml:mo>^</mml:mo>                           </mml:mover>                           <mml:mo stretchy="false">(</mml:mo>                           <mml:msubsup>                              <mml:mi>W</mml:mi>                              <mml:mi>k</mml:mi>                              <mml:mi>R</mml:mi>                           </mml:msubsup>                           <mml:mo stretchy="false">)</mml:mo>                           <mml:mo>=</mml:mo>                           <mml:mfrac>                              <mml:mn>1</mml:mn>                              <mml:mi>B</mml:mi>                           </mml:mfrac>                           <mml:mstyle displaystyle="true">                              <mml:munderover>                                 <mml:mo>&#x02211;</mml:mo>                                 <mml:mrow>                                    <mml:mi>b</mml:mi>                                    <mml:mo>=</mml:mo>                                    <mml:mn>1</mml:mn>                                 </mml:mrow>                                 <mml:mi>B</mml:mi>                              </mml:munderover>                              <mml:mrow>                                 <mml:msubsup>                                    <mml:mi>W</mml:mi>                                    <mml:mi>k</mml:mi>                                    <mml:mrow>                                       <mml:mi>R</mml:mi>                                       <mml:mo stretchy="false">(</mml:mo>                                       <mml:mi>b</mml:mi>                                       <mml:mo stretchy="false">)</mml:mo>                                    </mml:mrow>                                 </mml:msubsup>                              </mml:mrow>                           </mml:mstyle>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>We then estimate the number of clusters by finding the smallest <italic>K </italic>such that</p><p><disp-formula id="bmcM6"><label>(6)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M18" name="1471-2105-10-S1-S73-i17" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:mtext>Gap</mml:mtext>                           <mml:mo stretchy="false">(</mml:mo>                           <mml:mi>K</mml:mi>                           <mml:mo stretchy="false">)</mml:mo>                           <mml:mo>&#x02265;</mml:mo>                           <mml:mtext>Gap</mml:mtext>                           <mml:mo stretchy="false">(</mml:mo>                           <mml:mi>K</mml:mi>                           <mml:mo>+</mml:mo>                           <mml:mn>1</mml:mn>                           <mml:mo stretchy="false">)</mml:mo>                           <mml:mo>&#x02212;</mml:mo>                           <mml:msub>                              <mml:msup>                                 <mml:mi>s</mml:mi>                                 <mml:mo>&#x02032;</mml:mo>                              </mml:msup>                              <mml:mrow>                                 <mml:mi>K</mml:mi>                                 <mml:mo>+</mml:mo>                                 <mml:mn>1</mml:mn>                              </mml:mrow>                           </mml:msub>                           <mml:mo>,</mml:mo>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>where <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M19" name="1471-2105-10-S1-S73-i18" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:msup><mml:mi>s</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:mfrac></mml:mrow></mml:msqrt></mml:mrow></mml:semantics></mml:math></inline-formula> and <italic>s</italic><sub><italic>K</italic>+1 </sub>is the standard error of <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M20" name="1471-2105-10-S1-S73-i19" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula>. The gap statistic can be used with any clustering algorithm. In this study, we use it along with K-means to predict the number of clusters. It is generally the case that we can better fit a dataset to the model with more parameters, resulting in higher likelihood or lower sum of squared error. Therefore, the BIC score addresses this issue by penalizing the number of parameters. It is defined as</p><p><disp-formula>BIC = 2L(<italic>&#x003b8;</italic>*) - log(<italic>m</italic>)|<italic>&#x003b8;</italic>*|,</disp-formula></p><p>where L is the log likelihood function, <italic>&#x003b8;</italic>* is the parameter set maximizing the likelihood and <italic>m </italic>is the number of observations or samples. The BIC score is used in MCLUST Version 3 [<xref ref-type="bibr" rid="B8">8</xref>] as the model selection criterion.</p></sec><sec><title>Evaluation metric</title><p>In population structure inference, given the number of clusters, each individual in the dataset is assigned an estimated membership coefficient for each cluster. The coefficient indicates the likelihood that an individual descends from a specific population origin. By assigning each individual to the most likely cluster, we have obtained a partition of the individuals in a dataset. A partition is a set of mutually exclusive and collectively exhaustive clusters. Given two partitions, we use the algorithm proposed by Konovalov <italic>et al</italic>. [<xref ref-type="bibr" rid="B13">13</xref>] to measure the distance between them. The distance between two partitions is defined as the minimum number of individuals that need to be removed from each partition in order to make the two partitions identical. For clarity, we scale the distance measure to [0, 1].</p><p>For the simulated datasets, we calculate the distance between the gold-standard partition and the partition generated by each clustering algorithm. The smaller the distance between the two partitions, the better the performance. For the real datasets, we compare the partition produced by STRUCTURE to the partitions produced by all other clustering algorithms investigated in this study. This is because STRUCTURE is a widely used algorithm in inferring population structure.</p></sec></sec><sec><title>Results and discussion</title><p>Table <xref ref-type="table" rid="T2">2</xref> shows the number of significant PCs selected for each dataset using the TW statisitc at <italic>p</italic>-value = 0.05. We can see that PCA reduces the number of variables from around 5,000 to at most 70. However, we suspect that there are still noisy and non-informative PCs hidden in those selected significant ones. Therefore, we are also interested in using only the top-3 PCs with the largest eigenvalues. We then perform cluster analyses on the reduced datasets using those generic algorithms described in Sectoin <bold>Methods</bold>. The results are shown in the following subsections.</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Number of principal components selected using TW statistic at <italic>p</italic>-value = 0.05. The simulated datasets are denoted as s1 through s4.</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Set</td><td align="right">close</td><td align="right">dist</td><td align="right">s1</td><td align="right">s2</td><td align="right">s3</td><td align="right">s4</td></tr></thead><tbody><tr><td align="left">#PCs</td><td align="right">15</td><td align="right">70</td><td align="right">2</td><td align="right">4</td><td align="right">18</td><td align="right">3</td></tr></tbody></table></table-wrap><sec><title>Simulated Data</title><p>Evaluating the performance of the clustering algorithms on simulated datasets is straightforward since the gold standard partition for each dataset is available. The performance, in terms of distance between the gold standard partition and the predicted one, is summarized in Table <xref ref-type="table" rid="T4">4</xref>. The measure of distance is described in Section <bold>Methods</bold>. The parameter <italic>&#x003b3; </italic>in Equation 4 is not tuned for all the simulated datasets. It is set to either 1 or <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M21" name="1471-2105-10-S1-S73-i20" overflow="scroll"><mml:semantics><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula>, except for the third dataset. The reason for setting <italic>&#x003b3; </italic>= 2<sup>-4 </sup>is because when the algorithm tries to obtain the eigenvalues and eigenvectors of <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M22" name="1471-2105-10-S1-S73-i10" overflow="scroll"><mml:semantics><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>D</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>D</mml:mi></mml:mstyle><mml:mo>&#x02212;</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>S</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>D</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> (as described in Figure <xref ref-type="fig" rid="F2">2</xref>) the R function eigen seems to be caught in an infinite loop for <italic>&#x003b3; </italic>= 2<sup>-<italic>g</italic></sup>, <italic>g </italic>&#x02208; {0, 1, 2, 3}. For the first two datasets, all the clustering algorithms show perfect results. This is probably because these two datasets contain independent and equal-sized subpopulations. For the third dataset, apart from the two variants of spectral clustering algorithm, soft K-means and STRUCTURE perform equally well while K-means produces comparable results. Moreover, soft K-means performs the best on the fourth dataset while STRUCTURE gives the worst performance. To better analyze the results, we visually compare the clustering algorithms using bar plots shown in Figure <xref ref-type="fig" rid="F3">3</xref>. The bar plots are generated using software DISTRUCT [<xref ref-type="bibr" rid="B14">14</xref>]. According to the demography in Figure <xref ref-type="fig" rid="F1">1</xref>, population 3 does not contain admixed individuals but STRUCTURE fails to assign the individuals in population 3 to only one cluster as the other algorithms do. However, when setting <italic>K </italic>= 3, STRUCTURE performs very well and reflects the demography used to simulate the data. The bar plots are shown in Figure <xref ref-type="fig" rid="F4">4</xref>. We can see that individuals in population 1, 3 and 4 are clustered into distinct groups, while individuals in population 2 equally likely belong to the two clusters occupied by population 1 and 3. Soft K-means produces similar results, while the other algorithms group individuals in population 2 with individuals in either population 1 or population 3. Table <xref ref-type="table" rid="T3">3</xref> shows the number of clusters inferred by the gap statistic, the BIC score and STRUCTURE. We can see that the BIC score with PCs suggested by the TW distribution and STRUCTURE make identical predictions on the simulated datasets. When the BIC score is used with the top-3 PCs, it makes the correct prediction on the second simulated dataset but fails on the third one. Therefore, these two approaches perform comparably on the simulated datasets. The gap statistic fails to make the correct prediction on all but the first simulated dataset unless only 2 or 3 PCs are used.</p><table-wrap position="float" id="T3"><label>Table 3</label><caption><p>Predicted number of clusters for each dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Set</td><td align="right">close</td><td align="right">dist</td><td align="right">s1</td><td align="right">s2</td><td align="right">s3</td><td align="right">s4</td></tr></thead><tbody><tr><td align="left">True <italic>K</italic></td><td align="right">NA</td><td align="right">NA</td><td align="right">3</td><td align="right">4</td><td align="right">4</td><td align="right">4</td></tr><tr><td align="left">Gap</td><td align="right">1</td><td align="right">7</td><td align="right"><bold>3</bold></td><td align="right">1</td><td align="right">1</td><td align="right">1</td></tr><tr><td></td><td align="right">1<sup>1</sup></td><td align="right">1<sup>1</sup></td><td align="right">--</td><td align="right"><bold>4</bold><sup>1</sup></td><td align="right"><bold>4</bold><sup>3</sup></td><td align="right"><bold>4</bold><sup>2</sup></td></tr><tr><td align="left">BIC</td><td align="right">3</td><td align="right">3</td><td align="right"><bold>3</bold></td><td align="right">5</td><td align="right"><bold>4</bold></td><td align="right"><bold>4</bold></td></tr><tr><td></td><td align="right">3<sup>1</sup></td><td align="right">6<sup>1</sup></td><td align="right">--</td><td align="right"><bold>4</bold><sup>1</sup></td><td align="right">6<sup>1</sup></td><td align="right"><bold>4</bold><sup>1</sup></td></tr><tr><td align="left">STRU<sup>4</sup></td><td align="right">6</td><td align="right">6</td><td align="right"><bold>3</bold></td><td align="right">5</td><td align="right"><bold>4</bold></td><td align="right"><bold>4</bold></td></tr></tbody></table><table-wrap-foot><p><sup>1 </sup>3 PCs. <sup>2 </sup>2<sup>nd </sup><italic>K</italic>. <sup>3 </sup>3PCS, 2<sup>nd </sup><italic>K</italic>. <sup>4 </sup>STRUCTURE.</p></table-wrap-foot></table-wrap><table-wrap position="float" id="T4"><label>Table 4</label><caption><p>Results on the simulated datasets in terms of distance</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Set</td><td align="right">K<sup>1</sup></td><td align="right">SK<sup>2</sup></td><td align="right">SpK<sup>3</sup></td><td align="right">SpSK<sup>4</sup></td><td align="right">STRU<sup>5</sup></td></tr></thead><tbody><tr><td align="left">1</td><td align="right"><bold>0</bold></td><td align="right"><bold>0</bold></td><td align="right"><bold>0</bold><sup>6</sup></td><td align="right"><bold>0</bold><sup>6</sup></td><td align="right"><bold>0</bold></td></tr><tr><td align="left">2</td><td align="right"><bold>0</bold></td><td align="right"><bold>0</bold></td><td align="right"><bold>0</bold><sup>7</sup></td><td align="right"><bold>0</bold><sup>7</sup></td><td align="right"><bold>0</bold></td></tr><tr><td align="left">3</td><td align="right">0.01</td><td align="right"><bold>0</bold></td><td align="right">0.598<sup>8</sup></td><td align="right">0.596<sup>8</sup></td><td align="right"><bold>0</bold></td></tr><tr><td align="left">4</td><td align="right">0.058</td><td align="right"><bold>0.034</bold></td><td align="right">0.048<sup>7</sup></td><td align="right">0.089<sup>7</sup></td><td align="right">0.342</td></tr></tbody></table><table-wrap-foot><p><sup>1</sup>K-means. <sup>2</sup>Soft K-means. <sup>3</sup>Spectral + K-means. <sup>4</sup>Spectral + Soft K-means. <sup>5</sup>STRUCTURE. <sup>6 </sup><italic>&#x003b3; </italic>= <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M23" name="1471-2105-10-S1-S73-i20" overflow="scroll"><mml:semantics><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:semantics></mml:math></inline-formula>. <sup>7 </sup><italic>&#x003b3; </italic>= 1. <sup>8 </sup><italic>&#x003b3; </italic>= 2<sup>-4</sup>.</p></table-wrap-foot></table-wrap><fig position="float" id="F3"><label>Figure 3</label><caption><p>Bar plots of results of the fourth simulated dataset (<italic>K </italic>= 4).</p></caption><graphic xlink:href="1471-2105-10-S1-S73-3"/></fig><fig position="float" id="F4"><label>Figure 4</label><caption><p>Bar plots of results of the fourth simulated dataset (<italic>K </italic>= 3).</p></caption><graphic xlink:href="1471-2105-10-S1-S73-4"/></fig></sec><sec><title>Real Data</title><p>In this section, we compare the results generated by the generic clutering algorithms to those produced by STRUCTURE since no gold standard partitions are available for the real datasets. The results for the distant and close dataset are shown in Table <xref ref-type="table" rid="T5">5</xref> and Table <xref ref-type="table" rid="T6">6</xref>, respectively. For the distant dataset, using all the 70 significant PCs, the partition given by soft K-means at <italic>K </italic>= 2 is identical to that produced by STRUCTURE. When only the top-3 PCs are used, all the clustering algorithms produce partitions similar to that predicted by STRUCTURE. This implies that all the distance-based generic algorithms investigated in this study are sensitive to noisy and non-informative variables, which are used in the calculation of distance or similarity.</p><table-wrap position="float" id="T5"><label>Table 5</label><caption><p>Comparison of the results on the distant dataset with STRUCTURE</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">K</td><td align="right">#PCs</td><td align="right">K<sup>1</sup></td><td align="right">SK<sup>2</sup></td><td align="right">SpK<sup>3</sup></td><td align="right">SpSK<sup>4</sup></td></tr></thead><tbody><tr><td align="left">2</td><td align="right">70</td><td align="right">0.252</td><td align="right"><bold>0</bold></td><td align="right">0.137<sup>5</sup></td><td align="right">0.103<sup>6</sup></td></tr><tr><td align="left">2</td><td align="right">3</td><td align="right">0.03</td><td align="right"><bold>0.003</bold></td><td align="right">0.004<sup>7</sup></td><td align="right">0.019<sup>7</sup></td></tr><tr><td align="left">3</td><td align="right">70</td><td align="right">0.3</td><td align="right">0.101</td><td align="right">0.422<sup>8</sup></td><td align="right">0.349<sup>9</sup></td></tr><tr><td align="left">3</td><td align="right">3</td><td align="right"><bold>0.042</bold></td><td align="right"><bold>0.045</bold></td><td align="right"><bold>0.041</bold><sup>7</sup></td><td align="right">0.123<sup>7</sup></td></tr><tr><td align="left">4</td><td align="right">70</td><td align="right">0.401</td><td align="right">0.617</td><td align="right">0.414<sup>8</sup></td><td align="right">0.433<sup>10</sup></td></tr><tr><td align="left">4</td><td align="right">3</td><td align="right">0.304</td><td align="right">0.277</td><td align="right">0.311<sup>7</sup></td><td align="right">0.337<sup>7</sup></td></tr></tbody></table><table-wrap-foot><p><sup>1</sup>K-means. <sup>2</sup>Soft K-means. <sup>3</sup>Spectral + K-means. <sup>4</sup>Spectral + Soft K-means. <sup>5</sup><italic>&#x003b3; </italic>= 2<sup>-5</sup>. <sup>6</sup><italic>&#x003b3; </italic>= 2<sup>-1.5</sup>. <sup>7</sup><italic>&#x003b3; </italic>= 1. <sup>8</sup><italic>&#x003b3; </italic>= 2<sup>-6.5</sup>. <sup>9</sup><italic>&#x003b3; </italic>= 2<sup>-6</sup>. <sup>10</sup><italic>&#x003b3; </italic>= 2<sup>6</sup></p></table-wrap-foot></table-wrap><table-wrap position="float" id="T6"><label>Table 6</label><caption><p>Comparison of the results on the close dataset with STRUCTURE</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">K</td><td align="right">#PCs</td><td align="right">K<sup>1</sup></td><td align="right">SK<sup>2</sup></td><td align="right">SpK<sup>3</sup></td><td align="right">SpSK<sup>4</sup></td></tr></thead><tbody><tr><td align="left">2</td><td align="right">15</td><td align="right">0.415</td><td align="right">0.372</td><td align="right">0.337<sup>5</sup></td><td align="right">0.31<sup>6</sup></td></tr><tr><td align="left">2</td><td align="right">3</td><td align="right"><bold>0.109</bold></td><td align="right">0.194</td><td align="right">0.252<sup>5</sup></td><td align="right"><bold>0.101</bold><sup>5</sup></td></tr><tr><td align="left">3</td><td align="right">15</td><td align="right">0.512</td><td align="right">0.271</td><td align="right">0.403<sup>7</sup></td><td align="right">0.353<sup>8</sup></td></tr><tr><td align="left">3</td><td align="right">3</td><td align="right">0.36</td><td align="right">0.252</td><td align="right">0.298<sup>5</sup></td><td align="right">0.384<sup>5</sup></td></tr><tr><td align="left">4</td><td align="right">15</td><td align="right">0.554</td><td align="right">0.558</td><td align="right">0.473<sup>9</sup></td><td align="right">0.376<sup>10</sup></td></tr><tr><td align="left">4</td><td align="right">3</td><td align="right">0.426</td><td align="right">0.419</td><td align="right">0.481<sup>5</sup></td><td align="right">0.523<sup>5</sup></td></tr></tbody></table><table-wrap-foot><p><sup>1</sup>K-means. <sup>2</sup>Soft K-means. <sup>3</sup>Spectral + K-means. <sup>4</sup>Spectral + Soft K-means. <sup>5</sup><italic>&#x003b3; </italic>= 1. <sup>6</sup><italic>&#x003b3; </italic>= 2<sup>-2.5</sup>. <sup>7</sup><italic>&#x003b3; </italic>= 2<sup>-2</sup>. <sup>8</sup><italic>&#x003b3; </italic>= 2<sup>-6.5</sup>. <sup>9</sup><italic>&#x003b3; </italic>= 2<sup>-4.5</sup>. <sup>10</sup><italic>&#x003b3; </italic>= 2<sup>-4</sup></p></table-wrap-foot></table-wrap><p>The bar plots of the partitions produced using the top-3 PCs are shown and compared to the one by STRUCTURE in Figure <xref ref-type="fig" rid="F5">5</xref>. We can see that the populations in Africa are grouped into one cluster and all the other populations are grouped into the other one. This phenomenon is more evident when <italic>K </italic>= 3. As seen in Table <xref ref-type="table" rid="T5">5</xref>, the partitions produced by the generic algorithms using 3 PCs are more similar to the one produced by STRUCTURE than those produced using 70 PCs. The bar plots are shown in Figure <xref ref-type="fig" rid="F6">6</xref>. For <italic>K </italic>= 4, however, the partitions generated by the generic clustering algorithms are very different from that by STRUCTURE. Using the top-3 PCs hardly makes the distance smaller. From the plots in Figure <xref ref-type="fig" rid="F7">7</xref>, we can see that STRUCTURE infers that the genome of individuals in Pakistan is the mixture of the blue, yellow and pink clusters and the yellow one makes the most contribution. The other algorithms group the individuals in Pakistan and Europe into the same cluster.</p><fig position="float" id="F5"><label>Figure 5</label><caption><p>Bar plots of results of the distant dataset (<italic>K </italic>= 2).</p></caption><graphic xlink:href="1471-2105-10-S1-S73-5"/></fig><fig position="float" id="F6"><label>Figure 6</label><caption><p>Bar plots of results of the distant dataset (<italic>K </italic>= 3).</p></caption><graphic xlink:href="1471-2105-10-S1-S73-6"/></fig><fig position="float" id="F7"><label>Figure 7</label><caption><p>Bar plots of results of the distant dataset (<italic>K </italic>= 4).</p></caption><graphic xlink:href="1471-2105-10-S1-S73-7"/></fig><p>As for the close dataset, it can be seen in Table <xref ref-type="table" rid="T6">6</xref> that K-means and spectral clustering with soft K-means produce the most similar partitions to the one generated by STRUCTURE at <italic>K </italic>= 2 using the top-3 PCs. The bar plots for <italic>K </italic>= 2 and <italic>K </italic>= 3 using 3PCs are shown in Figure <xref ref-type="fig" rid="F8">8</xref> and <xref ref-type="fig" rid="F9">9</xref>, respectively. When <italic>K </italic>= 2, K-means groups almost all the individuals in Israel into one cluster and groups the rest into the other cluster, which is very similar to the results given by STRUCTURE. At <italic>K </italic>= 3, although K-means does not produce the most similar partition, it subdivides the individuals in Israel into two clusters, which correspond to the Druze and Bedouin populations. We can also observe a similar pattern in the bar plot produced by STRUCTURE. The individuals in the Bedouin population generally have a higher proportion of genome from the blue cluster than the individuals in the Druze population, enabling us to distinguish between the two populations.</p><fig position="float" id="F8"><label>Figure 8</label><caption><p>Bar plots of results of the close dataset (<italic>K </italic>= 2).</p></caption><graphic xlink:href="1471-2105-10-S1-S73-8"/></fig><fig position="float" id="F9"><label>Figure 9</label><caption><p>Bar plots of results of the close dataset (<italic>K </italic>= 3).</p></caption><graphic xlink:href="1471-2105-10-S1-S73-9"/></fig><p>It is difficult if not impossible to assess the correctness of the predicted number of clusters for the real datasets. We can see in Table <xref ref-type="table" rid="T3">3</xref> that, the three methods give completely different predictions on the two real datasets. STRUCTURE suggests that there are 6 clusters in the close dataset. However, the bar plot (not shown) at <italic>K </italic>= 6 is very noisy and does not reveal 6 clusters in the population. The BIC score predicts 3 clusters in the close dataset. The bar plot generated by soft K-means at <italic>K </italic>= 3 in Figure <xref ref-type="fig" rid="F9">9</xref>, however, is not convincing, since only one individual is assigned to the yellow cluster. STRUCTURE and the BIC score (with 70 PCs) suggest 6 and 3 clusters, repectively. Three clusters seem reasonable according to the bar plots in Figure <xref ref-type="fig" rid="F6">6</xref>. However, we can not observe 6 clusters in the bar plots generated by STRUCTURE at <italic>K </italic>= 6 (not shown). For both real datasets, the likelihood given by STRUCTURE increases as <italic>K </italic>increases, which is a sign of over-fitting. The gap statistic seems to suffer from the presence of noisy and non-informative PCs and either predicts no structure (<italic>K </italic>= 1) or a large <italic>K </italic>of 7, which is not supported by the bar plot (not shown).</p></sec></sec><sec><title>Conclusion</title><p>In this study, we investigated three generic clustering algorithms on genotype data. We applied PCA to genotype data in order to reduce the number of variables. Based on the TW-statistic, the significant PCs were kept for subsequent cluster analyses. A <italic>p</italic>-value of 0.05 was used in selecting significant PCs. We showed that all the generic clustering algorithms perform as well as STRUCTURE on the first three simulated datasets. Moreover, for the fourth dataset, all these algorithms produce better partitions than the one predicted by STRUCTURE. We showed that soft K-means and K-means perform comparably well to STRUCTURE on the distant and close datasets, respectively. However, all the three generic clustering algorithms show different degrees of susceptibility to noisy and non-informative PCs. Therefore, the choice of <italic>p</italic>-value remains an important issue.</p><p>We also showed that STRUCTURE and the BIC score produce identical predictions on the simulated datasets. When it comes to real datasets, STRUCTURE predicts the number of clusters to be the largest <italic>K </italic>investigated, showing a sign of over-fitting. The BIC score is, therefore, a better index in predicting the number of clusters for real datasets, which reinforces the finding by Zhu <italic>et al</italic>. [<xref ref-type="bibr" rid="B15">15</xref>]. The gap statistic performs poorly due to the presence of non-informative PCs.</p><p>While STRUCTURE is a sophisticated clustering algorithm designed for genotype data, it is very time-consuming because of the nature of MCMC. We believe that the choice of clustering algorithms depends on the purpose of population structure inference. If we want to infer recent demographic events, STRUCTURE would be a good choice since it even considers the origin of an alelle copy in the model. However, if population structure inference is used as a preprocessing step in association studies, PCA with soft K-means would be very handy. In stratified association study, we need sufficient individuals in each cluster to make significant and meaningful associations. Hence, splitting two slightly different populations and thus making each cluster smaller may not be helpful to association studies.</p><p>Based on the results of this study, we recommend choosing suitable clustering algorithms according to the nature of applications of population structure inference. In addition to the proper choice of <italic>p</italic>-value in selecting PCs, we recommend applying unsupervised feature selection algorithms, such as the one proposed by Paschou <italic>et al</italic>. [<xref ref-type="bibr" rid="B16">16</xref>], to genotype data to improve the stability and robustness of the combination of PCA and a generic clustering algorithm.</p></sec><sec><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec><sec><title>Authors' contributions</title><p>CL conceived the study, collected the real data, carried out the implementation, conducted cluster analyses with the generic clustering algorithms and drafted the manuscript. AA conducted the STRUCTURE experiments. CH guided the study and revised the manuscript. All authors read and approved the final manuscript.</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>The authors would like to thank Liming Liang for help with using software GENOME; Nick Patterson for precious discussion on their work [<xref ref-type="bibr" rid="B4">4</xref>]; Ion Mandoiu for suggesting the evaluation metric and parameters in data simulation. This study was supported by National Science Foundation through grant CCF-0755373.</p><p>This article has been published as part of <italic>BMC Bioinformatics </italic>Volume 10 Supplement 1, 2009: Proceedings of The Seventh Asia Pacific Bioinformatics Conference (APBC) 2009. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10?issue=S1"/></p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ewens</surname><given-names>WJ</given-names></name><name><surname>Spielman</surname><given-names>RS</given-names></name></person-group><article-title>The Transmission/Disequilibrium Test: History, Subdivision, and Admixture</article-title><source>American Journal of Human Genetics</source><year>1995</year><volume>57</volume><fpage>455</fpage><lpage>465</lpage><pub-id pub-id-type="pmid">7668272</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Johnstone</surname><given-names>I</given-names></name></person-group><article-title>On the distribution of the largest eigenvalue in principal components analysis</article-title><source>The Annals of Statistics</source><year>2001</year><volume>29</volume><fpage>295</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1214/aos/1009210544</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tracy</surname><given-names>C</given-names></name><name><surname>Widom</surname><given-names>H</given-names></name></person-group><article-title>Level-spacing distribution and the Airy kernel</article-title><source>Communications in Mathematical Physics</source><year>1994</year><volume>159</volume><fpage>151</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1007/BF02100489</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname><given-names>N</given-names></name><name><surname>Price</surname><given-names>AL</given-names></name><name><surname>Reich</surname><given-names>D</given-names></name></person-group><article-title>Population structure and eigenanalysis</article-title><source>PLoS Genetics</source><year>2006</year><volume>2</volume><fpage>2074</fpage><lpage>2093</lpage><pub-id pub-id-type="doi">10.1371/journal.pgen.0020190</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Cann</surname><given-names>HM</given-names></name><name><surname>de Toma</surname><given-names>C</given-names></name><name><surname>Cazes</surname><given-names>L</given-names></name><name><surname>Legrand</surname><given-names>M</given-names></name><name><surname>Morel</surname><given-names>V</given-names></name><name><surname>Piouffre</surname><given-names>L</given-names></name><name><surname>Bodmer</surname><given-names>J</given-names></name><name><surname>Bodmer</surname><given-names>WF</given-names></name><name><surname>Bonne-Tamir</surname><given-names>B</given-names></name><name><surname>Cambon-Thomsen</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Chu</surname><given-names>J</given-names></name><name><surname>Carcassi</surname><given-names>C</given-names></name><name><surname>Contu</surname><given-names>L</given-names></name><name><surname>Du</surname><given-names>R</given-names></name><name><surname>Excoffier</surname><given-names>L</given-names></name><name><surname>Friedlaender</surname><given-names>JS</given-names></name><name><surname>Groot</surname><given-names>H</given-names></name><name><surname>Gurwitz</surname><given-names>D</given-names></name><name><surname>Herrera</surname><given-names>RJ</given-names></name><name><surname>Huang</surname><given-names>X</given-names></name><name><surname>Kidd</surname><given-names>J</given-names></name><name><surname>Kidd</surname><given-names>KK</given-names></name><name><surname>Langaney</surname><given-names>A</given-names></name><name><surname>Lin</surname><given-names>AA</given-names></name><name><surname>Mehdi</surname><given-names>SQ</given-names></name><name><surname>Parham</surname><given-names>P</given-names></name><name><surname>Piazza</surname><given-names>A</given-names></name><name><surname>Pistillo</surname><given-names>MP</given-names></name><name><surname>Qian</surname><given-names>Y</given-names></name><name><surname>Shu</surname><given-names>Q</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Zhu</surname><given-names>S</given-names></name><name><surname>Weber</surname><given-names>JL</given-names></name><name><surname>Greely</surname><given-names>HT</given-names></name><name><surname>Feldman</surname><given-names>MW</given-names></name><name><surname>Thomas</surname><given-names>G</given-names></name><name><surname>Dausset</surname><given-names>J</given-names></name><name><surname>Cavalli-Sforza</surname><given-names>LL</given-names></name></person-group><article-title>A Human Genome Diversity Cell Line Panel</article-title><source>Science</source><year>2002</year><volume>296</volume><fpage>261b</fpage><lpage>262</lpage><pub-id pub-id-type="pmid">11954565</pub-id><pub-id pub-id-type="doi">10.1126/science.296.5566.261b</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>L</given-names></name><name><surname>Z&#x000f6;llner</surname><given-names>S</given-names></name><name><surname>Abecasis</surname><given-names>GR</given-names></name></person-group><article-title>GENOME: a rapid coalescent-based whole genome simulator</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><fpage>1565</fpage><lpage>1567</lpage><pub-id pub-id-type="pmid">17459963</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btm138</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hartigan</surname><given-names>JA</given-names></name><name><surname>Wong</surname><given-names>MA</given-names></name></person-group><article-title>A k-means clustering algorithm</article-title><source>Applied Statistics</source><year>1979</year><volume>28</volume><fpage>100</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.2307/2346830</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Fraley</surname><given-names>C</given-names></name><name><surname>Raftery</surname><given-names>AE</given-names></name></person-group><article-title>Enhanced software for model-based clustering, density estimation, and discriminant analysis: MCLUST</article-title><source>Journal of Classification</source><year>2003</year><volume>20</volume><fpage>263</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1007/s00357-003-0015-3</pub-id></citation></ref><ref id="B9"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Ng</surname><given-names>AY</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name><name><surname>Weiss</surname><given-names>Y</given-names></name></person-group><article-title>On Spectral Clustering: Analysis and an algorithm</article-title><source>Proceedings of NIPS 14</source><fpage>1002</fpage></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pritchard</surname><given-names>JK</given-names></name><name><surname>Stephens</surname><given-names>M</given-names></name><name><surname>Donnelly</surname><given-names>P</given-names></name></person-group><article-title>Inference of Population Structure Using Multilocus Genotype Data</article-title><source>Genetics</source><year>2000</year><volume>155</volume><fpage>945</fpage><lpage>959</lpage><pub-id pub-id-type="pmid">10835412</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Walther</surname><given-names>G</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name></person-group><article-title>Estimating the number of clusters in a data set via the gap statistic</article-title><source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source><year>2001</year><volume>63</volume><fpage>411</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1111/1467-9868.00293</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname><given-names>G</given-names></name></person-group><article-title>Estimating the dimension of a model</article-title><source>The Annals of Statistics</source><year>1978</year><volume>6</volume><fpage>461</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1214/aos/1176344136</pub-id></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Konovalov</surname><given-names>DA</given-names></name><name><surname>Litow</surname><given-names>B</given-names></name><name><surname>Bajema</surname><given-names>N</given-names></name></person-group><article-title>Partition-distance via the assignment problem</article-title><source>Bioinformatics</source><year>2005</year><volume>21</volume><fpage>2463</fpage><lpage>2468</lpage><pub-id pub-id-type="pmid">15746275</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/bti373</pub-id></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname><given-names>NA</given-names></name></person-group><article-title>Distruct: a program for the graphical display of population structure</article-title><source>Molecular Ecology Notes</source><year>2004</year><volume>4</volume><fpage>137</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1046/j.1471-8286.2003.00566.x</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Zhao</surname><given-names>H</given-names></name><name><surname>Cooper</surname><given-names>RS</given-names></name></person-group><article-title>Association mapping, using a mixture model for complex traits</article-title><source>Genetic Epidemiology</source><year>2002</year><volume>23</volume><fpage>181</fpage><lpage>196</lpage><pub-id pub-id-type="pmid">12214310</pub-id><pub-id pub-id-type="doi">10.1002/gepi.210</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Paschou</surname><given-names>P</given-names></name><name><surname>Ziv</surname><given-names>E</given-names></name><name><surname>Burchard</surname><given-names>EG</given-names></name><name><surname>Choudhry</surname><given-names>S</given-names></name><name><surname>Rodriguez-Cintron</surname><given-names>W</given-names></name><name><surname>Mahoney</surname><given-names>MW</given-names></name><name><surname>Drineas</surname><given-names>P</given-names></name></person-group><article-title>PCA-correlated SNPs for structure identification in worldwide human populations</article-title><source>PLoS Genetics</source><year>2007</year><volume>3</volume><fpage>e160</fpage><pub-id pub-id-type="doi">10.1371/journal.pgen.0030160</pub-id></citation></ref></ref-list></back></article>