<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19144132</article-id><article-id pub-id-type="pmc">2647532</article-id><article-id pub-id-type="publisher-id">1471-2105-10-18</article-id><article-id pub-id-type="doi">10.1186/1471-2105-10-18</article-id><article-categories><subj-group subj-group-type="heading"><subject>Methodology Article</subject></subj-group></article-categories><title-group><article-title>Incorporating pathway information into boosting estimation of high-dimensional risk prediction models</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Binder</surname><given-names>Harald</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><email>binderh@fdm.uni-freiburg.de</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Schumacher</surname><given-names>Martin</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>ms@imbi.uni-freiburg.de</email></contrib></contrib-group><aff id="I1"><label>1</label>Department of Medical Biometry and Statistics, University Medical Center Freiburg, Stefan-Meier-Str 26, 79104 Freiburg, Germany</aff><aff id="I2"><label>2</label>Freiburg Center for Data Analysis and Modeling, University of Freiburg, Eckerstr 1, 79104 Freiburg, Germany</aff><pub-date pub-type="collection"><year>2009</year></pub-date><pub-date pub-type="epub"><day>13</day><month>1</month><year>2009</year></pub-date><volume>10</volume><fpage>18</fpage><lpage>18</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10/18"/><history><date date-type="received"><day>11</day><month>6</month><year>2008</year></date><date date-type="accepted"><day>13</day><month>1</month><year>2009</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2009 Binder and Schumacher; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2009</copyright-year><copyright-holder>Binder and Schumacher; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>               Binder               Harald                                             binderh@fdm.uni-freiburg.de            </dc:author><dc:title>            Incorporating pathway information into boosting estimation of high-dimensional risk prediction models         </dc:title><dc:date>2009</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 10(1): 18-. (2009)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2009)10:1&#x0003c;18&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>There are several techniques for fitting risk prediction models to high-dimensional data, arising from microarrays. However, the biological knowledge about relations between genes is only rarely taken into account. One recent approach incorporates pathway information, available, e.g., from the KEGG database, by augmenting the penalty term in Lasso estimation for continuous response models.</p></sec><sec><title>Results</title><p>As an alternative, we extend componentwise likelihood-based boosting techniques for incorporating pathway information into a larger number of model classes, such as generalized linear models and the Cox proportional hazards model for time-to-event data. In contrast to Lasso-like approaches, no further assumptions for explicitly specifying the penalty structure are needed, as pathway information is incorporated by adapting the penalties for single microarray features in the course of the boosting steps. This is shown to result in improved prediction performance when the coefficients of connected genes have opposite sign. The properties of the fitted models resulting from this approach are then investigated in two application examples with microarray survival data.</p></sec><sec><title>Conclusion</title><p>The proposed approach results not only in improved prediction performance but also in structurally different model fits. Incorporating pathway information in the suggested way is therefore seen to be beneficial in several ways.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>When using microarray data for analyzing connections between gene expression and a clinical response, such as survival time, additional knowledge is often available, e.g., on pathway or ontology relations. While several proposals exist, that take the latter into account, for statistical testing, there are only few techniques that consider such meta-information for building of predictive models.</p><p>One prominent source of knowledge on genes is the KEGG database [<xref ref-type="bibr" rid="B1">1</xref>]. Several authors have demonstrated that it can be highly beneficial to consider the pathway information found there into approaches for statistical testing [<xref ref-type="bibr" rid="B2">2</xref>-<xref ref-type="bibr" rid="B4">4</xref>]. While pathways can directly provide information on relations of genes, annotation databases, such as Gene Ontology [<xref ref-type="bibr" rid="B5">5</xref>], can also be employed for testing for the association between a clinical response and groups of genes (see [<xref ref-type="bibr" rid="B6">6</xref>], for example).</p><p>When building predictive models, Gene Ontology information, or the knowledge that two microarray features belong to the same pathway, can be incorporated by approaches that allow for explicit grouping of features [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B7">7</xref>]. Alternatively, pathway signatures can be developed. For example in [<xref ref-type="bibr" rid="B8">8</xref>], pathway signatures are determined by experimental techniques, and it is shown that these are related to survival in several independent cancer data sets.</p><p>However, simple grouping of features discards information on specific relations between genes within a pathway. A recent approach [<xref ref-type="bibr" rid="B9">9</xref>] not only uses the information that two genes are in the same pathway, but allows to incorporate information on specific gene relations. This is implemented by augmenting the log-likelihood criterion, to be maximized for estimating the parameters of a predictive model, by a penalty term that explicitly takes differences between the coefficients of linked genes into account.</p><p>As a basis for the approach in [<xref ref-type="bibr" rid="B9">9</xref>], the Lasso [<xref ref-type="bibr" rid="B10">10</xref>] is used, which provides for sparse estimates, i.e., predictive models where only few microarray features have non-zero influence. Similar to the fused Lasso [<xref ref-type="bibr" rid="B11">11</xref>], an additional term is added to the Lasso penalty. While there are techniques for fitting models to various response types when employing the original Lasso penalty [<xref ref-type="bibr" rid="B12">12</xref>], often only continuous response techniques are available for approaches which extend the Lasso penalty. Also, only an algorithm for estimation with a continuous response is provided for the approach in [<xref ref-type="bibr" rid="B9">9</xref>]. However, mainly binary and time-to-event responses are of interest for predictive microarray models.</p><p>Another problem with extensions of the Lasso approach is that several assumptions have to be made when choosing the structure of the penalty term. For example, the criterion employed in [<xref ref-type="bibr" rid="B9">9</xref>] penalizes the squared difference between (standardized) parameter estimates, which might be problematic when the true parameters have opposite sign. This is, e.g., the case when in a pair of connected genes one is up-regulated and the other one is down-regulated for patients with increased risk.</p><p>Boosting is an alternative technique for fitting high-dimensional predictive models (see, e.g., [<xref ref-type="bibr" rid="B13">13</xref>] for an overview). It uses a stepwise approach that allows to build up an overall model from many simple fits, refining the overall fit in every boosting step. When only the parameter estimate for one covariate is updated in each boosting step, componentwise boosting is obtained, resulting in sparse fits similar to the Lasso [<xref ref-type="bibr" rid="B14">14</xref>]. In addition, likelihood-based componentwise boosting allows for adequate consideration of clinical covariates in predictive microarray models [<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B16">16</xref>]. The latter approach is available for all response types where estimation can be performed by Newton-Raphson steps for maximization of a likelihood, which are then adapted for penalized estimation in every boosting step.</p><p>For incorporating pathway information into boosting algorithms, one approach is to dedicate each single boosting step to the genes in one specific pathway [<xref ref-type="bibr" rid="B2">2</xref>]. However, just like grouping Lasso approaches, this does not take into account specific relations between genes.</p><p>As an alternative, we are going to adapt the componentwise likelihood-based boosting approach [<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B16">16</xref>] for specifically incorporating pathway knowledge about gene relations into estimation of predictive models from gene expression data. The proposed <italic>PathBoost </italic>approach can be used for various response types, including binary and time-to-event responses. As pathway information is incorporated by adapting penalty parameters of connected genes in the course of the boosting steps, the approach also does not require an explicit specification of a penalty structure.</p><p>After outlining the details of the PathBoost algorithm in the following, it will be evaluated in a small simulation study, where it will be compared to the approach given in [<xref ref-type="bibr" rid="B9">9</xref>]. Its advantages on terms of prediction performance and interpretability are furthermore illustrated in two application examples with microarray survival data.</p></sec><sec><title>Results and discussion</title><sec><title>The PathBoost algorithm</title><p>There are different response types for predictive models built from microarray data, the two most prominent being binary responses, employed, e.g., when classification of tumors is wanted, and time-to-event responses when prediction of survival is wanted. Our proposal for incorporating pathway information is based on likelihood-based boosting [<xref ref-type="bibr" rid="B15">15</xref>-<xref ref-type="bibr" rid="B17">17</xref>]. It is therefore suitable for all settings where parameter estimation can be performed by maximization of a likelihood via Newton-Raphson steps. For generalized linear models, the response, which might be continuous, binary or a counting response, is taken to be from an exponential family. Given observations (<italic>y</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>i</italic></sub>), <italic>i </italic>= 1,..., <italic>n</italic>, with response <italic>y</italic><sub><italic>i </italic></sub>and covariate vector <italic>x</italic><sub><italic>i </italic></sub>= (<italic>x</italic><sub><italic>i</italic>1</sub>,..., <italic>x</italic><sub><italic>ip</italic></sub>)', the structural part of such models is</p><p><disp-formula><italic>E</italic>(<italic>y</italic><sub><italic>i</italic></sub>|<italic>x</italic><sub><italic>i</italic></sub>) = <italic>h</italic>(<italic>&#x003b7;</italic><sub><italic>i</italic></sub>),</disp-formula></p><p>where <italic>h </italic>is a known link function and <italic>&#x003b7;</italic><sub><italic>i </italic></sub>is the linear predictor</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" name="1471-2105-10-18-i1" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>with intercept parameter <italic>&#x003b2;</italic><sub><italic>inter </italic></sub>and parameter vector <italic>&#x003b2; </italic>= (<italic>&#x003b2;</italic><sub>1</sub>,..., <italic>&#x003b2;</italic><sub><italic>p</italic></sub>)', which are estimated by maximization of the log-likelihood <italic>l</italic>(<italic>&#x003b2;</italic>) (see e.g. [<xref ref-type="bibr" rid="B18">18</xref>] for more details).</p><p>In a time-to-event setting, observations (<italic>t</italic><sub><italic>i</italic></sub>, <italic>&#x003b4;</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>i</italic></sub>), <italic>i </italic>= 1,..., <italic>n</italic>, typically comprise of an observed time <italic>t</italic><sub><italic>i</italic></sub>, a censoring indicator <italic>&#x003b4;</italic><sub><italic>i</italic></sub>, that takes value 1 if the observed time is the time of the event of interest and value 0 if it is the time of censoring, and a covariate vector <italic>x</italic><sub><italic>i</italic></sub>. Due to censoring, direct modeling of <italic>t</italic><sub><italic>i </italic></sub>as a continuous response is problematic. Models for the hazard <italic>&#x003bb;</italic>(<italic>t</italic>|<italic>x</italic><sub><italic>i</italic></sub>), i.e., the instantaneous risk of having an event at time <italic>t</italic>, given the covariate information, are preferred.</p><p>The Cox proportional hazards model has the form</p><p><disp-formula><italic>&#x003bb;</italic>(<italic>t</italic>|<italic>x</italic><sub><italic>i</italic></sub>) = <italic>&#x003bb;</italic><sub>0</sub>(<italic>t</italic>) exp(<italic>&#x003b7;</italic><sub><italic>i</italic></sub>),</disp-formula></p><p>where <italic>&#x003bb;</italic><sub>0</sub>(<italic>t</italic>) is an unspecified baseline hazards and <italic>&#x003b7;</italic><sub><italic>i </italic></sub>is a linear predictor of the form</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M2" name="1471-2105-10-18-i2" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>with parameter vector <italic>&#x003b2;</italic>. Estimation of <italic>&#x003b2; </italic>is performed by maximizing the partial log-likelihood</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M3" name="1471-2105-10-18-i3" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>log</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>exp</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <italic>I</italic>( ) is an indicator function that takes value 1 if its argument is true and value 0 otherwise, avoiding estimation of the baseline hazard.</p><sec><title>Componentwise likelihood-based boosting</title><p>The basic idea of boosting is to fit several models to the data in a stepwise manner. In each boosting step, a new model is fitted, which gives larger weight to those observations that were fitted poorly in the previous boosting steps [<xref ref-type="bibr" rid="B19">19</xref>]. All individual fits are then combined into one overall model. It has been recognized that this procedure is in specific settings equivalent to gradient descent in function space [<xref ref-type="bibr" rid="B20">20</xref>], which in turn is equivalent to repeated fitting of residuals for the continuous response case with squared error loss function [<xref ref-type="bibr" rid="B14">14</xref>].</p><p>In [<xref ref-type="bibr" rid="B15">15</xref>], the latter idea is extended to generalized linear models by incorporating the previous boosting steps as an offset into the linear predictor <italic>&#x003b7;</italic><sub><italic>i</italic></sub>. In [<xref ref-type="bibr" rid="B16">16</xref>], a similar approach for boosting estimation of the Cox proportional hazards model is suggested. The basic likelihood-based boosting algorithm is given in the following for both types of models.</p><p>Starting with parameter estimate <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M4" name="1471-2105-10-18-i4" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b2;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> = (0,...,0), in each of <italic>k</italic>, <italic>k </italic>= 1,..., <italic>M</italic>, boosting steps, for each covariate <italic>x</italic><sub><italic>ij</italic></sub>, <italic>j </italic>= 1,..., <italic>p</italic>, candidate models with linear predictor</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M5" name="1471-2105-10-18-i5" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#x003b7;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>are fitted by estimating parameters <italic>&#x003b3;</italic><sub><italic>j</italic>, <italic>k</italic></sub>. The offset <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M6" name="1471-2105-10-18-i6" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b7;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> incorporates the information from the previous boosting steps, i.e.,</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M7" name="1471-2105-10-18-i7" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b7;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>&#x003b2;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>for the Cox model and</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M8" name="1471-2105-10-18-i8" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b7;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#x003b2;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>&#x003b2;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>for generalized linear models. The intercept parameter <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M9" name="1471-2105-10-18-i9" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b2;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is updated before each boosting step by fitting an intercept-only model.</p><p>For estimation of the <italic>&#x003b3;</italic><sub><italic>j</italic>, <italic>k</italic></sub><italic>s</italic>, a penalized log-likelihood criterion</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M10" name="1471-2105-10-18-i10" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>&#x003b3;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>is employed, where <italic>&#x003bb;</italic><sub><italic>j</italic>, <italic>k </italic></sub>is a penalty parameter that determines the size of the boosting steps. Typically, the same value of <italic>&#x003bb;</italic><sub><italic>j</italic>, <italic>k </italic></sub>= <italic>&#x003bb; </italic>is employed for all covariates and all boosting steps. As the number of boosting steps <italic>M</italic>, which can, e.g., be determined by cross-validation, is the more important tuning parameter, the penalty parameter <italic>&#x003bb; </italic>is chosen only very coarsely, such that the resulting number of boosting steps is not too small (say larger than 50).</p><p>Using score function <italic>U</italic>(<italic>&#x003b3;</italic>) = &#x02202;<italic>l</italic>(<italic>&#x003b3;</italic>)/&#x02202;<italic>&#x003b3; </italic>and information matrix <italic>I</italic>(<italic>&#x003b3;</italic>) = -&#x02202;<sup>2</sup><italic>l</italic>(<italic>&#x003b3;</italic>)/&#x02202;<sup>2</sup><italic>&#x003b3;</italic>, more specifically the scalar values <italic>U</italic><sub><italic>j</italic>, <italic>k </italic></sub>= <italic>U</italic>(0) and <italic>I</italic><sub><italic>j</italic>, <italic>k </italic></sub>= <italic>I</italic>(0), we employ Newton-Raphson steps, resulting in estimates</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M11" name="1471-2105-10-18-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>This is based on only one Newton-Raphson step, as further refinements can potentially be performed in later boosting steps.</p><p>The estimate <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M12" name="1471-2105-10-18-i12" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> for the covariate with index <italic>j</italic>* which improves the fit the most (in terms of log-likelihood for generalized linear models or according to the penalized score statistic <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M13" name="1471-2105-10-18-i13" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula>/(<italic>I</italic><sub><italic>j</italic>, <italic>k </italic></sub>+ <italic>&#x003bb;</italic><sub><italic>j</italic>, <italic>k</italic></sub>) for the Cox model) is then used to update the elements of the overall parameter vector via</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M14" name="1471-2105-10-18-i14" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b2;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b2;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>for&#x000a0;</mml:mtext><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b2;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>This componentwise boosting approach results in sparse fits, i.e., where many elements of the estimated parameter vector are equal to zero.</p><p>One of the advantages of likelihood-based boosting is that it is very easy to incorporate mandatory, unpenalized covariates (see [<xref ref-type="bibr" rid="B16">16</xref>], for example). This is useful when clinical covariates have to be incorporated in addition to microarray features, in order to compare the resulting model fit to a purely clinical model. The clinical covariates are then added to the linear predictor <italic>&#x003b7;</italic><sub><italic>i</italic></sub>, and their coefficients are updated in or after every boosting step, but they do not enter into the penalty term.</p></sec><sec><title>Incorporating pathway information</title><p>The sparseness of the fits, resulting from approaches such as the Lasso or componentwise boosting, is a desirable property in settings with many microarray features, as it potentially results in a short list of genes, that are deemed influential. It can, however, also have a negative effect on interpretability. For example, if the level of activity of (parts of) a specific pathway is related to the response, the microarray features associated with that pathway will be highly correlated and have similar predictive power. However, sparse fitting techniques will probably pick out only one of the features. This makes it difficult to identify the underlying pathway. Also, model fits might be less stable when relying only on one measurement instead of several features.</p><p>For discouraging selection of only single microarray features associated with a pathway, we suggest to increase the penalty <italic>&#x003bb;</italic><sub><italic>j</italic>*, <italic>l</italic></sub>, <italic>l </italic>&#x0003e; <italic>k</italic>, used for a specific covariate <italic>x</italic><sub><italic>ij</italic>*</sub>, after it has been selected in boosting step <italic>k</italic>. This decreases the size of the boosting steps for this covariate and makes it less likely that this covariate will be selected in future boosting steps. In turn, the penalties for the microarray features that belong to genes that are directly connected in the respective pathway are decreased, making it more likely that they will be selected in future steps.</p><p>This approach requires specification of two rules, one for increasing the penalty of a selected covariate and one for decreasing the penalties for connected covariates. In the following, we provide such rules for penalty updates, which, in combination with componentwise likelihood-based boosting, constitute the <italic>PathBoost </italic>algorithm.</p><sec><title>Increasing the penalty for a selected covariate</title><p>In order to provide a rule for penalty updates, a common metric for all covariates is needed. Therefore, we quantify the size of the boosting step <italic>k</italic>, performed for a covariate with index <italic>j</italic>* that has been selected in this step, by considering the estimate <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M15" name="1471-2105-10-18-i12" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> relative to the estimate</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M16" name="1471-2105-10-18-i15" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>obtained from unpenalized estimation, i.e., for <italic>&#x003bb;</italic><sub><italic>j</italic>*, <italic>k </italic></sub>= 0. The step-size factor <italic>&#x003bd;</italic><sub><italic>j</italic>*, <italic>k </italic></sub>then is given by</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M17" name="1471-2105-10-18-i16" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>For incorporating pathway information, we suggest to decrease the step-size factor for a selected covariate by a constant step-size modification factor 0 &#x0003c;<italic>c</italic><sub><italic>smf </italic></sub>&#x02264; 1. So, after the covariate with index <italic>j</italic>* has been selected in boosting step <italic>k</italic>, the new step-size factor for further boosting steps <italic>l </italic>&#x0003e; <italic>k </italic>becomes</p><p><disp-formula><italic>&#x003bd;</italic><sub><italic>j</italic>*, <italic>l </italic></sub>= <italic>c</italic><sub><italic>smf </italic></sub>&#x000b7; <italic>&#x003bd;</italic><sub><italic>j</italic>*, <italic>k</italic></sub>,</disp-formula></p><p>implying a penalty increase via</p><p><disp-formula id="bmcM1"><label>(1)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M18" name="1471-2105-10-18-i17" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>For computational simplicity, we will use a fixed value of <italic>I</italic><sub><italic>j</italic>*, <italic>k</italic>+1 </sub>instead of the flexible term <italic>I</italic><sub><italic>j</italic>*, <italic>l </italic></sub>in this penalty update rule. This means that the new penalty for a covariate can be calculated immediately after it has been selected in a boosting step and that the penalty stays the same until the covariate, or a covariate that is connected to it, is selected again.</p></sec><sec><title>Decreasing the penalty for connected covariates</title><p>If the penalty for a covariate <italic>x</italic><sub><italic>ij</italic>* </sub>is increased, and it is then selected again in a later boosting step, the explained variability due to this covariate and the pathways it belongs to will be decreased. To maintain the amount of variability explained by a pathway, the loss in explained variability for covariate <italic>x</italic><sub><italic>ij</italic>* </sub>is distributed to related covariates, e.g., to covariates that are connected to covariate <italic>x</italic><sub><italic>ij</italic>* </sub>in the pathway. The amount of potentially lost explained variability, that is to be distributed after a boosting step therefore has to be quantified. A proposal for this is provided in the following.</p><p>If <italic>k </italic>is the first boosting step where covariate <italic>x</italic><sub><italic>ij</italic>* </sub>is selected, then the unpenalized estimate <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M19" name="1471-2105-10-18-i18" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> (obtained with <italic>&#x003bb;</italic><sub><italic>j</italic>*, <italic>k </italic></sub>= 0) will be approximately equal to the (unpenalized) maximum likelihood estimate <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M20" name="1471-2105-10-18-i19" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> obtained from standard non-boosting estimation. As the relative step size, not realized due to penalized estimation, in boosting step <italic>k </italic>is given by 1 - <italic>&#x003bd;</italic><sub><italic>j</italic>*, <italic>k</italic></sub>, for boosting step <italic>k </italic>+ 1, the unpenalized estimate <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M21" name="1471-2105-10-18-i20" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> will be approximately equal to</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M22" name="1471-2105-10-18-i21" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02248;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>Thus, the penalized estimate <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M23" name="1471-2105-10-18-i22" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> will be</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M24" name="1471-2105-10-18-i23" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02248;</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>&#x003b3;</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>The approximate fraction <italic>&#x003c0;</italic><sub><italic>j</italic>,(<italic>m</italic>) </sub>of the maximum likelihood estimate that has been realized for covariate <italic>x</italic><sub><italic>ij </italic></sub>in the <italic>m</italic><sub><italic>th </italic></sub>boosting step, where this covariate has been selected, then is</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M25" name="1471-2105-10-18-i24" overflow="scroll"><mml:semantics><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow/></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>Let now <italic>j</italic><sub>1 </sub>be the index of a covariate that has been selected in boosting step <italic>k </italic>and <italic>j</italic><sub>2 </sub>be the index of the covariate to which a potential loss in explained variability is to be transferred. There is a potential loss that is incurred for <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M26" name="1471-2105-10-18-i25" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> in a future boosting step <italic>l </italic>by employing a penalty that is updated via (1), with corresponding step-size factor <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M27" name="1471-2105-10-18-i26" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, instead of not modifying the penalty, i.e., keeping the step-size factor <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M28" name="1471-2105-10-18-i27" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>. In terms of the fraction of the maximum likelihood estimate this loss is given by</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M29" name="1471-2105-10-18-i28" overflow="scroll"><mml:semantics><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>The aim is now to choose the penalty <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M30" name="1471-2105-10-18-i29" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, or correspondingly the step-size factor <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M31" name="1471-2105-10-18-i30" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, for the covariate with index <italic>j</italic><sub>2 </sub>for a future boosting step <italic>l </italic>(compared to step <italic>k</italic>), such that the loss for covariate <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M32" name="1471-2105-10-18-i25" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is compensated by covariate <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M33" name="1471-2105-10-18-i31" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>. Equating</p><p><disp-formula id="bmcM2"><label>(2)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M34" name="1471-2105-10-18-i32" overflow="scroll"><mml:semantics><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>results in an update for the step-size factor</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M35" name="1471-2105-10-18-i33" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bd;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>This implies a decrease of the penalty parameter <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M36" name="1471-2105-10-18-i34" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> via</p><p><disp-formula id="bmcM3"><label>(3)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M37" name="1471-2105-10-18-i35" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>Again, for computational simplicity, we use a fixed value of <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M38" name="1471-2105-10-18-i36" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> instead of <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M39" name="1471-2105-10-18-i37" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, and a value of <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M40" name="1471-2105-10-18-i38" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> instead of <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M41" name="1471-2105-10-18-i39" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> in this update rule. Therefore, the new penalties of connected covariates can be calculated immediately after the boosting step, avoiding recalculation after every boosting step and storage of results from past boosting steps.</p><p>As an increase of the penalty via (1) would leave the potential loss in explained variability undistributed for a covariate without connections, the penalty update is only performed for covariates that correspond to genes that have a connection to another gene, with corresponding covariate, in a pathway. For connected genes, however, the question remains whether the total amount should be transferred to every connected covariate or whether the right-hand side of (2) should be divided by the number of connections. As componentwise boosting results in very sparse fits, it can be expected that only few connected covariates will be selected in the remaining boosting steps. It therefore seems to be reasonable to assign the amount to each connected covariate.</p><p>While a measure of uncertainty is not available for connections in a pathway in the KEGG pathway database, it might be available from other sources. Such information could easily be incorporated into the PathBoost algorithm by multiplying the right-hand side of (2) by the measure of uncertainty (given that the latter has values between 0 an 1). Also, information on the direction of relations could be incorporated by propagating changes of a penalty only into one direction.</p></sec></sec><sec><title>Choice of tuning parameters</title><p>The proposed PathBoost algorithm has three flexible parameters: an initial penalty <italic>&#x003bb;</italic><sub><italic>j</italic>,1 </sub>= <italic>&#x003bb;</italic>, <italic>j </italic>= 1,..., <italic>p</italic>, common to all covariates, the number of boosting steps <italic>M</italic>, and the step-size modification factor <italic>c</italic><sub><italic>smf</italic></sub>. The initial penalty parameter is of minor importance and can be chosen very coarsely. A value that roughly corresponds to initial step-size factors of about 0.01 works very well in our experience. For determining the step-size modification factor <italic>c</italic><sub><italic>smf</italic></sub>, a coarse line search is performed. For each value of <italic>c</italic><sub><italic>smf</italic></sub>, the optimal number of boosting steps is determined by 10-fold cross-validation. Then the value of c<sub><italic>smf</italic></sub>, which results in the overall maximum of cross-validated (partial) log-likelihood, is chosen.</p></sec></sec><sec><title>Simulation study</title><p>To evaluate the performance of the PathBoost approach, we perform a small simulation study that is identical, in terms of design, to the study employed in [<xref ref-type="bibr" rid="B9">9</xref>]. Models for a continuous response are built from p = 2200 covariates. Of these, 200 take the role of transcription factors. The remaining 2000 covariates comprise of blocks of 10 covariates, where the covariates in each block are correlated with one specific transcription factor. The connection information, required for the approach given in [<xref ref-type="bibr" rid="B9">9</xref>] and for the PathBoost approach, is chosen such that there is a bidirectional connection between each transcription factor and each of the 10 covariates associated with it.</p><p>The true parameter vector in the generating linear model is chosen such that only four transcription factors (and the corresponding blocks of correlated covariates) have an effect on the response. There are six types of generating models with varying size and type of effect.</p><p>In Model 1, the true parameters of the covariates that are related to a transcription factor have the same sign as the parameter of the transcription factor itself. This is expected to be favorable for the approach given in [<xref ref-type="bibr" rid="B9">9</xref>], as the penalty term employed there penalizes the squared (standardized) differences of parameters. However, for true parameters with opposite sign, this difference will be large, making it rather unlikely that the true values are recovered. Model 2 features such a setting, where in each block of 10 informative covariates, the parameters of three covariates have a sign opposite to that of the associated transcription factor. In [<xref ref-type="bibr" rid="B9">9</xref>] it was found that this considerably affected the performance of the approach with an explicit penalty structure. In contrast, we do not expect a performance degradation for the PathBoost approach as it does not rely on differences of parameters.</p><p>Model 3 is similar to Model 1, and Model 4 is similar to Model 2, the only difference being a smaller effect of the covariates. Extending the design given in [<xref ref-type="bibr" rid="B9">9</xref>], we added two further settings, Model 5 and Model 6, which are based on Model 2 and Model 4 respectively. In these settings, only the first and the third block of informative covariates contain effects with opposite sign. Therefore, only six of a total of 40 informative connected covariates have an effect with a sign opposite to the associated transcription factor.</p><p>As a minimal performance reference, an intercept-only model, i.e., a model that does not use any covariate information, is fitted. A more specific performance reference for the PathBoost approach is provided by componentwise likelihood-based boosting without pathway information [<xref ref-type="bibr" rid="B15">15</xref>]. The main tuning parameter there is the number of boosting steps, which is determined by 10-fold cross-validation. As already suggested, the additional parameter <italic>c</italic><sub><italic>smf </italic></sub>for the PathBoost approach is determined by a coarse line search.</p><p>As a performance reference for the approach given in [<xref ref-type="bibr" rid="B9">9</xref>], models are fitted by the Lasso [<xref ref-type="bibr" rid="B10">10</xref>], which also penalizes the absolute values of the parameters, but does not incorporate pathway information. For both approaches, fitting is performed by the least angle regression technique [<xref ref-type="bibr" rid="B21">21</xref>], which allows for fast computation of solutions for a large range of values for the penalty parameter that governs the absolute value term in the penalty. For the Lasso, only the latter has to be chosen, which is done by 10-fold cross-validation. For the approach given in [<xref ref-type="bibr" rid="B9">9</xref>], a second penalty parameter is required, which, similar to the PathBoost approach, is determined by a coarse line search.</p><p>All approaches are fitted to training sets of size <italic>n </italic>= 100, and prediction performance is evaluated on a test set of the same size. This is repeated 50 times. Table <xref ref-type="table" rid="T1">1</xref> shows the corresponding mean values and standard errors of the predictive mean squared error.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Results of the simulation study.</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">Model</td><td align="left">intercept</td><td align="left">Lasso</td><td align="left">Li&#x00026;Li</td><td align="left">lik.boost</td><td align="left">PathBoost</td></tr></thead><tbody><tr><td align="center">1</td><td align="left">762.5 (14.4)</td><td align="left">83.6 (2.6)</td><td align="left">42.5 (1.1)</td><td align="left">83.4 (2.4)</td><td align="left">61.0 (1.7)</td></tr><tr><td align="center">2</td><td align="left">305.8 (5.1)</td><td align="left">91.0 (2.7)</td><td align="left">80.8 (1.9)</td><td align="left">89.7 (2.7)</td><td align="left">64.8 (1.8)</td></tr><tr><td align="center">3</td><td align="left">215.6 (4.1)</td><td align="left">32.6 (0.9)</td><td align="left">24.9 (0.8)</td><td align="left">32.1 (0.9)</td><td align="left">26.5 (0.7)</td></tr><tr><td align="center">4</td><td align="left">131.1 (2.4)</td><td align="left">32.6 (0.9)</td><td align="left">29.9 (0.7)</td><td align="left">32.5 (0.9)</td><td align="left">26.9 (0.7)</td></tr><tr><td align="center">5</td><td align="left">525.7 (9.9)</td><td align="left">87.9 (2.6)</td><td align="left">61.6 (1.5)</td><td align="left">85.6 (2.2)</td><td align="left">62.2 (1.6)</td></tr><tr><td align="center">6</td><td align="left">171.6 (3.3)</td><td align="left">32.9 (0.9)</td><td align="left">27.6 (0.7)</td><td align="left">32.2 (0.9)</td><td align="left">26.9 (0.8)</td></tr></tbody></table><table-wrap-foot><p>Predictive mean squared error, mean and standard errors (in parentheses), for an intercept-only model, the Lasso, the pathway-based procedure proposed in [<xref ref-type="bibr" rid="B9">9</xref>] (Li&#x00026;Li), componentwise likelihood-based boosting (lik.boost), and boosting with pathway information (PathBoost) for six types of generating models.</p></table-wrap-foot></table-wrap><p>The predictive mean squared error for all approaches is far below that of the intercept-only model, indicating that the prediction problems are very simple. As would be expected, the performance of the Lasso and componentwise boosting is very similar. So, there is no disadvantage of choosing one of the two as a basis for an approach that incorporates pathway information.</p><p>The approach given in [<xref ref-type="bibr" rid="B9">9</xref>] outperforms the Lasso in all six settings. However, the performance difference is greatly diminished with Models 2 and 4, where several of the parameters of connected covariates have opposite sign. This highlights the difficulties potentially arising from an explicitly specified penalty structure. In contrast, the PathBoost approach is seen to result in a consistent improvement over boosting without pathway information in all settings. As would be expected from the design of the algorithm, the sign of the true parameters does not matter.</p><p>Comparing the PathBoost approach to that given in [<xref ref-type="bibr" rid="B9">9</xref>], the latter shows better prediction performance for Models 1 and 2, i.e., where its penalty structure matches the sign of the true parameters. However, for Models 3 and 4, where the sign of parameters of connected covariates may be different, the approach given in [<xref ref-type="bibr" rid="B9">9</xref>] performs worse. The performance of the two approaches is similar for Models 5 and 6, implying that already a small mismatch in sign information can nullify potential performance advantages gained by explicitly specifying the penalty structure in the approach given in [<xref ref-type="bibr" rid="B9">9</xref>].</p></sec><sec><title>Application examples</title><p>In the following, we investigate the properties of the PathBoost approach in two application examples with microarray survival data, where a Cox proportional hazards model is fitted. When applying a technique for fitting predictive models that incorporates pathway information in a real application setting, there are two objectives. The first is to get better interpretability of the model fit, but the interpretation of a fit will only be credible if the second objective, that of improved prediction performance, is met. For adequately evaluating a potential gain in prediction performance from incorporating pathway information in a time-to-event setting, we employ bootstrap .632+ prediction error curve estimates [<xref ref-type="bibr" rid="B22">22</xref>-<xref ref-type="bibr" rid="B24">24</xref>].</p><p>Pathway information is extracted from the KEGG pathway database [<xref ref-type="bibr" rid="B1">1</xref>]. Similar to [<xref ref-type="bibr" rid="B9">9</xref>], we restrict analyses to regulatory pathways, but also include cancer pathways. As a restriction to gene-gene relations would have resulted in a very small number of connections, any genes that are linked by some kind of KEGG relation are considered to be connected.</p><p>While the glioblastoma data analyzed in [<xref ref-type="bibr" rid="B9">9</xref>] has a time-to-event response, closer inspection showed that the genes which have predictive power are not represented in KEGG pathways. Therefore, an approach focussed on the latter cannot improve over a null model that does not use any microarray information [<xref ref-type="bibr" rid="B25">25</xref>]. We investigate two other data sets, one from patients with large B-cell lymphoma [<xref ref-type="bibr" rid="B26">26</xref>] and a second from patients with ovarian cancer [<xref ref-type="bibr" rid="B8">8</xref>].</p><sec><title>Diffuse large B-cell lymphoma</title><p>The data from patients with diffuse large B-cell lymphoma (DLBCL) has already been used for illustrating prediction error curve techniques [<xref ref-type="bibr" rid="B23">23</xref>] and the likelihood-based boosting technique for the Cox proportional hazards model [<xref ref-type="bibr" rid="B16">16</xref>], on which the PathBoost approach is based. Details of preprocessing are described there. There are <italic>n </italic>= 240 observation with <italic>p </italic>= 7399 microarray features. Only 1281 of the latter could be related to KEGG pathways, based on the information available. To avoid restriction to a (relatively) small number of microarray features and to maintain comparability to previous analyses, also the features not represented in KEGG pathways are considered.</p><p>A coarse line search, in combination with 10-fold cross validation, results in selection of a step-size modification factor of <italic>c</italic><sub><italic>smf </italic></sub>= 0.9, which indicates that there might be some predictive pathway information in the data. Use of this factor results in 47 non-zero coefficients. In comparison, application of boosting without pathway information results in only 27 non-zero coefficients. There is an overlap of 20 non-zero coefficients, indicating that seven microarray features are no longer deemed important when pathway information is included, with 27 new features being added to the model.</p><p>For checking whether the 27 added features just contain information similar to the seven features not found in the PathBoost fit, we applied componentwise boosting to a data set where the seven features were removed. If the 27 seven features would be a substitute for the seven removed features, some of the former should now be included. However, the resulting model has 20 non-zero coefficients, which all belong to the same covariates as the overlapping coefficients above, i.e., none of the 27 microarray features, identified by PathBoost, are included in the model. The prediction performance decreases (not shown), indicating that the seven microarray features contain information which is useful in combination with componentwise boosting. However, as PathBoost does not utilize these seven microarray features and nevertheless performs better, this underlines that PathBoost results in structurally different model fits.</p><p>While in the model, fitted by boosting without pathway information, only two connected microarray features receive non-zero coefficient estimates, PathBoost results in 12 connected microarray features that receive non-zero estimates. This indicates that the fit from the latter algorithm reflects pathway knowledge. The coefficients of connected microarray features have different sign in several instances. As such a constellation did not influence the performance of PathBoost in the simulation study, an impact is also not expected in this application example.</p><p>The change in structure of the fitted models is also seen from the coefficient paths, i.e., the parameter estimates plotted against the boosting steps. Figure <xref ref-type="fig" rid="F1">1</xref> shows the coefficient paths for boosting without pathway information (left panel) and PathBoost (right panel). While they are rather similar, there are some features with strong effect that appear only in the PathBoost fit (e.g., UNIQIDs 29911 and 27573). As the PathBoost algorithm increases the penalty for a covariate after it has been selected, it could be expected that the estimates are somewhat shrunken compared to the CoxBoost fit. This is seen, e.g., for the microarray features with UNIQIDs 32238 and 32679, which are no longer selected by PathBoost after a certain boosting step, as the penalty for them has become too large. This is different from approaches that use an explicit shrinkage term in the penalized (partial) log-likelihood criterion, as there it would be expected that the whole path is shrunken.</p><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold>Coefficient paths for the DLBCL data</bold>. Coefficient paths for boosting without pathway information (left panel) and PathBoost (right panel), applied to DLBCL data. The models selected by 10-fold cross validation are indicated by vertical lines. Microarray features common to both models are indicated by solid curves, the others by dotted curves.</p></caption><graphic xlink:href="1471-2105-10-18-1"/></fig><p>While use of pathway information is seen to have influenced the model fit, interpretation of the latter can only be assumed to be more valid, compared to the fit obtained without pathway information, if prediction performance is also improved. The thick curves in Figure <xref ref-type="fig" rid="F2">2</xref> indicate .632+ prediction error curve estimates (based on 100 bootstrap samples of size 0.632<italic>n</italic>, drawn without replacement). The Kaplan-Meier benchmark (grey curve) that does not use any covariate information is given as a reference. All procedures are seen to improve over the Kaplan-Meier benchmark, where PathBoost (solid curve) seems to have a slight advantage over boosting without pathway information (dashed curve). While the difference is not very large, it nevertheless improves the credibility of the PathBoost fit.</p><fig position="float" id="F2"><label>Figure 2</label><caption><p><bold>Prediction error curves for the DLBCL data</bold>. Bootstrap .632+ prediction error curve estimates for boosting without pathway information (dashed curves) and PathBoost (solid curves), applied to DLBCL data, without (thick curves) and with clinical covariates (thin curves). The Kaplan-Meier benchmark (grey curve) and a purely clinical model (dotted curve) are given as a reference.</p></caption><graphic xlink:href="1471-2105-10-18-2"/></fig><p>For 222 patients, a clinical predictor, the International Prognostic Index (IPI), is available. As it is typically of interest how much microarray information can improve over purely clinical models, we include the clinical covariate as a mandatory, unpenalized covariate, as described in [<xref ref-type="bibr" rid="B16">16</xref>]. The corresponding prediction error curve estimates are indicated by thin curves in Figure <xref ref-type="fig" rid="F2">2</xref>. The prediction performance of a purely clinical model is indicated by the dotted curve. It is seen that the combined models can improve over the purely clinical model. However, PathBoost (solid curve) can no longer improve over boosting without pathway information (dashed curve). The lack of additional value of pathway information in this setting is also reflected by the step-size modification factor, chosen by a line search, which is <italic>c</italic><sub><italic>smf </italic></sub>= 1. Therefore it seems that, in the present example, pathway information is most useful in describing phenomena that are already reflected by the clinical covariate.</p></sec><sec><title>Ovarian cancer</title><p>The second data set, to be used for illustration of the PathBoost approach, is from patients with ovarian cancer. The original analysis of this data [<xref ref-type="bibr" rid="B8">8</xref>] already showed that there is a connection between pathway activity and survival, where pathway signatures were derived from prior experiments. In contrast, we will investigate whether pathway knowledge derived from the KEGG database can also add to prediction of survival.</p><p>For the 133 patients, where time-to-event information is available, we performed preprocessing of the microarray data, using the RMA approach [<xref ref-type="bibr" rid="B27">27</xref>], resulting in 21801 microarray features. We restrict analysis to those 4868 features that are related to any of the human KEGG pathways.</p><p>The connections between genes, just as for the DLBCL data, are extracted from the regulatory KEGG pathways, including the cancer pathways. The step-size modification factor, selected by a line search in combination with 10-fold cross-validation, then is <italic>c</italic><sub><italic>smf </italic></sub>= 1, i.e., pathway information would not be expected to be useful for prediction of survival. However, when only the connections from the cancer pathways are considered, the resulting factor is <italic>c</italic><sub><italic>smf </italic></sub>= 0.9. This indicates that targeted pathway information might be useful, while use of too many pathways is detrimental to prediction performance. Figure <xref ref-type="fig" rid="F3">3</xref> shows bootstrap .632+ prediction error curve estimates for boosting without pathway information (thick dashed curve) and for PathBoost approach (thick solid curve), when considering only the cancer pathways. We also investigate models that incorporate the clinical covariate "tumor stage" as a mandatory unpenalized covariate (thin curves). All models perform considerably better than the Kaplan-Meier benchmark. Just as for the DLBCL data, there is an advantage of PathBoost over boosting without pathway information, albeit a smaller one, indicating usefulness of pathway information for prediction. In contrast to the DLBCL example, PathBoost also performs better when the clinical covariate is included. This indicates that the pathways provide information in addition to the clinical covariate.</p><fig position="float" id="F3"><label>Figure 3</label><caption><p><bold>Prediction error curves for the ovarian cancer data</bold>. Bootstrap .632+ prediction error curve estimates for boosting without pathway information (dashed curves) and PathBoost (solid curves), applied to ovarian cancer data, without (thick curves) and with clinical covariates (thin curves). The Kaplan-Meier benchmark (grey curve) is given as a reference.</p></caption><graphic xlink:href="1471-2105-10-18-3"/></fig></sec></sec></sec><sec><title>Conclusion</title><p>Integration of different sources of information promises to result in improved predictive models built from microarray data. For example, the potential of experimentally derived pathway signatures was already demonstrated in [<xref ref-type="bibr" rid="B8">8</xref>] for various independent cancer data sets.</p><p>Another source of pathway knowledge is the KEGG pathway database [<xref ref-type="bibr" rid="B1">1</xref>]. In [<xref ref-type="bibr" rid="B9">9</xref>], an approach was presented that utilizes this source for tailoring the penalty term in Lasso-like estimation. However, such approaches are not readily available for binary response and time-to-event data. Furthermore, they require explicit specification of a penalty structure, which is, e.g., problematic when the parameters of connected genes might have different sign.</p><p>As an alternative, we proposed a new likelihood-based boosting approach that also incorporates pathway information. Penalties are adapted after every boosting step, such that a microarray feature that is connected to another feature that already has a received a non-zero parameter estimate, is more likely to also receive a non-zero estimate. This avoids specification of a penalty structure, and therefore is not affected by parameters with opposite sign.</p><p>The proposed PathBoost was seen to perform well in various settings of a simulation study, using the design employed in [<xref ref-type="bibr" rid="B9">9</xref>]. While the approach given in [<xref ref-type="bibr" rid="B9">9</xref>] performed better in settings where the sign of the true parameters matched with its penalty structure, PathBoost showed equal or better performance in the other settings. This pattern of prediction performance might have been expected, as knowledge of the true sign of the parameters (in this case incorporated into the penalty structure) should result in increased prediction performance. However, in typical application settings such knowledge will rarely be available. Therefore, the PathBoost approach should be preferred. There still is a certain arbitrariness with respect to the suggested updated rules, i.e., other rules that might also work could be devised. However, the good performance, resulting from the suggested rules, provides at least some justification.</p><p>We employed the simulation design used in [<xref ref-type="bibr" rid="B9">9</xref>] to allow for better comparison to the results there. However, the design itself has some limitations, making it difficult to draw conclusions on performance with real data. For example, the pathway information employed does not contain inaccuracies, which will probably be present in sources such as the KEGG database. Also, the signal-to-noise ratios are large, resulting in simple prediction problems, untypical for microarray data. Furthermore, the simulation study is limited to continuous response settings, due to lack of an algorithm for the approach given in [<xref ref-type="bibr" rid="B9">9</xref>] for other response types. However, in most microarray applications the response is binary or a time-to-event response. Fitting predictive models for these is more difficult, and, therefore, less benefit from incorporating pathway information might be expected.</p><p>The proposed boosting approach is easily adapted to different response types. Variants for generalized linear models and the Cox proportional hazards model were given. The latter was employed in two application examples, where the gain in prediction performance by incorporating pathway information was more moderate, compared to the simulation study. As indicated, this might, e.g., be due to inaccuracies in the KEGG database. The estimated parameters of several connected microarray features had opposite sign, indicating similarity to those scenarios of the simulation study, where only PathBoost could fully utilize pathway information.</p><p>In comparison to models fitted without pathway information, application of PathBoost resulted in structurally different model fits, now honoring knowledge from external sources such as the KEGG database. Credibility of the interpretation of the new model fits was underlined by improved prediction performance. Given more detailed pathway knowledge, e.g., with information on the direction of gene relations and measures of uncertainty being available, further improvement of model fits could be expected. As demonstrated, the proposed boosting algorithm is highly flexible in terms of being able to incorporate additional sources of knowledge. While further refinements could be devised, e.g., for including information from Gene Ontology, it can already now be expected to provide for better model fits with better prediction performance in many applications.</p></sec><sec><title>Authors' contributions</title><p>HB developed and implemented the initial version of the proposed algorithm, performed the simulation study, applied the algorithm to the example data, and wrote most of the manuscript. MS contributed design decisions for the algorithm, helped with interpretation of the results for the simulation study and the example data, and revised the manuscript.</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>We gratefully acknowledge support from Deutsche Forschungsgemeinschaft (DFG Forschergruppe FOR 534).</p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kanehisa</surname><given-names>M</given-names></name><name><surname>Goto</surname><given-names>S</given-names></name></person-group><article-title>KEGG: Kyoto Encyclopedia of Genes and Genomes</article-title><source>Nucleic Acids Research</source><year>2000</year><volume>28</volume><fpage>27</fpage><lpage>30</lpage><pub-id pub-id-type="pmid">10592173</pub-id><pub-id pub-id-type="doi">10.1093/nar/28.1.27</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>H</given-names></name></person-group><article-title>Nonparametric Pathway-Based Regression Models for Analysis of Genomic Data</article-title><source>Biostatistics</source><year>2007</year><volume>8</volume><fpage>265</fpage><lpage>284</lpage><pub-id pub-id-type="pmid">16772399</pub-id><pub-id pub-id-type="doi">10.1093/biostatistics/kxl007</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>H</given-names></name></person-group><article-title>A Hidden Spatial-Temporal Markov Random Field Model for Network-Based Analysis of Time Course Gene Expression Data</article-title><source>Annals of Applied Statistics</source><year>2008</year><volume>2</volume><fpage>408</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1214/07--AOAS145</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>P</given-names></name><name><surname>Pan</surname><given-names>W</given-names></name></person-group><article-title>Incorporating Gene Networks into Statistical Tests for Genomic Data via a Spatially Correlated Mixture Model</article-title><source>Bioinformatics</source><year>2008</year><volume>24</volume><fpage>404</fpage><lpage>411</lpage><pub-id pub-id-type="pmid">18083717</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btm612</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname><given-names>M</given-names></name><name><surname>Ball</surname><given-names>CA</given-names></name><name><surname>Blake</surname><given-names>JA</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name><name><surname>Butler</surname><given-names>H</given-names></name><name><surname>Cherry</surname><given-names>JM</given-names></name><name><surname>Davis</surname><given-names>AP</given-names></name><name><surname>Dolinski</surname><given-names>K</given-names></name><name><surname>Dwight</surname><given-names>SS</given-names></name><name><surname>Eppig</surname><given-names>JT</given-names></name><name><surname>Harris</surname><given-names>MA</given-names></name><name><surname>Hill</surname><given-names>DP</given-names></name><name><surname>Issel-Tarver</surname><given-names>L</given-names></name><name><surname>Kasarskis</surname><given-names>A</given-names></name><name><surname>Lewis</surname><given-names>AK</given-names></name><name><surname>Matese</surname><given-names>JC</given-names></name><name><surname>Richardson</surname><given-names>JE</given-names></name><name><surname>Ringwald</surname><given-names>M</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Sherlock</surname><given-names>G</given-names></name></person-group><article-title>Gene Ontology: Tool for the Unification of Biology</article-title><source>Nature Genetics</source><year>2000</year><volume>25</volume><fpage>25</fpage><lpage>29</lpage><pub-id pub-id-type="pmid">10802651</pub-id><pub-id pub-id-type="doi">10.1038/75556</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Goeman</surname><given-names>JJ</given-names></name><name><surname>Mansmann</surname><given-names>U</given-names></name></person-group><article-title>Multiple Testing on the Directed Acyclic Graph of Gene Ontology</article-title><source>Bioinformatics</source><year>2008</year><volume>24</volume><fpage>537</fpage><lpage>544</lpage><pub-id pub-id-type="pmid">18203773</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btm628</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Luan</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>H</given-names></name></person-group><article-title>Group Additive Regression Models for Genomic Data Analysis</article-title><source>Biostatistics</source><year>2008</year><volume>9</volume><fpage>100</fpage><lpage>113</lpage><pub-id pub-id-type="pmid">17513311</pub-id><pub-id pub-id-type="doi">10.1093/biostatistics/kxm015</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bild</surname><given-names>AH</given-names></name><name><surname>Yao</surname><given-names>G</given-names></name><name><surname>Chang</surname><given-names>JT</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Potti</surname><given-names>A</given-names></name><name><surname>Chasse</surname><given-names>D</given-names></name><name><surname>Joshi</surname><given-names>MB</given-names></name><name><surname>Harpole</surname><given-names>D</given-names></name><name><surname>Lancaster</surname><given-names>JM</given-names></name><name><surname>Berchuck</surname><given-names>A</given-names></name><name><surname>Olson</surname><given-names>JA</given-names></name><name><surname>Marks</surname><given-names>JR</given-names></name><name><surname>Dressman</surname><given-names>HK</given-names></name><name><surname>West</surname><given-names>M</given-names></name></person-group><article-title>Oncogenic Pathway Signatures in Human Cancers as a Guide to Targeted Therapies</article-title><source>Nature</source><year>2006</year><volume>439</volume><fpage>353</fpage><lpage>357</lpage><pub-id pub-id-type="pmid">16273092</pub-id><pub-id pub-id-type="doi">10.1038/nature04296</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>H</given-names></name></person-group><article-title>Network-constrained Regularization and Variable Selection for Analysis of Genomic Data</article-title><source>Bioinformatics</source><year>2008</year><volume>24</volume><fpage>1175</fpage><lpage>1182</lpage><pub-id pub-id-type="pmid">18310618</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btn081</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>Regression Shrinkage and Selection via the Lasso</article-title><source>Journal of the Royal Statistical Society B</source><year>1996</year><volume>58</volume><fpage>267</fpage><lpage>288</lpage></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Saunders</surname><given-names>M</given-names></name><name><surname>Rosset</surname><given-names>S</given-names></name><name><surname>Zhu</surname><given-names>J</given-names></name><name><surname>Kneight</surname><given-names>K</given-names></name></person-group><article-title>Sparsity and Smoothness Via the Fused Lasso</article-title><source>Journal of the Royal Statistical Society B</source><year>2005</year><volume>67</volume><fpage>91</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00490.x</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>MY</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name></person-group><article-title>L<sub>1</sub>-Regularization Path Algorithms for Generalized Linear Models</article-title><source>Journal of the Royal Statistical Society B</source><year>2007</year><volume>69</volume><fpage>659</fpage><lpage>677</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9868.2007.00607.x</pub-id></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>B&#x000fc;hlmann</surname><given-names>P</given-names></name><name><surname>Hothorn</surname><given-names>T</given-names></name></person-group><article-title>Boosting Algorithms: Regularization, Prediction and Model Fitting</article-title><source>Statistical Science</source><year>2007</year><volume>22</volume><fpage>477</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1214/07-STS242</pub-id></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>B&#x000fc;hlmann</surname><given-names>P</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name></person-group><article-title>Boosting With the L2 Loss: Regression and Classification</article-title><source>Journal of the American Statistical Association</source><year>2003</year><volume>98</volume><fpage>324</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1198/016214503000125</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tutz</surname><given-names>G</given-names></name><name><surname>Binder</surname><given-names>H</given-names></name></person-group><article-title>Boosting Ridge Regression</article-title><source>Computational Statistics &#x00026; Data Analysis</source><year>2007</year><volume>51</volume><fpage>6044</fpage><lpage>6059</lpage><pub-id pub-id-type="doi">10.1016/j.csda.2006.11.041</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>H</given-names></name><name><surname>Schumacher</surname><given-names>M</given-names></name></person-group><article-title>Allowing for Mandatory Covariates in Boosting Estimation of Sparse High-Dimensional Survival Models</article-title><source>BMC Bioinformatics</source><year>2008</year><volume>9</volume><fpage>14</fpage><pub-id pub-id-type="pmid">18186927</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-9-14</pub-id></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tutz</surname><given-names>G</given-names></name><name><surname>Binder</surname><given-names>H</given-names></name></person-group><article-title>Generalized Additive Modelling with Implicit Variable Selection by Likelihood Based Boosting</article-title><source>Biometrics</source><year>2006</year><volume>62</volume><fpage>961</fpage><lpage>971</lpage><pub-id pub-id-type="pmid">17156269</pub-id><pub-id pub-id-type="doi">10.1111/j.1541-0420.2006.00578.x</pub-id></citation></ref><ref id="B18"><citation citation-type="book"><person-group person-group-type="author"><name><surname>McCullagh</surname><given-names>P</given-names></name><name><surname>Nelder</surname><given-names>JA</given-names></name></person-group><source>Generalized Linear Models</source><year>1989</year><edition>2</edition><publisher-name>London, U.K.: Chapman &#x00026; Hall</publisher-name></citation></ref><ref id="B19"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Freund</surname><given-names>Y</given-names></name><name><surname>Schapire</surname><given-names>RE</given-names></name></person-group><article-title>Experiments with a new boosting algorithm</article-title><source>Machine Learning: Proc Thirteenth International Conference</source><year>1996</year><publisher-name>San Francisco, CA: Morgan Kaufman</publisher-name><fpage>148</fpage><lpage>156</lpage></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>JH</given-names></name></person-group><article-title>Greedy Function Approximation: A Gradient Boosting Machine</article-title><source>The Annals of Statistics</source><year>2001</year><volume>29</volume><fpage>1189</fpage><lpage>1232</lpage><pub-id pub-id-type="doi">10.1214/aos/1013203451</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Efron</surname><given-names>B</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Johnstone</surname><given-names>I</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>Least Angle Regression</article-title><source>The Annals of Statistics</source><year>2004</year><volume>32</volume><fpage>407</fpage><lpage>499</lpage><pub-id pub-id-type="doi">10.1214/009053604000000067</pub-id></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gerds</surname><given-names>TA</given-names></name><name><surname>Schumacher</surname><given-names>M</given-names></name></person-group><article-title>Efron-type measures of prediction error for survival analysis</article-title><source>Biometrics</source><year>2007</year><volume>63</volume><fpage>1283</fpage><lpage>1287</lpage><pub-id pub-id-type="pmid">17651459</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schumacher</surname><given-names>M</given-names></name><name><surname>Binder</surname><given-names>H</given-names></name><name><surname>Gerds</surname><given-names>TA</given-names></name></person-group><article-title>Assessment of Survival Prediction Models Based on Microarray Data</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><fpage>1768</fpage><lpage>1774</lpage><pub-id pub-id-type="pmid">17485430</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btm232</pub-id></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>H</given-names></name><name><surname>Schumacher</surname><given-names>M</given-names></name></person-group><article-title>Adapting Prediction Error Estimates for Biased Complexity Selection in High-Dimensional Bootstrap Samples</article-title><source>Stat Appl Genet Mol Biol</source><year>2008</year><volume>7</volume><fpage>Article 12</fpage><pub-id pub-id-type="pmid">18384265</pub-id></citation></ref><ref id="B25"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>H</given-names></name><name><surname>Schumacher</surname><given-names>M</given-names></name></person-group><article-title>Comment on 'Network-Constrained Regularization and Variable Selection for Analysis of Genomic Data'</article-title><source>Bioinformatics</source><year>2008</year><volume>24</volume><fpage>2566</fpage><lpage>2568</lpage><pub-id pub-id-type="pmid">18682424</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btn412</pub-id></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rosenwald</surname><given-names>A</given-names></name><name><surname>Wright</surname><given-names>G</given-names></name><name><surname>Chan</surname><given-names>WC</given-names></name><name><surname>Connors</surname><given-names>JM</given-names></name><name><surname>Campo</surname><given-names>E</given-names></name><name><surname>Fisher</surname><given-names>RI</given-names></name><name><surname>Gascoyna</surname><given-names>RD</given-names></name><name><surname>Muller-Hermelink</surname><given-names>HK</given-names></name><name><surname>Smeland</surname><given-names>EB</given-names></name><name><surname>Staudt</surname><given-names>LM</given-names></name></person-group><article-title>The Use of Molecular Profiling to Predict Survival After Chemotherapy for Diffuse Large-B-cell Lymphoma</article-title><source>The New England Journal of Medicine</source><year>2002</year><volume>346</volume><fpage>1937</fpage><lpage>1946</lpage><pub-id pub-id-type="pmid">12075054</pub-id><pub-id pub-id-type="doi">10.1056/NEJMoa012914</pub-id></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Irizarry</surname><given-names>RA</given-names></name><name><surname>Hobbs</surname><given-names>B</given-names></name><name><surname>Collin</surname><given-names>F</given-names></name><name><surname>Beazer-Barclay</surname><given-names>YD</given-names></name><name><surname>Antonellis</surname><given-names>KJ</given-names></name><name><surname>Scherf</surname><given-names>U</given-names></name><name><surname>Speed</surname><given-names>TP</given-names></name></person-group><article-title>Exploration, Normalization, and Summaries of High Denisty Oligonucleotide Array Probe Level Data</article-title><source>Biostatistics</source><year>2003</year><volume>4</volume><fpage>249</fpage><lpage>264</lpage><pub-id pub-id-type="pmid">12925520</pub-id><pub-id pub-id-type="doi">10.1093/biostatistics/4.2.249</pub-id></citation></ref></ref-list></back></article>