<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article" xml:lang="EN"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Neuroinformatics</journal-id><journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id><journal-title>Frontiers in Neuroinformatics</journal-title><issn pub-type="epub">1662-5196</issn><publisher><publisher-name>Frontiers Research Foundation</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19212459</article-id><article-id pub-id-type="pmc">2638552</article-id><article-id pub-id-type="doi">10.3389/neuro.11.003.2009</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>PyMVPA: A Unifying Approach to the Analysis of Neuroscientific Data</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hanke</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="author-notes" rid="fn002"><sup>&#x02020;</sup></xref></contrib><contrib contrib-type="author"><name><surname>Halchenko</surname><given-names>Yaroslav O.</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref><xref ref-type="author-notes" rid="fn002"><sup>&#x02020;</sup></xref></contrib><contrib contrib-type="author"><name><surname>Sederberg</surname><given-names>Per B.</given-names></name><xref ref-type="aff" rid="aff6"><sup>6</sup></xref><xref ref-type="aff" rid="aff7"><sup>7</sup></xref></contrib><contrib contrib-type="author"><name><surname>Olivetti</surname><given-names>Emanuele</given-names></name><xref ref-type="aff" rid="aff8"><sup>8</sup></xref><xref ref-type="aff" rid="aff9"><sup>9</sup></xref></contrib><contrib contrib-type="author"><name><surname>Fr&#x000fc;nd</surname><given-names>Ingo</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff10"><sup>10</sup></xref><xref ref-type="aff" rid="aff11"><sup>11</sup></xref></contrib><contrib contrib-type="author"><name><surname>Rieger</surname><given-names>Jochem W.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff12"><sup>12</sup></xref><xref ref-type="aff" rid="aff13"><sup>13</sup></xref></contrib><contrib contrib-type="author"><name><surname>Herrmann</surname><given-names>Christoph S.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff11"><sup>11</sup></xref><xref ref-type="aff" rid="aff13"><sup>13</sup></xref></contrib><contrib contrib-type="author"><name><surname>Haxby</surname><given-names>James V.</given-names></name><xref ref-type="aff" rid="aff14"><sup>14</sup></xref><xref ref-type="aff" rid="aff15"><sup>15</sup></xref></contrib><contrib contrib-type="author"><name><surname>Hanson</surname><given-names>Stephen Jos&#x000e9;</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib><contrib contrib-type="author"><name><surname>Pollmann</surname><given-names>Stefan</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff13"><sup>13</sup></xref><xref ref-type="author-notes" rid="fn001">*</xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Department of Psychology, University of Magdeburg</institution><country>Magdeburg, Germany</country></aff><aff id="aff2"><sup>2</sup><institution>Center for Advanced Imaging</institution><country>Magdeburg, Germany</country></aff><aff id="aff3"><sup>3</sup><institution>Psychology Department, Rutgers Newark</institution><country>New Jersey, USA</country></aff><aff id="aff4"><sup>4</sup><institution>Computer Science Department, New Jersey Institute of Technology, Newark</institution><country>New Jersey, USA</country></aff><aff id="aff5"><sup>5</sup><institution>Rutgers University Mind Brain Analysis, Rutgers Newark</institution><country>New Jersey, USA</country></aff><aff id="aff6"><sup>6</sup><institution>Department of Psychology, Princeton University, Princeton</institution><country>New Jersey, USA</country></aff><aff id="aff7"><sup>7</sup><institution>Princeton Neuroscience Institute, Princeton University, Princeton</institution><country>New Jersey, USA</country></aff><aff id="aff8"><sup>8</sup><institution>Center for Information Technology (Irst), Fondazione Bruno Kessler</institution><country>Trento, Italy</country></aff><aff id="aff9"><sup>9</sup><institution>Center for Mind/Brain Sciences (CIMeC/NILab), University of Trento</institution><country>Italy</country></aff><aff id="aff10"><sup>10</sup><institution>Leibniz Institute for Neurobiology</institution><country>Magdeburg, Germany</country></aff><aff id="aff11"><sup>11</sup><institution>Bernstein Group for Computational Neuroscience</institution><country>Magdeburg, Germany</country></aff><aff id="aff12"><sup>12</sup><institution>Department of Neurology, University of Magdeburg</institution><country>Magdeburg, Germany</country></aff><aff id="aff13"><sup>13</sup><institution>Center for Behavioral Brain Sciences</institution><country>Magdeburg, Germany</country></aff><aff id="aff14"><sup>14</sup><institution>Center for Cognitive Neuroscience, Dartmouth College, Hanover</institution><country>New Hampshire, USA</country></aff><aff id="aff15"><sup>15</sup><institution>Department of Psychological and Brain Sciences, Dartmouth College, Hanover</institution><country>New Hampshire, USA</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Rolf K&#x000f6;tter, Radboud University Nijmegen, The Netherlands</p></fn><fn fn-type="edited-by"><p>Reviewed by: Martin A. Spacek, The University of British Columbia, Canada; Samuel Garcia, Universit&#x000e9; Claude Bernard Lyon I, France</p></fn><corresp id="fn001">*Correspondence: Stefan Pollmann, Institut f&#x000fc;r Psychologie II, Otto-von-Guericke-Universit&#x000e4;t Magdeburg, PF 4120, D-39016 Magdeburg, Germany. e-mail: <email>stefan.pollmann@ovgu.de</email></corresp><fn fn-type="other" id="fn002"><p><sup>&#x02020;</sup>Hanke and Halchenko contributed equally to this article.</p></fn></author-notes><pub-date pub-type="epreprint"><day>20</day><month>10</month><year>2008</year></pub-date><pub-date pub-type="epub"><day>04</day><month>2</month><year>2009</year></pub-date><pub-date pub-type="collection"><year>2009</year></pub-date><volume>3</volume><elocation-id>3</elocation-id><history><date date-type="received"><day>14</day><month>9</month><year>2008</year></date><date date-type="accepted"><day>20</day><month>1</month><year>2009</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2009 Hanke, Halchenko, Sederberg, Olivetti, Fr&#x000fc;nd, Rieger, Herrmann, Haxby, Hanson and Pollmann.</copyright-statement><copyright-year>2009</copyright-year><license license-type="open-access" xlink:href="http://www.frontiersin.org/licenseagreement"><p>This is an open-access article subject to an exclusive license agreement between the authors and the Frontiers Research Foundation, which permits unrestricted use, distribution, and reproduction in any medium, provided the original authors and source are credited.</p></license></permissions><abstract><p>The Python programming language is steadily increasing in popularity as the language of choice for scientific computing. The ability of this scripting environment to access a huge code base in various languages, combined with its syntactical simplicity, make it the ideal tool for implementing and sharing ideas among scientists from numerous fields and with heterogeneous methodological backgrounds. The recent rise of reciprocal interest between the machine learning (ML) and neuroscience communities is an example of the desire for an inter-disciplinary transfer of computational methods that can benefit from a Python-based framework. For many years, a large fraction of both research communities have addressed, almost independently, very high-dimensional problems with almost completely non-overlapping methods. However, a number of recently published studies that applied ML methods to neuroscience research questions attracted a lot of attention from researchers from both fields, as well as the general public, and showed that this approach can provide novel and fruitful insights into the functioning of the brain. In this article we show how <italic>PyMVPA</italic>, a specialized Python framework for machine learning based data analysis, can help to facilitate this inter-disciplinary technology transfer by providing a single interface to a wide array of machine learning libraries and neural data-processing methods. We demonstrate the general applicability and power of <italic>PyMVPA</italic> via analyses of a number of neural data modalities, including fMRI, EEG, MEG, and extracellular recordings.</p></abstract><kwd-group><kwd>functional magnetic resonance imaging</kwd><kwd>electroencephalography</kwd><kwd>magnetoencephalography</kwd><kwd>extracellular recordings</kwd><kwd>machine learning</kwd><kwd>Python</kwd></kwd-group><counts><fig-count count="6"/><table-count count="1"/><equation-count count="0"/><ref-count count="38"/><page-count count="13"/><word-count count="8700"/></counts></article-meta></front><body><sec sec-type="introduction"><title>Introduction</title><p>Understanding how the brain is able to give rise to complex behavior has stimulated a plethora of brain measures such as non-invasive EEG<xref ref-type="fn" rid="fn1">1</xref>, MEG<xref ref-type="fn" rid="fn2">2</xref>, MRI<xref ref-type="fn" rid="fn3">3</xref>, PET<xref ref-type="fn" rid="fn4">4</xref>, optical imaging, and invasive extracellular and intracellular recordings, often in conjunction with new methods, models, and techniques. Each data acquisition method has offered a unique set of properties in terms of spatio-temporal resolution, signal to noise, data acquisition cost, applicability to humans, and the corresponding neural correlates that result from the measurement process.</p><p>Neuroscientists often focus on only one or a smaller subset of these neural modalities partly due to the kinds of questions investigated and partly due to the cost of learning to analyze data from these different modalities. The diverse measurement approaches to brain function can heavily influence the selection of a research question and, in turn, the development of specific software packages to answer them. Consequently, the peculiarities of each data acquisition modality and the lack of strong interaction between the neuroscience communities employing them have produced distinct software packages specialized for the conventional analyses within a particular modality. Some analysis techniques have become, due to normative concerns, <italic>de facto</italic> standards despite their limitations and inappropriate assumptions for the given data type. For instance, the general linear model (GLM) is the prevalent approach used in fMRI data analysis, despite being a restrictive mass-univariate method (Kriegeskorte and Bandettini, <xref ref-type="bibr" rid="B21">2007</xref>; O'Toole et al., <xref ref-type="bibr" rid="B28">2007</xref>).</p><p>While specialized software packages are useful when dealing with the specific properties of a single data modality, they limit the flexibility to transfer newly developed analysis techniques to other fields of neuroscience. This issue is compounded by the closed-source, or restrictive licensing of many software packages, which further limits software flexibility and extensibility.</p><p>However, outside the neuroscience community, machine learning (ML) research has spawned a set of analysis techniques that are typically generic, flexible (e.g., classification, regression, clustering), powerful (e.g., multivariate, linear and non-linear) and often applicable to various data modalities with minor modality-specific preprocessing (see Pereira et al., in press, for a tutorial on application of ML methods to the analysis of fMRI data). Moreover, large parts of this community favor the open-source software development model (Sonnenburg et al., <xref ref-type="bibr" rid="B35">2007</xref>, see also <italic>MLOSS</italic><xref ref-type="fn" rid="fn5">5</xref> project website), which leads to an increase in scientific progress due to the superior accessibility of information and reproducibility of scientific results. These advantages have recently attracted considerable interest throughout the neuroscience community (see Haynes and Rees, <xref ref-type="bibr" rid="B16">2006</xref>; Norman et al., <xref ref-type="bibr" rid="B27">2006</xref>, for reviews).</p><p>Nevertheless, various factors have delayed the adoption of these newer methods for the analysis of neural information. First and foremost, existing conventional techniques are well-tested and often perfectly suitable for the standard analysis of data from the modality for which they were designed. Most importantly, however, a set of sophisticated software packages has evolved over time that allow researchers to apply these conventional and modality-specific methods without requiring in-depth knowledge about low-level programming languages or underlying numerical methods. In fact, most of these packages come with convenient graphical and command line interfaces that abstract the peculiarities of the methods and allow researchers to focus on designing experiments and to address actual research questions without having to develop specialized analyses for each study.</p><p>However, only a few software packages exist that are specifically tailored towards straightforward and interactive exploration of neuroscientific data using a broad range of ML techniques, such as the Matlab<sup>&#x000ae;</sup><xref ref-type="fn" rid="fn6">6</xref> MVPA toolbox for fMRI data<xref ref-type="fn" rid="fn7">7</xref> (Detre et al., <xref ref-type="bibr" rid="B4">2006</xref>). At present only independent component analysis (ICA), an unsupervised method, seems to be supported by numerous software packages (see Beckmann and Smith, <xref ref-type="bibr" rid="B1">2005</xref>, for fMRI, and Makeig et al., <xref ref-type="bibr" rid="B26">2004</xref>, for EEG data analysis). Therefore, the application of machine learning analyses, referred to in the literature as <italic>decoding</italic> (Haynes et al., <xref ref-type="bibr" rid="B17">2007</xref>; Kamitani and Tong, <xref ref-type="bibr" rid="B20">2005</xref>), <italic>information-based</italic> analysis (Kriegeskorte et al., <xref ref-type="bibr" rid="B22">2006</xref>) or <italic>multi-voxel pattern analysis</italic> (Norman et al., <xref ref-type="bibr" rid="B27">2006</xref>), usually involves the development of a significant amount of custom code. Hence, users are typically required to have in-depth knowledge about both data modality peculiarities and software implementation details.</p><p>At the same time, Python has become the open-source scripting language of choice in the research community to prototype and carry out scientific data analyses or to develop complete software solutions quickly. It has attracted attention due to its openness, flexibility, and the availability of a constantly evolving set of tools for the analysis of many types of data. Python's automatic memory management, in conjunction with its powerful libraries for efficient computation (<italic>NumPy</italic><xref ref-type="fn" rid="fn8">8</xref> and <italic>SciPy</italic><xref ref-type="fn" rid="fn9">9</xref>) abstracts users from low-level &#x0201c;software engineering&#x0201d; tasks and allows them to fully concentrate their attention on the development of computational methods.</p><p>As an interpreted, high-level scripting language with a simple and consistent syntax, a plethora of available modules, easy ways to interface to low-level libraries written in other languages<xref ref-type="fn" rid="fn10">10</xref> and high-level computing environments<xref ref-type="fn" rid="fn11">11</xref>, Python is the language of choice for solving many scientific computing problems. Table <xref ref-type="table" rid="T1">1</xref> lists a number of Python modules which might be of interest in the neuroscientific context, and is meant to complement the material presented in the other articles in this special issue.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p><bold>Various free and open-source projects, either written in Python or providing Python bindings, which are germane to acquiring or processing neural information datasets using machine learning (ML) methods</bold>. The last column indicates whether PyMVPA internally uses a particular project or provides public interfaces to it.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="1" colspan="1">Name</th><th align="left" rowspan="1" colspan="1">Description</th><th align="left" rowspan="1" colspan="1">URL</th><th align="left" rowspan="1" colspan="1">PyMVPA</th></tr></thead><tbody><tr><td align="left" colspan="4" rowspan="1"><bold>MACHINE LEARNING</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">Elephant</td><td align="left" rowspan="1" colspan="1">Multi-purpose library for ML</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://elefant.developer.nicta.com.au">http://elefant.developer.nicta.com.au</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">Shogun</td><td align="left" rowspan="1" colspan="1">Comprehensive ML toolbox</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://www.shogun-toolbox.org">http://www.shogun-toolbox.org</uri></td><td align="left" rowspan="1" colspan="1">&#x02713;</td></tr><tr><td align="left" rowspan="1" colspan="1">Orange</td><td align="left" rowspan="1" colspan="1">General-purpose data mining</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://www.ailab.si/orange">http://www.ailab.si/orange</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">PyML</td><td align="left" rowspan="1" colspan="1">ML in Python</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://pyml.sourceforge.net">http://pyml.sourceforge.net</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">MDP</td><td align="left" rowspan="1" colspan="1">Modular data processing</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://mdp-toolkit.sourceforge.net">http://mdp-toolkit.sourceforge.net</uri></td><td align="left" rowspan="1" colspan="1">&#x02713;</td></tr><tr><td align="left" rowspan="1" colspan="1">hcluster</td><td align="left" rowspan="1" colspan="1">Agglomerative clustering</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://code.google.com/p/scipy-cluster">http://code.google.com/p/scipy-cluster</uri></td><td align="left" rowspan="1" colspan="1">&#x02713;</td></tr><tr><td align="left" rowspan="1" colspan="1">&#x02013;</td><td align="left" rowspan="1" colspan="1">Other Python modules</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://www.mloss.org/software/language/python">http://www.mloss.org/software/language/python</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" colspan="4" rowspan="1"><bold>NEUROSCIENCE RELATED</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">NiPy</td><td align="left" rowspan="1" colspan="1">Neuroimaging data analysis</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://neuroimaging.scipy.org">http://neuroimaging.scipy.org</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">PyMGH</td><td align="left" rowspan="1" colspan="1">Access FreeSurfers.mghfiles</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://code.google.com/p/pyfsio">http://code.google.com/p/pyfsio</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">PyNIfTI</td><td align="left" rowspan="1" colspan="1">Access NIfTI/Analyzefiles</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://niftilib.sourceforge.net/pynifti">http://niftilib.sourceforge.net/pynifti</uri></td><td align="left" rowspan="1" colspan="1">&#x02713;</td></tr><tr><td align="left" rowspan="1" colspan="1">OpenMEEG</td><td align="left" rowspan="1" colspan="1">EEG/MEG inverse problems</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://www-sop.inria.fr/odyssee/software/OpenMEEG">http://www-sop.inria.fr/odyssee/software/OpenMEEG</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" colspan="4" rowspan="1"><bold>STIMULI AND EXPERIMENT DESIGN</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">PyEPL</td><td align="left" rowspan="1" colspan="1">Create complete experiments</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://pyepl.sourceforge.net">http://pyepl.sourceforge.net</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">VisionEgg</td><td align="left" rowspan="1" colspan="1">Visual stimuli generation</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://www.visionegg.org">http://www.visionegg.org</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">PsychoPy</td><td align="left" rowspan="1" colspan="1">Create psychophysical stimuli</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://www.psychopy.org">http://www.psychopy.org</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">PIL</td><td align="left" rowspan="1" colspan="1">Python Imaging Library</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://www.pythonware.com/products/pil">http://www.pythonware.com/products/pil</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" colspan="4" rowspan="1"><bold>INTERFACES TO OTHER COMPUTING ENVIRONMENTS</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">RPy</td><td align="left" rowspan="1" colspan="1">Interface to R</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://rpy.sourceforge.net">http://rpy.sourceforge.net</uri></td><td align="left" rowspan="1" colspan="1">&#x02713;</td></tr><tr><td align="left" rowspan="1" colspan="1">mlabwrap</td><td align="left" rowspan="1" colspan="1">Interface to Matlab</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://mlabwrap.sourceforge.net">http://mlabwrap.sourceforge.net</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" colspan="4" rowspan="1"><bold>GENERIC</bold></td></tr><tr><td align="left" rowspan="1" colspan="1">Matplotlib</td><td align="left" rowspan="1" colspan="1">2D Plotting</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://matplotlib.sourceforge.net">http://matplotlib.sourceforge.net</uri></td><td align="left" rowspan="1" colspan="1">&#x02713;</td></tr><tr><td align="left" rowspan="1" colspan="1">Mayavi2</td><td align="left" rowspan="1" colspan="1">Interactive 3D visualization</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://code.enthought.com/projects/mayavi">http://code.enthought.com/projects/mayavi</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">PyExcelerator</td><td align="left" rowspan="1" colspan="1">Access MS Excel files</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://sourceforge.net/projects/pyexcelerator">http://sourceforge.net/projects/pyexcelerator</uri></td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">pywavelets</td><td align="left" rowspan="1" colspan="1">Discrete wavelet transforms</td><td align="left" rowspan="1" colspan="1"><uri xlink:type="simple" xlink:href="http://www.pybytes.com/pywavelets">http://www.pybytes.com/pywavelets</uri></td><td align="left" rowspan="1" colspan="1">&#x02713;</td></tr></tbody></table></table-wrap><p>Despite the fact that it is possible to perform complex data analyses solely within Python, it <italic>once again</italic> often requires in-depth knowledge of numerous Python modules, as well as the development of a large amount of code to lay the foundation for one's work. Therefore, it would be of great value to have a framework that helps to abstract from both data modality specifics and the implementation details of a particular analysis method. Ideally, such a framework should help to expose any form of data in an optimal format applicable to a broad range of machine learning methods, and on the other hand provide a versatile, yet simple, interface to plug in additional algorithms operating on the data. In the neuroscience context it would also be useful to bridge between well-established neuroimaging tools and ML software packages by providing cross library integration and transparent data handling for typical containers of neuroimaging data (e.g., NIfTI in fMRI research).</p><p>As an attempt to provide such a framework we have implemented PyMVPA<xref ref-type="fn" rid="fn12">12</xref> (MultiVariate Pattern Analysis in Python) &#x02013; a free and open-source Python framework to facilitate uniform analysis of the neural information obtained from different neural modalities. PyMVPA heavily utilizes Python's ability to access libraries written in a large variety of programming languages and computing environments to interface with the wealth of existing machine learning packages developed outside the neuroscience community. Although the framework is eminently suited for neuroscientific datasets, it is by no means limited to this field. However, the neuroscience tuning is a unique aspect of PyMVPA in comparison to other Python-based ML or computing toolboxes, such as <italic>MDP</italic><xref ref-type="fn" rid="fn13">13</xref> or <italic>scipy-cluster</italic><xref ref-type="fn" rid="fn14">14</xref> which are developed as domain-neutral packages.</p><p>The following section provides a short summary of the principal design concepts, and the basic building blocks of the PyMVPA framework. The main focus of this article is, however, a demonstration of PyMVPA's flexibility by applying various ML techniques to typical EEG, MEG, fMRI and extracellular recordings datasets.</p></sec><sec><title>PyMVPA</title><p>One of the main goals of PyMVPA is to reduce the gap between the neuroscience and ML communities. To reach this goal, we designed PyMVPA to provide a convenient, easy to use, community developed (free and open source<xref ref-type="fn" rid="fn15">15</xref>), and extensible framework to facilitate the use of ML techniques on neural information. PyMVPA combines Python data processing, visualization, and basic I/O facilities together with I/O code and examples tailored for neuroscience. For an easy start into PyMVPA a fMRI example dataset (a single subject from the study by Haxby et al., <xref ref-type="bibr" rid="B15">2001</xref>) is available for download from the PyMVPA website.</p><p>As Table <xref ref-type="table" rid="T1">1</xref> highlighted, PyMVPA is not the only ML framework available for scripting and interactive data exploration in Python. In contrast to some of the primarily GUI-based ML toolboxes (e.g., Orange, Elephant), PyMVPA is designed to provide not just a toolbox, but a framework for concise, yet intuitive, scripting of possibly complex analysis pipelines. To achieve this goal, PyMVPA provides a number of building blocks that can be combined in a very flexible way. Figure <xref ref-type="fig" rid="F1">1</xref> shows a schematic representation of the framework design, its building blocks and how they can be combined into complete analysis pipelines.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>PyMVPA workflow and design</bold>. PyMVPA is a modular framework. It consists of several components (gray boxes) such as ML algorithms or dataset storage facilities. Each component contains one or more modules (white boxes) providing a certain functionality, e.g., classifiers, but also feature-wise measures (e.g., I-RELIEF; Sun, <xref ref-type="bibr" rid="B36">2007</xref>), and feature selection methods (recursive feature elimination, RFE; Guyon and Elisseeff, <xref ref-type="bibr" rid="B9">2003</xref>; Guyon et al., <xref ref-type="bibr" rid="B10">2002</xref>). Typically, all implementations within a module are accessible through a uniform interface and can therefore be used interchangeably, i.e., any algorithm using a classifier can be used with any available classifier implementation, such as <italic>support vector machine</italic> (SVM; Vapnik, <xref ref-type="bibr" rid="B37">1995</xref>), or <italic>sparse multinomial logistic regression</italic> (SMLR; Krishnapuram et al., <xref ref-type="bibr" rid="B23">2005</xref>). Some ML modules provide generic <italic>meta</italic> algorithms that can be combined with the <italic>basic</italic> implementations of ML algorithms. For example, a Multi-Class meta classifier provides support for multi-class problems, even if an underlying classifier is only capable to deal with binary problems. Additionally, most of the components in PyMVPA make use of some functionality provided by external software packages (black boxes). In the case of <italic>SVM</italic>, classifiers are interfaced to the implementations in <italic>Shogun</italic> or <italic>LIBSVM</italic>. PyMVPA only provides a convenience wrapper to expose them through a uniform interface. By providing simple, yet flexible interfaces, PyMVPA is specifically designed to connect to and use externally developed software. Any analysis built from those basic elements can be cross-validated by running them on multiple dataset splits that can be generated with a variety of data resampling procedures (e.g., bootstrapping, Efron and Tibshirani, <xref ref-type="bibr" rid="B6">1993</xref>). Detailed information about analysis results can be queried from any building block and can be visualized with various plotting functions that are part of PyMVPA, or can be mapped back into the original data space and format to be further processed by specialized tools (i.e., to create an overlay volume analogous to a statistical parametric mapping). The solid arrows represent a typical connection pattern between the modules. Dashed arrows refer to additional compatible interfaces which, although potentially useful, are not necessarily used in a standard processing chain.</p></caption><graphic xlink:href="fninf-03-003-g001"/></fig><p>This article does not aim to provide a detailed description of the PyMVPA framework, and therefore only a rough overview about the most important technical aspects is presented here. However, a comprehensive introduction is available in Hanke et al. (<xref ref-type="bibr" rid="B11">2009</xref>) and the PyMVPA manual (Hanke et al., <xref ref-type="bibr" rid="B12">2008</xref>).</p><p>In PyMVPA, each building block (e.g., all classifiers) follows a simple, standardized, interface. This allows one to use various types of classifiers interchangeably, without additional changes in the source code, and makes it easy to test the performance of newly developed algorithms on one of the many didactical neuroscience-related examples and datasets that are included in PyMVPA. In addition, any implementation of an analysis method/algorithm benefits from the basic <italic>house-keeping</italic> functionality done by the base classes, reducing the necessary amount of code needed to contribute a new fully-functional algorithm. PyMVPA takes care of hiding implementation-specific details, such as a classifier algorithm provided by an external C++ library. At the same time it tries to expose all available information (e.g., classifier training performance) through a consistent interface (for reference, this interface is called <italic>states</italic> in PyMVPA).</p><p>PyMVPA makes use of a number of external software packages, including other Python modules and low-level libraries (e.g., LIBSVM<xref ref-type="fn" rid="fn16">16</xref>) and computing environments (e.g., R<xref ref-type="fn" rid="fn17">17</xref>). Using externally developed software instead of reimplementing algorithms has the advantage of a larger developer and user base and makes it more likely to find and fix bugs in a software package to ensure a high level of quality. However, using external software also carries the risk of breaking functionality when any of the external dependencies break. To address this problem PyMVPA utilizes an automatic testing framework performing various types of tests ranging from unittests (currently covering 84% of all lines of code) to sample code snippet tests in the manual and the source code documentation itself to more evolved &#x0201c;real-life&#x0201d; examples. This facility allows one to test the framework within a variety of specific settings, such as the unique combination of program and library versions found on a particular user machine.</p><p>At the same time, the testing framework also significantly eases the inclusion of code by a novel contributor by catching errors that would potentially break the project's functionality. Being open-source does not always mean <italic>easy to contribute</italic> due to various factors such as a complicated application programming interface (API) coupled with undocumented source code and unpredictable outcomes from any code modifications (bug fixes, optimizations, improvements). PyMVPA welcomes contributions, and thus, addresses all the previously mentioned points:</p><p><bold>Accessibility of source code and documentation:</bold> All the source code (including website and examples) together with the full development history is publicly available via a distributed version control system<xref ref-type="fn" rid="fn18">18</xref> which makes it very easy to track the development of the project, as well as to develop independently and to submit back into the project.</p><p><bold>Inplace code documentation:</bold> Large parts of the source code are well documented using reStructuredText<xref ref-type="fn" rid="fn19">19</xref>, a lightweight markup language that is highly readable in source format as well as being suitable for automatic conversion into HTML or PDF reference documentation. In fact, <italic>Ohloh.net</italic><xref ref-type="fn" rid="fn20">20</xref> source code analysis judges PyMVPA as having &#x0201c;extremely well-commented source code.&#x0201d;</p><p><bold>Developer guidelines:</bold> A brief summary defines a set of coding conventions to facilitate uniform code and documentation look and feel. Automatic checking of compliance to a subset of the coding standards is provided through a custom <italic>PyLint</italic><xref ref-type="fn" rid="fn21">21</xref> configuration, allowing early stage minor bug catching.</p><p>Moreover, PyMVPA does not raise barriers by being limited to specific platforms. It could fully or partially be used on any platform supported by Python (depending on the availability of external dependencies). However, to improve the accessibility, we provide binary installers for Windows, and MacOS X, as well as binary packages for Debian GNU/Linux (included in the official repository), Ubuntu, and a large number of RPM-based GNU/Linux distributions, such as OpenSUSE, RedHat, CentOS, Mandriva, and Fedora. Additionally, the available documentation provides detailed instructions on how to build the packages from source on many platforms.</p><p>A final important feature of PyMVPA is that it allows, by design, researchers to compress complex analyses into a small amount of code. This makes it possible to complement publications with the source code actually used to perform the analysis as Supplementary Material. Making this critical piece of information publicly available allows for in-depth reviews of the applied methods on a level well beyond what is possible with verbal descriptions. To demonstrate this feature, this paper is accompanied by the full source code to perform all analyses shown in the following sections.</p></sec><sec><title>Illustrative Examples: PyMVPA on Different Modalities</title><p>In this section we provide example analyses of four datasets, each from a different modality (EEG, MEG, fMRI, and extracellular recordings). All examples follow the same basic analysis pipeline: initial modality-specific preprocessing, application of ML methods, and visualization of the results. For the modality-independent machine learning stage, all four examples employ the same analysis with <italic>exactly</italic> the same source code. Specifically, we first perform cross-validation with one or more classifiers on each dataset then compute feature-wise sensitivity measures. These measures can then be examined to reveal their implications in terms of the underlying research question.</p><p>These examples do not aim to provide an overview of the full functionality available within PyMVPA, but rather to show that ML methods can be easily applied to various types of data to provide meaningful and even thought-provoking results.</p><sec><title>EEG</title><p>The dataset used for the EEG example consists of a single participant from a previously published study on object recognition (Fr&#x000fc;nd et al., <xref ref-type="bibr" rid="B8">2008</xref>). In the experiment, participants indicated, for a sequence of images, whether they considered each particular image a meaningful object or just object-like with a meaningless configuration. This task was performed for two sets of stimuli with different statistical properties and under two different speed constraints. EEG was recorded from 31 electrodes at a sampling rate of 500&#x02009;Hz using standard recording techniques. Details of the recording procedure can be found in Fr&#x000fc;nd et al. (<xref ref-type="bibr" rid="B8">2008</xref>). A detailed description of the stimuli can be found in Busch et al. (<xref ref-type="bibr" rid="B3">2006</xref>, colored images) and in Herrmann et al. (<xref ref-type="bibr" rid="B18">2004</xref>, line-art pictures).</p><p>Fr&#x000fc;nd et al. (<xref ref-type="bibr" rid="B8">2008</xref>) performed a wavelet-based time-frequency analyses of channels from a posterior region of interest (ROI) (i.e., no multivariate methods were employed). Here, we apply multivariate methods to differentiate between two conditions: trials with colored stimuli (broad spectrum of spatial frequencies and a high level of detail) and trials with black and white line-art stimuli (Figure <xref ref-type="fig" rid="F2">2</xref>A), collapsing the data across all other conditions. This discrimination is orthogonal to the participants task of indicating object vs. non-object stimuli.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Sensitivities for the classification of color and line-art conditions</bold>. Panel <bold>(A)</bold> shows ERPs of each condition for electrode <italic>Pz</italic>. The light shaded area shows the standard deviation, the darker shade the 95% confidence interval around the mean ERP of each condition. The black curve is the difference wave of both ERPs. The stimulus example images are from Fr&#x000fc;nd et al. (<xref ref-type="bibr" rid="B8">2008</xref>). Panel <bold>(B)</bold> shows feature sensitivity measures for the different methods. Sensitivities were normalized by scaling the vector norm of each sensitivity vector (covering all timepoints from all electrodes) to unit length. This allows for comparison of the relative weight each classifier puts on each feature. The head topography plots in the lower panel show the channel-wise sum over time of the absolute scaled sensitivities. The upper panel shows the same scaled sensitivities plotted over time for the <italic>Pz</italic> electrode (indicated as the dark dot on the head topographies). This electrode was chosen as Fr&#x000fc;nd et al. (<xref ref-type="bibr" rid="B8">2008</xref>) made it the subject of most visualizations. The shape of the sensitivity curves nicely resemble the ERP difference wave. Interestingly, for a time window around 350&#x02009;ms after stimulus onset (indicated by the gray bar), all multivariate sensitivity measures assign a considerable amount of weight on the respective timepoints, whereas the univariate ANOVA is completely flat at zero.</p></caption><graphic xlink:href="fninf-03-003-g002"/></fig><p>The data for this analysis were 700&#x02009;ms EEG segments starting 200&#x02009;ms prior to the stimulus onset of each trial, to which we applied the following preprocessing procedure. We only included trials that passed the semi-automatic artifact rejection procedure performed in the original study, yielding 852 trials (422 color and 430 line-art). Each trial timeseries was downsampled to 200&#x02009;Hz, leaving 140 sample points per trial and electrode. We then defined each trial, including the EEG signal of all sample points from all channels, as a sample to be classified (4340 features total). Finally, all features for each sample were normalized to zero mean and unit variance (<italic>z</italic>-scored).</p><p>As the main analysis we applied a standard sixfold cross-validation<xref ref-type="fn" rid="fn22">22</xref> procedure with <italic>linear support vector machine</italic> (linCSVM; Vapnik, <xref ref-type="bibr" rid="B37">1995</xref>), <italic>sparse multinomial logistic regression</italic> (SMLR; Krishnapuram et al., <xref ref-type="bibr" rid="B23">2005</xref>) and <italic>Gaussian process regression</italic> with linear kernel (linGPR; Rasmussen and Williams, <xref ref-type="bibr" rid="B30">2006</xref>) classifiers. Additionally, we computed the multivariate I-RELIEF (Sun, <xref ref-type="bibr" rid="B36">2007</xref>) feature sensitivity measures, and, for comparison, a univariate analysis of variance (ANOVA) <italic>F</italic>-score on the same cross-validation dataset splits.</p><p>All three classifiers performed with high accuracy on the independent test datasets, achieving 86.2% (linCSVM), 91.8% (SMLR), and 89.6% (linGPR) correct single trial predictions, respectively. However, more interesting than the plain accuracy are the features each classifier relied upon to perform its predictions. PyMVPA makes it very easy to extract feature sensitivity information from all its classifiers using a uniform interface. Figure <xref ref-type="fig" rid="F2">2</xref>B shows the computed sensitivities from all classifiers and measures. There is a striking similarity between the shape of the classifier sensitivities plotted over time and the corresponding event-related potential (ERP) difference wave between the two experimental conditions (Figure <xref ref-type="fig" rid="F2">2</xref>A; example shown for electrode <italic>Pz</italic>, Fr&#x000fc;nd et al., <xref ref-type="bibr" rid="B8">2008</xref>). The head topography plot of the sensitivities reveals a high variability with respect to the specificity among the multivariate measures. SVM, GPR and SMLR weights congruently identify three posterior electrodes as being most informative (SMLR weights provide the highest contrast of all measures). The I-RELIEF topography is much less specific and more similar to the ANOVA topography in its global spatial structure than to the other multivariate measures. It should be noted, however, that these topographies aggregate information over all timepoints and, therefore, do not provide information about specific temporal EEG components.</p><p>One particularly interesting result is the difference between the multivariate sensitivities and the univariate ANOVA <italic>F</italic>-scores from 300 to 400&#x02009;ms following stimulus onset. Only the multivariate methods (especially SMLR, linCSVM and linGPR) detected a relevant contribution to the classification task of the signal in this time window. This late signal may be related to the intracranial EEG gamma-band responses that Lachaux et al. (<xref ref-type="bibr" rid="B24">2005</xref>) observed at around the same time range when participants viewed complex stimuli. Given that the present data also seem to show a similar evoked gamma-band response (Fr&#x000fc;nd et al., <xref ref-type="bibr" rid="B8">2008</xref>), it is possible that the multivariate methods are sensitive to the gamma-band activity in the data. Still, further work would be required to prove this correlation.</p></sec><sec><title>MEG</title><p>The example MEG dataset was collected with the aim to test whether it is possible to predict the recognition of briefly presented natural scenes from single trial MEG-recordings of brain activity (Rieger et al., <xref ref-type="bibr" rid="B32">2008</xref>) and to use ML methods to investigate the properties of the brain activity that is predictive of later recognition. On each trial participants saw a briefly presented photograph (37&#x02009;ms) of a natural scene that was immediately followed by a pattern mask (1000&#x02013;1400&#x02009;ms). The short masked presentation effectively limits the processing interval of the scene in the brain (Rieger et al., <xref ref-type="bibr" rid="B31">2005</xref>) and, therefore, participants will later recognize only some of the scenes. After the mask was turned off, participants indicated via button presses whether they would subsequently recognize the photograph, or if they would fail. Immediately after this judgement, four natural scene photographs were presented and participants had to indicate which of the four scenes had been previously presented (i.e., a four-alternative forced-choice delayed match to sample task).</p><p>The MEG was recorded with a 151 channel CTF Omega MEG system from the whole head (sampling rate 625&#x02009;Hz and a 120&#x02009;Hz analogue low pass filter) while participants performed this task. The 600&#x02009;ms interval of the MEG time series data that was used for the analysis started at the onset of the briefly presented scene and ended before the mask was turned off. As in the original study, we analyzed only those trials in which participants both judged they would be correct and also correctly recognized the scene (<italic>RECOG</italic>) and the trials in which participants both predicted they would fail and gave an incorrect response (<italic>NRECOG</italic>). For details about the rationale of this selection, the stimulus presentation information, and the recording procedure see Rieger et al. (<xref ref-type="bibr" rid="B32">2008</xref>). In this example analysis we have used data from a single participant (labeled P1 in the original publication).</p><p>The MEG timeseries were first downsampled to 80&#x02009;Hz and then all trial segments were channel-wise normalized by subtracting their mean baseline signal (determined from a 200&#x02009;ms window prior to scene onset). Only timepoints within the first 600&#x02009;ms after stimulus onset were considered for further analysis. The resulting dataset consisted of 151 channels with 48 timepoints each (7248 features), and a total of 294 samples (233 <italic>RECOG</italic> trials and 61 <italic>NRECOG</italic> trials).</p><p>The original study contained analyses based upon SVM classifiers, which revealed, by means of the spatio-temporal distribution of the sensitivities, that the theta band alone provides the most discriminative signal. The authors also addressed the topic of how to interpret heavily unbalanced datasets<xref ref-type="fn" rid="fn23">23</xref>. Given this comprehensive analysis, we aimed here to replicate their basic analysis strategy with PyMVPA and were able to achieve almost identical results.</p><p>As with the EEG data, we applied a standard cross-validation procedure, this time eightfold, using linear SVM and SMLR classifiers. Additionally, we again computed univariate ANOVA <italic>F</italic>-scores on the same cross-validation dataset splits. The SVM classifier was configured to use different per-class C-values<xref ref-type="fn" rid="fn24">24</xref>, scaled with respect to the number of samples in each class to address the unbalanced number of samples. Similar to Rieger et al. (<xref ref-type="bibr" rid="B32">2008</xref>), we also ran a second cross-validation on balanced datasets (by performing multiple selections of a random subset of samples from the larger <italic>RECOG</italic> category).</p><p>Both, classifiers performed almost identically on the full, unbalanced dataset, achieving 84.69% (SMLR) and 82.31% (linCSVM) correct single trial predictions (83.0% in the original study). Figure <xref ref-type="fig" rid="F3">3</xref> shows sample timeseries of the classifier sensitivities and the ANOVA <italic>F</italic>-score of two posterior channels. Due to the significant difference in the number of samples of each category, it is important to additionally report mean true positive rate (TPR)<xref ref-type="fn" rid="fn25">25</xref>, that amounted to 72% (SMLR), and 76% (linCSVM) respectively. The second SVM classifier trained on the balanced dataset achieved a comparable accuracy of 76.07% correct predictions (mean across 100 subsampled datasets), which is a slightly larger drop in accuracy when compared to the 80.8% achieved in the original study (see Table 3 in Rieger et al., <xref ref-type="bibr" rid="B32">2008</xref>).</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Event-related magnetic fields (EMF) and classifier sensitivities</bold>. The upper part shows EMFs for two exemplary MEG channels. On the left sensor MRO22 (right occipital), and on the right sensor MZO01 (central occipital). The lower part shows classifier sensitivities and ANOVA <italic>F</italic>-scores plotted over time for both sensors. Both classifiers showed equivalent generalization performance of approximately 82% correct single trial predictions.</p></caption><graphic xlink:href="fninf-03-003-g003"/></fig><p>Importantly, these results show that PyMVPA produces reproducible results that depend on the ML methods employed, but not on a particular implementation. However, the integrated framework of PyMVPA allowed us to achieve these results with much less effort than what was necessary in the original study.</p></sec><sec><title>fMRI</title><p>A single participant (participant 1) from a study published by Haxby et al. (<xref ref-type="bibr" rid="B15">2001</xref>), which has been repeatedly reanalyzed since the original publication (Hanson and Halchenko, <xref ref-type="bibr" rid="B14">2008</xref>; Hanson et al., <xref ref-type="bibr" rid="B13">2004</xref>; O'Toole et al., <xref ref-type="bibr" rid="B28">2007</xref>), served as the example fMRI dataset. The dataset itself consists of 12 runs. In each run, the participant passively viewed greyscale images of eight object categories, grouped in 24&#x02009;s blocks separated by rest periods. Each image was shown for 500&#x02009;ms and was followed by a 1500&#x02009;ms inter-stimulus interval. Full-brain fMRI data were recorded with a volume repetition time of 2500&#x02009;ms, thus, a stimulus block was covered by roughly nine volumes. For a complete description of the experimental design and fMRI acquisition parameters see Haxby et al. (<xref ref-type="bibr" rid="B15">2001</xref>).</p><p>First, the raw fMRI data were motion corrected using FLIRT<xref ref-type="fn" rid="fn26">26</xref> from <italic>FSL</italic><xref ref-type="fn" rid="fn27">27</xref> (Jenkinson et al., <xref ref-type="bibr" rid="B19">2002</xref>). All subsequent data processing was done with PyMVPA. After motion correction, linear detrending was performed for each run individually. No additional spatial or temporal filtering was applied.</p><p>For the sake of simplicity, we reduced the dataset to a four-class problem (<italic>faces, houses, cats, and shoes</italic>). All volumes recorded during any of these blocks were extracted and voxel-wise <italic>z</italic>-scored. This normalization was performed individually for each run to prevent any kind of information transfer across runs.</p><p>After preprocessing, we applied the same sensitivity analysis performed for all other data modalities to this dataset. Here, only a SMLR classifier was used (sixfold cross-validation, with 2 of the 12&#x02009;experimental runs grouped into one chunk, and trained on single fMRI volumes that covered the full brain). For comparison, a univariate ANOVA was again computed for the same cross-validation dataset splits.</p><p>The SMLR classifier performed very well on the independent test datasets, correctly predicting the category for 94.7% of all single volume samples in the test datasets. To examine what information was used by the classifier to reach this performance level, we computed ROI-based sensitivity scores for 48 non-overlapping structures defined by the probabilistic Harvard-Oxford cortical atlas (Flitney et al., <xref ref-type="bibr" rid="B7">2007</xref>), as shipped with FSL (Smith et al., <xref ref-type="bibr" rid="B34">2004</xref>). To create the ROIs, we thresholded the probability maps of all structures at 25% and assigned ambiguous voxels to the structure with the higher probability. The resulting map was projected into the space of the functional dataset using an affine transformation and nearest neighbor interpolation.</p><p>In order to determine the contribution of each ROI, the sensitivity vector was first normalized (across all ROIs), so that all absolute sensitivities summed up to 1 (L1-normed). Afterwards ROI-wise scores were computed by taking the sum of all sensitivities in a particular ROI. The upper part of Figure <xref ref-type="fig" rid="F4">4</xref> shows these scores for the 20 highest-scoring and the three lowest-scoring ROIs.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Sensitivity analysis of the four-category fMRI dataset</bold>. The upper part shows the ROI-wise scores computed from SMLR classifier weights and ANOVA <italic>F</italic>-scores (limited to the 20 highest and the three lowest-scoring ROIs). The lower part shows dendrograms with clusters of average category samples (computed using squared Euclidean distances) for voxels with non-zero SMLR-weights and a matching number of voxels with the highest <italic>F</italic>-scores in each ROI.</p></caption><graphic xlink:href="fninf-03-003-g004"/></fig><p>The lower part of the figure shows dendrograms from a hierarchical cluster analysis<xref ref-type="fn" rid="fn28">28</xref> on relevant voxels from a block-averaged variant of the dataset (but otherwise identical to the classifier training data). For SMLR, only voxels with a non-zero sensitivity were considered in each particular ROI. For ANOVA, only the voxels with the highest <italic>F</italic>-scores (limited to the same number as for the SMLR case) were considered. For visualization purposes the dendrograms show the distances and clusters computed from the average samples of each condition in each dataset chunk (i.e., two experimental blocks), yielding six samples per condition.</p><p>The four chosen ROIs clearly show four different cluster patterns. The 92 selected voxels in temporal occipital fusiform cortex (TOFC) show a clear clustering of the experimental categories, with relatively large sample distances between categories. The pattern of the 36 voxels in angular gyrus reveals an animate/inanimate clustering, although with much smaller distances. The largest group of 148 voxels in the frontal pole ROI seems to have no obvious structure in their samples. Despite that, both sensitivity measures assign substantial importance to this region. This might be due to the large inter-sample distances visualized in the corresponding dendrogram in Figure <xref ref-type="fig" rid="F4">4</xref>. Each leaf node (in this case an average volume of two stimulation blocks) is approximately as distinct from any other leaf node, in terms of the employed distance measure, as the semantic clusters identified in the TOFC ROI. Finally, the ROI covering the anterior division of the superior temporal gyrus shows no clustering at all, and, consequently, is among the lowest-scoring ROIs of both measures. On the whole, the cluster patterns from voxels selected by SMLR weights and <italic>F</italic>-scores are very similar in terms of inter-cluster distances.</p><p>Given that these results only include the data of a single participant, no far-reaching implications can be drawn from them. However, the distinct cluster patterns might provide indications for different levels of information encoding that could be addressed in future studies. Although voxels selected in both angular gyrus and the frontal pole ROIs do not provide a discriminative signal for all four stimulus categories, they nevertheless provide some disambiguating information and, thus, are picked up by the classifier. In angular gyrus, this seems to be an animate/inanimate pattern that additionally also differentiates between the two categories of animate stimuli. Finally, in the frontal pole ROI the pattern remains unclear, but the relatively large inter-sample distances indicate a differential code of some form that is not closely related to the semantic stimulus category.</p></sec><sec><title>Extracellular recordings</title><p>The extracellular dataset analyzed in this section is previously unpublished, thus, we first briefly describe the experimental and acquisition setup. Animal experiments were carried out in accordance with the National Institute of Health Guide for the Care and Use of Laboratory Animals and approved by Rutgers University. Sprague-Dawley rats (300&#x02013;500&#x02009;g) were anaesthetized with urethane (1.5&#x02009;g/kg) and held with a custom naso-orbital restraint. After preparing a 3&#x02009;mm square window in the skull over auditory cortex, the dura was removed and a silicon microelectrode consisting of eight four-site recording shanks (NeuroNexus Technologies, Ann Arbor, MI, USA) was inserted. The recording sites were in the primary auditory cortex, estimated by stereotaxic coordinates, vascular structure (Sally and Kelly, <xref ref-type="bibr" rid="B33">1988</xref>) and tonotopic variation of frequency tuning across recording shanks, and located within layer V, determined by electrode depth and firing patterns.</p><p>Five pure tones (3, 7, 12, 20, 30&#x02009;kHz at 60&#x02009;dB) and five different natural sounds (extracted from the CD &#x0201c;Voices of the Swamp&#x0201d;, Naturesound Studio, Ithaca, NY, USA) were used as stimuli. Each stimulus had a duration of 500&#x02009;ms followed by 1500&#x02009;ms of silence. All stimuli were tapered at beginning and end with a 5&#x02009;ms cosine window. The data acquisition took place in a single-walled sound isolation chamber (IAC, Bronx, NY, USA) with sounds presented free field (RP2/ES1, Tucker-Davis, Alachua, FL, USA).</p><p>Individual units<xref ref-type="fn" rid="fn29">29</xref> were isolated by a semi-automatic algorithm (<italic>KlustaKwik</italic><xref ref-type="fn" rid="fn30">30</xref>) followed by manual clustering (<italic>Klusters</italic><xref ref-type="fn" rid="fn31">31</xref>). Post-stimulus time histograms (PSTH) of spike counts per each unit for all 1734 stimulation onsets were estimated using a bin size of 3.2&#x02009;ms. To ensure an accurate estimation of PSTHs only units with a mean firing rate higher than 2&#x02009;Hz were selected for further analysis, leaving us with a total of 105 units.</p><p>Since the segregation of individual units out of the extracellular recordings is carried out without taking the respective stimulus condition into account, i.e., in unsupervised fashion (in ML terminology), it does not guarantee that the activity of any particular unit can be easily attributed to some set of stimulus conditions. From the stimulus-wise descriptive statistics of the units presented in the top plots of Figure <xref ref-type="fig" rid="F5">5</xref> it is difficult to state that the activity of any particular unit at some moment in time is specific for a given stimulus. Furthermore, due to the inter-trial variance in the spike counts, it is even more difficult to reliably assess what stimulus condition any particular trial belongs to. Hence, the purpose of the PyMVPA analysis was to complement the results of the unsupervised clustering with a characterization of all extracted units in terms of their specificity to any given stimulus at any given time.</p><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>Statistics of multiple single unit extracellular simultaneous recordings and corresponding classifier sensitivities</bold>. All plots sweep through different stimuli along vertical axis, with stimuli labels presented in the middle of the plots. The upper part shows basic descriptive statistics of spike counts for each stimulus per each time bin (on the left) and per each unit (on the right). Such statistics seem to lack stimulus specificity for any given category at a given time point or unit. The lower part on the left shows the temporal sensitivity profile of a representative unit for each stimulus. It shows that stimulus specific information in the response can be coded primarily temporally (few specific offsets with maximal sensitivity like for <italic>song2</italic> stimulus) or in a slowly modulated pattern of spikes counts (see 3&#x02009;kHz stimulus). Associated aggregate sensitivities of all units for all stimuli in the lower right figure indicate each unit's specificity to any given stimulus. It provides better specificity than simple statistics like variance, e.g., unit 19 is active in all stimulation conditions according to its high variance, but according to its classifier sensitivity it carries little, if any, stimuli-specific information for natural songs 1&#x02013;3.</p></caption><graphic xlink:href="fninf-03-003-g005"/></fig><p>The analysis pipeline was similar to the one used for EEG, MEG, and fMRI data. We ran a standard eightfold cross-validation procedure for an SMLR classifier, which achieved a mean of 77.57% accuracy estimate across all 10 types of stimuli. This generalization accuracy is well above chance (10%) for all stimulus categories and allows one to conclude that the neuronal population activity pattern at the recording site carries a differential signal across all 10&#x02009;stimuli. Misclassifications mostly occurred for low-frequency stimuli. Pure tones with 3 and 7&#x02009;kHz were more often confused with each other than tones with a larger frequency difference (see Figure <xref ref-type="fig" rid="F6">6</xref>), which suggests a high similarity in the spiking patterns for these stimuli. We could further speculate that this neuronal population is more tuned towards the processing of higher frequency tones.</p><fig id="F6" position="float"><label>Figure 6</label><caption><p><bold>Confusion matrix of SMLR classifier predictions of stimulus conditions from of multiple unit recordings</bold>. The classifier was trained to discriminate between stimuli of five pure tones and five natural sounds. Elements of the matrix (numeric values and color-mapped visualization) show the number of trials which were correctly (diagonal) or incorrectly (off-diagonal) classified by a SMLR classifier during an eightfold cross-validation procedure. The results suggest a high similarity in the spiking patterns for stimuli of low-frequency pure tones, which lead the classifier to confuse them more often, whenever responses to natural sound stimuli and high-frequency tones were hardly ever confused with each other.</p></caption><graphic xlink:href="fninf-03-003-g006"/></fig><p>Besides being able to label yet unseen trials with high accuracy, the trained classifier can readily provide its sensitivity estimates for each unit, time bin, and stimulus condition (see bottom plots of Figure <xref ref-type="fig" rid="F5">5</xref>). Temporal sensitivity profiles of any particular unit (see unit #42 profiles in lower left plot of Figure <xref ref-type="fig" rid="F5">5</xref>) can reveal that the stimulus specific information is contained in spike times relative to stimulus onset or can be represented as slowly modulated pattern of spike counts (see 3&#x02009;kHz stimuli). An aggregate sensitivity (in this case the sum of absolute sensitivities) across all time-bins provides a summary statistic of any unit's sensitivity to a given stimulus condition (see lower right plot of Figure <xref ref-type="fig" rid="F5">5</xref>). In contrast to a simple variance measure, it provides an easier way to associate any given unit to a set of stimulus conditions. Additionally, it can identify units which might lack a substantial amount of variance, but nevertheless carry a stimulius-specific signal (e.g. unit #28 and 30&#x02009;kHz stimulus).</p></sec></sec><sec><title>Conclusions</title><p>In this article we presented PyMVPA, a data analysis framework especially tailored to neural data from a wide range of acquisition modalities. PyMVPA provides ML techniques as core functionality, addressing recent trends in neuroscience research. To illustrate the generalizability of the PyMVPA analysis pipeline we provided example analyses of data from EEG, MEG, fMRI and extracellular recordings.</p><p>The framework presented here is Python-based, sophisticated, free and open-source software. Its intended audience is threefold. First, there are <italic>neuroscience researchers</italic> interested in testing ML algorithms on neural data, e.g., people working on brain-computer interfaces (BCI, see Birbaumer and Cohen, <xref ref-type="bibr" rid="B2">2007</xref>; Lebedev and Nicolelis, <xref ref-type="bibr" rid="B25">2006</xref>). PyMVPA provides researchers with the ability to execute complex analysis tasks in very concise code. Second, it is also designed for <italic>ML researchers</italic> interested in testing new ML algorithms on neural data. PyMVPA offers a highly-modularized architecture designed to minimize the effort of adding new algorithms. Moreover, the availability of neuroscience-related code-examples (like the ones presented in this article) and datasets greatly reduces the time to get actual results. Finally, PyMVPA is welcoming <italic>code contributors</italic> from both neuroscience and ML communities interested in improving or adding modality-specific functions or new algorithms. PyMVPA offers a community-based development model together with a distributed version control system and extensive reference documentation.</p><sec><title>Future work</title><p>PyMVPA does not aim to provide all possible ML analysis algorithms, and it will likely not come close, even in the future. Given that PyMVPA is tailored towards the high-dimensional problems found in neuroscience, it currently provides many of the most common algorithms tuned for this target. Still, as the neuroscience and ML communities unite, new and promising algorithms are constantly emerging and being added to PyMVPA. Beyond the inclusion of new ML algorithms, there are numerous plans for future enhancements to PyMVPA.</p><p>Because the current use of ML techniques in neuroscience is mainly limited to the application of only basic algorithms to neural data, one of the next, most intriguing, new directions of PyMVPA will be to provide <italic>custom</italic> workflows designed for specific neuroscience modalities. An example of such a custom workflow is the analysis of fMRI data from experiments with event-related designs, where multiple fMRI volumes after the onset of the event compose a single sample within a dataset provided to the ML methods for processing. Combining multiple volumes into a single sample obviates the need to provide a hemodynamic response function because the important features can be extracted independently for each voxel.</p><p>In addition, PyMVPA has yet to confront the problem of model selection. Currently, only Gaussian process regression has the ability to select hyper-parameters of the model. Uniform model selection for ML methods within PyMVPA is planned for the next major release of the project. It will provide the facility to automatically search for the best set of parameters for each classifier without sacrificing unbiased estimates of the generalization performance.</p></sec></sec><sec sec-type="supplementary-material"><title>Supplemental Material</title><p>The Supplemental Materials (e.g., source code) for this article can be found online at <uri xlink:type="simple" xlink:href="http://www.frontiersin.org/neuroinformatics/">http://www.frontiersin.org/neuroinformatics/paper/10.3389/neuro.11/003.2009</uri>.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><ack><p>We are thankful to Dr. Artur Luczak and Dr. Kenneth D. Harris (CMBN, Rutgers University, Newark, NJ, USA) for providing the extracellular recordings dataset for the paper. Michael Hanke was supported by the German Academic Exchange Service (grant: PPP-USA D/05/504/7). Per Sederberg was supported by National Institutes of Health NRSA (grant: MH080526). Yaroslav O. Halchenko and Dr. Stephen J. Hanson were supported by the National Science Foundation (grant: SBE 0751008) and the James McDonnell Foundation (grant: 220020127). Stefan Pollmann and Jochem W. Rieger were supported by Deutsche Forschungsgemeinschaft (PO 548/6-1 and RI 1511/1-3 respectively).</p></ack><ref-list><title>References</title><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Beckmann</surname><given-names>C. F.</given-names></name><name><surname>Smith</surname><given-names>S. M.</given-names></name></person-group> (<year>2005</year>). <article-title>Tensorial extensions of independent component analysis for multisubject fMRI analysis</article-title>. <source>Neuroimage</source><volume>25</volume>, <fpage>294</fpage>&#x02013;<lpage>311</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.10.043</pub-id><pub-id pub-id-type="pmid">15734364</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Birbaumer</surname><given-names>N.</given-names></name><name><surname>Cohen</surname><given-names>L. G.</given-names></name></person-group> (<year>2007</year>). <article-title>Brain-computer interfaces: communication and restoration of movement in paralysis</article-title>. <source>J. Physiol.</source><volume>579</volume>, <fpage>621</fpage>&#x02013;<lpage>636</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2006.125633</pub-id><pub-id pub-id-type="pmid">17234696</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Busch</surname><given-names>N. A.</given-names></name><name><surname>Herrmann</surname><given-names>C. S.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>M. M.</given-names></name><name><surname>Lenz</surname><given-names>D.</given-names></name><name><surname>Gruber</surname><given-names>T.</given-names></name></person-group> (<year>2006</year>). <article-title>A cross-laboratory study of event-related gamma activity in a standard object recognition paradigm</article-title>. <source>Neuroimage</source><volume>33</volume>, <fpage>1169</fpage>&#x02013;<lpage>1177</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.07.034</pub-id><pub-id pub-id-type="pmid">17023180</pub-id></citation></ref><ref id="B4"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Detre</surname><given-names>G.</given-names></name><name><surname>Polyn</surname><given-names>S. M.</given-names></name><name><surname>Moore</surname><given-names>C.</given-names></name><name><surname>Natu</surname><given-names>V.</given-names></name><name><surname>Singer</surname><given-names>B.</given-names></name><name><surname>Cohen</surname><given-names>J.</given-names></name><name><surname>Haxby</surname><given-names>J. V.</given-names></name><name><surname>Norman</surname><given-names>K. A.</given-names></name></person-group> (<year>2006</year>). <article-title>The Multi-Voxel Pattern Analysis (MVPA) Toolbox</article-title>. Poster presented at the Annual Meeting of the Organization for Human Brain Mapping (Florence, Italy). Available at: <uri xlink:type="simple" xlink:href="http://www.csbmb.princeton.edu/mvpa">http://www.csbmb.princeton.edu/mvpa</uri></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Eads</surname><given-names>D.</given-names></name></person-group> (<year>2008</year>). <article-title>Hcluster: Hierarchical Clustering for SciPy</article-title>. Available at: <uri xlink:type="simple" xlink:href="http://scipy-cluster.googlecode.com/">http://scipy-cluster.googlecode.com/</uri></citation></ref><ref id="B6"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Efron</surname><given-names>B.</given-names></name><name><surname>Tibshirani</surname><given-names>R.</given-names></name></person-group> (<year>1993</year>). <article-title>An Introduction to the Bootstrap</article-title>. <publisher-loc>New York, NY</publisher-loc>, <publisher-name>Chapman &#x00026; Hall/CRC</publisher-name></citation></ref><ref id="B7"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Flitney</surname><given-names>D.</given-names></name><name><surname>Webster</surname><given-names>M.</given-names></name><name><surname>Patenaude</surname><given-names>B.</given-names></name><name><surname>Seidman</surname><given-names>L.</given-names></name><name><surname>Goldstein</surname><given-names>J.</given-names></name><name><surname>Tordesillas Gutierrez</surname><given-names>D.</given-names></name><name><surname>Eickhoff</surname><given-names>S.</given-names></name><name><surname>Amunts</surname><given-names>K.</given-names></name><name><surname>Zilles</surname><given-names>K.</given-names></name><name><surname>Lancaster</surname><given-names>J.</given-names></name><name><surname>Haselgrove</surname><given-names>C.</given-names></name><name><surname>Kennedy</surname><given-names>D.</given-names></name><name><surname>Jenkinson</surname><given-names>M.</given-names></name><name><surname>Smith</surname><given-names>S.</given-names></name></person-group> (<year>2007</year>). <article-title>Anatomical Brain Atlases and Their Application in the FSLView Visualisation Tool</article-title>. Thirteenth Annual Meeting of the Organization for Human Brain Mapping. Chicago, IL, USA.</citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Fr&#x000fc;nd</surname><given-names>I.</given-names></name><name><surname>Busch</surname><given-names>N. A.</given-names></name><name><surname>Schadow</surname><given-names>J.</given-names></name><name><surname>Gruber</surname><given-names>T.</given-names></name><name><surname>K&#x000f6;rner</surname><given-names>U.</given-names></name><name><surname>Herrmann</surname><given-names>C. S.</given-names></name></person-group> (<year>2008</year>). <article-title>Time pressure modulates electrophysiological correlates of early visual processing</article-title>. <source>PLoS ONE</source><volume>3</volume>, <fpage>e1675</fpage><pub-id pub-id-type="pmid">18301752</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Guyon</surname><given-names>I.</given-names></name><name><surname>Elisseeff</surname><given-names>A.</given-names></name></person-group> (<year>2003</year>). <article-title>An introduction to variable and feature selection</article-title>. <source>J. Mach. Learn. Res.</source><volume>3</volume>, <fpage>1157</fpage>&#x02013;<lpage>1182</lpage><pub-id pub-id-type="doi">10.1162/153244303322753616</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Guyon</surname><given-names>I.</given-names></name><name><surname>Weston</surname><given-names>J.</given-names></name><name><surname>Barnhill</surname><given-names>S.</given-names></name><name><surname>Vapnik</surname><given-names>V.</given-names></name></person-group> (<year>2002</year>). <article-title>Gene selection for cancer classification using support vector machines</article-title>. <source>Mach. Learn.</source><volume>46</volume>, <fpage>389</fpage>&#x02013;<lpage>422</lpage><pub-id pub-id-type="doi">10.1023/A:1012487302797</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hanke</surname><given-names>M.</given-names></name><name><surname>Halchenko</surname><given-names>Y. O.</given-names></name><name><surname>Sederberg</surname><given-names>P. B.</given-names></name><name><surname>Hanson</surname><given-names>S. J.</given-names></name><name><surname>Haxby</surname><given-names>J. V.</given-names></name><name><surname>Pollmann</surname><given-names>S.</given-names></name></person-group> (<year>2009</year>). <article-title>PyMVPA: A Python toolbox for multivariate pattern analysis of fMRI data</article-title>. <source>Neuroinformatics.</source><pub-id pub-id-type="doi">10.1007/s12021-008-9041-y</pub-id><pub-id pub-id-type="pmid">19184561</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hanke</surname><given-names>M.</given-names></name><name><surname>Halchenko</surname><given-names>Y. O.</given-names></name><name><surname>Sederberg</surname><given-names>P. B.</given-names></name><name><surname>Hughes</surname><given-names>J. M.</given-names></name></person-group> (<year>2008</year>). <article-title>The PyMVPA Manual</article-title>. Available at: <uri xlink:type="simple" xlink:href="http://www.pymvpa.org/PyMVPA-Manual.pdf">http://www.pymvpa.org/PyMVPA-Manual.pdf</uri></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hanson</surname><given-names>S.</given-names></name><name><surname>Matsuka</surname><given-names>T.</given-names></name><name><surname>Haxby</surname><given-names>J.</given-names></name></person-group> (<year>2004</year>). <article-title>Combinatorial codes in ventral temporal lobe for object recognition: Haxby (2001). revisited: is there a &#x0201c;face&#x0201d; area?</article-title><source>Neuroimage</source><volume>23</volume>, <fpage>156</fpage>&#x02013;<lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.05.020</pub-id><pub-id pub-id-type="pmid">15325362</pub-id></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hanson</surname><given-names>S. J.</given-names></name><name><surname>Halchenko</surname><given-names>Y. O.</given-names></name></person-group> (<year>2008</year>). <article-title>Brain reading using full brain support vector machines for object recognition: there is no &#x0201c;face&#x0201d; identification area</article-title>. <source>Neural Comput.</source><volume>20</volume>, <fpage>486</fpage>&#x02013;<lpage>503</lpage><pub-id pub-id-type="doi">10.1162/neco.2007.09-06-340</pub-id><pub-id pub-id-type="pmid">18047411</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>J.</given-names></name><name><surname>Gobbini</surname><given-names>M.</given-names></name><name><surname>Furey</surname><given-names>M.</given-names></name><name><surname>Ishai</surname><given-names>A.</given-names></name><name><surname>Schouten</surname><given-names>J.</given-names></name><name><surname>Pietrini</surname><given-names>P.</given-names></name></person-group> (<year>2001</year>). <article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title>. <source>Science</source><volume>293</volume>, <fpage>2425</fpage>&#x02013;<lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id><pub-id pub-id-type="pmid">11577229</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>J.-D.</given-names></name><name><surname>Rees</surname><given-names>G.</given-names></name></person-group> (<year>2006</year>). <article-title>Decoding mental states from brain activity in humans</article-title>. <source>Nat. Rev. Neurosci.</source><volume>7</volume>, <fpage>523</fpage>&#x02013;<lpage>534</lpage><pub-id pub-id-type="doi">10.1038/nrn1931</pub-id><pub-id pub-id-type="pmid">16791142</pub-id></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>J.-D.</given-names></name><name><surname>Sakai</surname><given-names>K.</given-names></name><name><surname>Rees</surname><given-names>G.</given-names></name><name><surname>Gilbert</surname><given-names>S.</given-names></name><name><surname>Frith</surname><given-names>C.</given-names></name><name><surname>Passingham</surname><given-names>R. E.</given-names></name></person-group> (<year>2007</year>). <article-title>Reading hidden intentions in the human brain</article-title>. <source>Curr. Biol.</source><volume>17</volume>, <fpage>323</fpage>&#x02013;<lpage>328</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.11.072</pub-id><pub-id pub-id-type="pmid">17291759</pub-id></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Herrmann</surname><given-names>C. S.</given-names></name><name><surname>Lenz</surname><given-names>D.</given-names></name><name><surname>Junge</surname><given-names>S.</given-names></name><name><surname>Busch</surname><given-names>N. A.</given-names></name><name><surname>Maess</surname><given-names>B.</given-names></name></person-group> (<year>2004</year>). <article-title>Memory-matches evoke human gamma-responses</article-title>. <source>BMC Neurosci.</source><volume>5</volume>, <fpage>13</fpage><pub-id pub-id-type="doi">10.1186/1471-2202-5-13</pub-id><pub-id pub-id-type="pmid">15084225</pub-id></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M.</given-names></name><name><surname>Bannister</surname><given-names>P.</given-names></name><name><surname>Brady</surname><given-names>J.</given-names></name><name><surname>Smith</surname><given-names>S.</given-names></name></person-group> (<year>2002</year>). <article-title>Improved optimisation for the robust and accurate linear registration and motion correction of brain images</article-title>. <source>Neuroimage</source><volume>17</volume>, <fpage>825</fpage>&#x02013;<lpage>841</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(02)91132-8</pub-id><pub-id pub-id-type="pmid">12377157</pub-id></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kamitani</surname><given-names>Y.</given-names></name><name><surname>Tong</surname><given-names>F.</given-names></name></person-group> (<year>2005</year>). <article-title>Decoding the visual and subjective contents of the human brain</article-title>. <source>Nat. Neurosci.</source><volume>8</volume>, <fpage>679</fpage>&#x02013;<lpage>685</lpage><pub-id pub-id-type="doi">10.1038/nn1444</pub-id><pub-id pub-id-type="pmid">15852014</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N.</given-names></name><name><surname>Bandettini</surname><given-names>P.</given-names></name></person-group> (<year>2007</year>). <article-title>Analyzing for information, not activation, to exploit high-resolution fMRI</article-title>. <source>Neuroimage</source><volume>38</volume>, <fpage>649</fpage>&#x02013;<lpage>662</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.02.022</pub-id><pub-id pub-id-type="pmid">17804260</pub-id></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N.</given-names></name><name><surname>Goebel</surname><given-names>R.</given-names></name><name><surname>Bandettini</surname><given-names>P.</given-names></name></person-group> (<year>2006</year>). <article-title>Information-based functional brain mapping</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><volume>103</volume>, <fpage>3863</fpage>&#x02013;<lpage>3868</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id><pub-id pub-id-type="pmid">16537458</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Krishnapuram</surname><given-names>B.</given-names></name><name><surname>Carin</surname><given-names>L.</given-names></name><name><surname>Figueiredo</surname><given-names>M. A.</given-names></name><name><surname>Hartemink</surname><given-names>A. J.</given-names></name></person-group> (<year>2005</year>). <article-title>Sparse multinomial logistic regression: fast algorithms and generalization bounds</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source><volume>27</volume>, <fpage>957</fpage>&#x02013;<lpage>968</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2005.127</pub-id><pub-id pub-id-type="pmid">15943426</pub-id></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lachaux</surname><given-names> J.-P.</given-names></name><name><surname>George</surname><given-names>N.</given-names></name><name><surname>Tallon-Baudry</surname><given-names>C.</given-names></name><name><surname>Martinerie</surname><given-names>J.</given-names></name><name><surname>Hugueville</surname><given-names>L.</given-names></name><name><surname>Minotti</surname><given-names>L.</given-names></name><name><surname>Kahane</surname><given-names>P.</given-names></name><name><surname>Renault</surname><given-names>B.</given-names></name></person-group> (<year>2005</year>). <article-title>The many faces of the gamma band response to complex visual stimuli</article-title>. <source>Neuroimage</source><volume>25</volume>, <fpage>491</fpage>&#x02013;<lpage>501</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.11.052</pub-id><pub-id pub-id-type="pmid">15784428</pub-id></citation></ref><ref id="B25"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lebedev</surname><given-names>M. A.</given-names></name><name><surname>Nicolelis</surname><given-names>M. A. L.</given-names></name></person-group> (<year>2006</year>). <article-title>Brain-machine interfaces: past, present and future</article-title>. <source>Trends Neurosci.</source><volume>29</volume>, <fpage>536</fpage>&#x02013;<lpage>546</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2006.07.004</pub-id><pub-id pub-id-type="pmid">16859758</pub-id></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Makeig</surname><given-names>S.</given-names></name><name><surname>Debener</surname><given-names>S.</given-names></name><name><surname>Onton</surname><given-names>J.</given-names></name><name><surname>Delorme</surname><given-names>A.</given-names></name></person-group> (<year>2004</year>). <article-title>Mining event-related brain dynamics</article-title>. <source>Trends Cogn. Sci.</source><volume>8</volume>, <fpage>204</fpage>&#x02013;<lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.03.008</pub-id><pub-id pub-id-type="pmid">15120678</pub-id></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>K. A.</given-names></name><name><surname>Polyn</surname><given-names>S. M.</given-names></name><name><surname>Detre</surname><given-names>G. J.</given-names></name><name><surname>Haxby</surname><given-names>J. V.</given-names></name></person-group> (<year>2006</year>). <article-title>Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title>. <source>Trends Cogn. Sci.</source><volume>10</volume>, <fpage>424</fpage>&#x02013;<lpage>430</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.07.005</pub-id><pub-id pub-id-type="pmid">16899397</pub-id></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>O'Toole</surname><given-names>A. J.</given-names></name><name><surname>Jiang</surname><given-names>F.</given-names></name><name><surname>Abdi</surname><given-names>H.</given-names></name><name><surname>Penard</surname><given-names>N.</given-names></name><name><surname>Dunlop</surname><given-names>J. P.</given-names></name><name><surname>Parent</surname><given-names>M. A.</given-names></name></person-group> (<year>2007</year>). <article-title>Theoretical, statistical, and practical perspectives on pattern-based classification approaches to the analysis of functional neuroimaging data</article-title>. <source>J. Cogn. Neurosci.</source><volume>19</volume>, <fpage>1735</fpage>&#x02013;<lpage>1752</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.11.1735</pub-id><pub-id pub-id-type="pmid">17958478</pub-id></citation></ref><ref id="B29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>F.</given-names></name><name><surname>Mitchell</surname><given-names>T.</given-names></name><name><surname>Botvinick</surname><given-names>M.</given-names></name></person-group> (in press). <article-title>Machine learning classifiers and fMRI: a tutorial overview</article-title>.<pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.11.007</pub-id></citation></ref><ref id="B30"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Rasmussen</surname><given-names>C. E.</given-names></name><name><surname>Williams</surname><given-names>C. K.</given-names></name></person-group> (<year>2006</year>). <article-title>Gaussian Processes for Machine Learning</article-title>. <publisher-loc>Cambridge, MA</publisher-loc>, <publisher-name>MIT Press</publisher-name></citation></ref><ref id="B31"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rieger</surname><given-names>J. W.</given-names></name><name><surname>Braun</surname><given-names>C.</given-names></name><name><surname>B&#x000fc;lthoff</surname><given-names>H. H.</given-names></name><name><surname>Gegenfurtner</surname><given-names>K. R.</given-names></name></person-group> (<year>2005</year>). <article-title>The dynamics of visual pattern masking in natural scene processing: a magnetoencephalography study</article-title>. <source>J. Vis.</source><volume>5</volume>, <fpage>275</fpage>&#x02013;<lpage>286</lpage><pub-id pub-id-type="doi">10.1167/5.3.10</pub-id><pub-id pub-id-type="pmid">15929651</pub-id></citation></ref><ref id="B32"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rieger</surname><given-names>J. W.</given-names></name><name><surname>Reichert</surname><given-names>C.</given-names></name><name><surname>Gegenfurtner</surname><given-names>K. R.</given-names></name><name><surname>Noesselt</surname><given-names>T.</given-names></name><name><surname>Braun</surname><given-names>C.</given-names></name><name><surname>Heinze</surname><given-names>H.-J.</given-names></name><name><surname>Kruse</surname><given-names>R.</given-names></name><name><surname>Hinrichs</surname><given-names>H.</given-names></name></person-group> (<year>2008</year>). <article-title>Predicting the recognition of natural scenes from single trial MEG recordings of brain activity</article-title>. <source>Neuroimage</source><volume>42</volume>, <fpage>1056</fpage>&#x02013;<lpage>1068</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.06.014</pub-id><pub-id pub-id-type="pmid">18620063</pub-id></citation></ref><ref id="B33"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sally</surname><given-names>S. L.</given-names></name><name><surname>Kelly</surname><given-names>J. B.</given-names></name></person-group> (<year>1988</year>). <article-title>Organization of auditory cortex in the albino rat: sound frequency</article-title>. <source>J. Neurophysiol.</source><volume>59</volume>, <fpage>1627</fpage>&#x02013;<lpage>1638</lpage><pub-id pub-id-type="pmid">3385476</pub-id></citation></ref><ref id="B34"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>S. M.</given-names></name><name><surname>Jenkinson</surname><given-names>M.</given-names></name><name><surname>Woolrich</surname><given-names>M. W.</given-names></name><name><surname>Beckmann</surname><given-names>C. F.</given-names></name><name><surname>Behrens</surname><given-names>T. E. J.</given-names></name><name><surname>Johansen-Berg</surname><given-names>H.</given-names></name><name><surname>Bannister</surname><given-names>P. R.</given-names></name><name><surname>De Luca</surname><given-names>M.</given-names></name><name><surname>Drobnjak</surname><given-names>I.</given-names></name><name><surname>Flitney</surname><given-names>D. E.</given-names></name><name><surname>Niazy</surname><given-names>R. K.</given-names></name><name><surname>Saunders</surname><given-names>J.</given-names></name><name><surname>Vickers</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>De Stefano</surname><given-names>N.</given-names></name><name><surname>Brady</surname><given-names>J. M.</given-names></name><name><surname>Matthews</surname><given-names>P. M.</given-names></name></person-group> (<year>2004</year>). <article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title>. <source>Neuroimage</source><volume>23</volume>, <fpage>208</fpage>&#x02013;<lpage>219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id></citation></ref><ref id="B35"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sonnenburg</surname><given-names>S.</given-names></name><name><surname>Braun</surname><given-names>M.</given-names></name><name><surname>Ong</surname><given-names>C. S.</given-names></name><name><surname>Bengio</surname><given-names>S.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name><name><surname>Holmes</surname><given-names>G.</given-names></name><name><surname>LeCun</surname><given-names>Y.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>K.-R.</given-names></name><name><surname>Pereira</surname><given-names>F.</given-names></name><name><surname>Rasmussen</surname><given-names>C. E.</given-names></name><name><surname>R&#x000e4;tsch</surname><given-names>G.</given-names></name><name><surname>Sch&#x000f6;lkopf</surname><given-names>B.</given-names></name><name><surname>Smola</surname><given-names>A.</given-names></name><name><surname>Vincent</surname><given-names>P.</given-names></name><name><surname>Weston</surname><given-names>J.</given-names></name><name><surname>Williamson</surname><given-names>R.</given-names></name></person-group> (<year>2007</year>). <article-title>The need for open source software in machine learning</article-title>. <source>J. Mach. Learn. Res.</source><volume>8</volume>, <fpage>2443</fpage>&#x02013;<lpage>2466</lpage></citation></ref><ref id="B36"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Y.</given-names></name></person-group> (<year>2007</year>). <article-title>Iterative RELIEF for feature weighting: algorithms, theories and applications</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source><volume>29</volume>, <fpage>1035</fpage>&#x02013;<lpage>1051</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2007.1093</pub-id><pub-id pub-id-type="pmid">17431301</pub-id></citation></ref><ref id="B37"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>V.</given-names></name></person-group> (<year>1995</year>). <article-title>The Nature of Statistical Learning Theory</article-title>. <publisher-loc>New York,</publisher-loc><publisher-name>Springer</publisher-name></citation></ref><ref id="B38"><citation citation-type="confproc"><person-group person-group-type="author"><name><surname>Veropoulos</surname><given-names>K.</given-names></name><name><surname>Campbell</surname><given-names>C.</given-names></name><name><surname>Cristianini</surname><given-names>N.</given-names></name></person-group> (<year>1999</year>). <article-title>Controlling the Sensitivity of Support Vector Machines</article-title>. <conf-name>Proceedings of the International Joint Conference on AI</conf-name> <conf-loc>Stockholm, Sweden</conf-loc></citation></ref></ref-list><fn-group><fn id="fn1"><p><sup>1</sup>Electroencephalography.</p></fn><fn id="fn2"><p><sup>2</sup>Magnetoencephalography.</p></fn><fn id="fn3"><p><sup>3</sup>Magnetic resonance imaging.</p></fn><fn id="fn4"><p><sup>4</sup>Positron emission tomography.</p></fn><fn id="fn5"><p><sup>5</sup><uri xlink:type="simple" xlink:href="http://www.mloss.org">http://www.mloss.org</uri>.</p></fn><fn id="fn6"><p><sup>6</sup>Closed source commercial product of MathWorks<sup>&#x000ae;</sup>.</p></fn><fn id="fn7"><p><sup>7</sup>It is possible to use the low-level functions of this toolbox for other modalities.</p></fn><fn id="fn8"><p><sup>8</sup><uri xlink:type="simple" xlink:href="http://numpy.scipy.org">http://numpy.scipy.org</uri>.</p></fn><fn id="fn9"><p><sup>9</sup><uri xlink:type="simple" xlink:href="http://www.scipy.org">http://www.scipy.org</uri>.</p></fn><fn id="fn10"><p><sup>10</sup>e.g., ctypes, SWIG, SIP, Cython.</p></fn><fn id="fn11"><p><sup>11</sup>e.g., mlabwrap and RPy.</p></fn><fn id="fn12"><p><sup>12</sup><uri xlink:type="simple" xlink:href="http://www.pymvpa.org">http://www.pymvpa.org</uri>.</p></fn><fn id="fn13"><p><sup>13</sup><uri xlink:type="simple" xlink:href="http://mdp-toolkit.sourceforge.net">http://mdp-toolkit.sourceforge.net</uri>.</p></fn><fn id="fn14"><p><sup>14</sup><uri xlink:type="simple" xlink:href="http://code.google.com/p/scipy-cluster/">http://code.google.com/p/scipy-cluster/</uri>.</p></fn><fn id="fn15"><p><sup>15</sup>PyMVPA is distributed under an MIT license, which complies with both Free Software and Open Source definitions.</p></fn><fn id="fn16"><p><sup>16</sup><uri xlink:type="simple" xlink:href="http://www.csie.ntu.edu.tw/ &#x0223c;cjlin/libsvm/">http://www.csie.ntu.edu.tw/ &#x0223c;cjlin/libsvm/</uri>.</p></fn><fn id="fn17"><p><sup>17</sup><uri xlink:type="simple" xlink:href="http://www.r-project.org">http://www.r-project.org</uri>.</p></fn><fn id="fn18"><p><sup>18</sup><uri xlink:type="simple" xlink:href="http://en.wikipedia.org/wiki/Version_control_system">http://en.wikipedia.org/wiki/Version_control_system</uri>.</p></fn><fn id="fn19"><p><sup>19</sup><uri xlink:type="simple" xlink:href="http://en.wikipedia.org/wiki/ReStructuredText">http://en.wikipedia.org/wiki/ReStructuredText</uri>.</p></fn><fn id="fn20"><p><sup>20</sup><uri xlink:type="simple" xlink:href="http://www.ohloh.net/projects/pymvpa/factoids">http://www.ohloh.net/projects/pymvpa/factoids</uri>.</p></fn><fn id="fn21"><p><sup>21</sup><uri xlink:type="simple" xlink:href="http://www.logilab.org/projects/pylint">http://www.logilab.org/projects/pylint</uri>.</p></fn><fn id="fn22"><p><sup>22</sup><uri xlink:type="simple" xlink:href="http://en.wikipedia.org/wiki/Cross-validation#K-fold_cross-validation">http://en.wikipedia.org/wiki/Cross-validation#K-fold_cross-validation</uri>.</p></fn><fn id="fn23"><p><sup>23</sup>Unbalanced datasets have a dominant category which has considerably more samples than any other category. That potentially leads to the problem when a classifier prefers to assign the label of that category to all samples to minimize total prediction error.</p></fn><fn id="fn24"><p><sup>24</sup>Parameter <italic>C</italic> in soft-margin SVM controls a trade-off between width of the SVM margin and number of support vectors (see Veropoulos et al., <xref ref-type="bibr" rid="B38">1999</xref>, for an evaluation of this approach).</p></fn><fn id="fn25"><p><sup>25</sup>Mean TPR is equivalent to accuracy in balanced sets, and is 50% at chance performance even with unbalanced set sizes (see Rieger et al., <xref ref-type="bibr" rid="B32">2008</xref>, for a discussion of this point).</p></fn><fn id="fn26"><p><sup>26</sup>FMRIB's Linear Image Registration Tool.</p></fn><fn id="fn27"><p><sup>27</sup><uri xlink:type="simple" xlink:href="http://www.fmrib.ox.ac.uk/fsl">http://www.fmrib.ox.ac.uk/fsl</uri>.</p></fn><fn id="fn28"><p><sup>28</sup>PyMVPA provides hierarchical clustering facilities through <italic>hcluster</italic> (Eads, <xref ref-type="bibr" rid="B5">2008</xref>).</p></fn><fn id="fn29"><p><sup>29</sup>The term &#x0201c;unit&#x0201d; in the text refers to a single entity, which was segregated from the recorded data, and is expected to represent a single neuron.</p></fn><fn id="fn30"><p><sup>30</sup><uri xlink:type="simple" xlink:href="http://klustakwik.sourceforge.net">http://klustakwik.sourceforge.net</uri>.</p></fn><fn id="fn31"><p><sup>31</sup><uri xlink:type="simple" xlink:href="http://klusters.sourceforge.net">http://klusters.sourceforge.net</uri>.</p></fn></fn-group></back></article>