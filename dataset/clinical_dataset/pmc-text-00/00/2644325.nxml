<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19063730</article-id><article-id pub-id-type="pmc">2644325</article-id><article-id pub-id-type="publisher-id">1471-2105-9-525</article-id><article-id pub-id-type="doi">10.1186/1471-2105-9-525</article-id><article-categories><subj-group subj-group-type="heading"><subject>Methodology Article</subject></subj-group></article-categories><title-group><article-title>Multi-label literature classification based on the Gene Ontology graph</article-title></title-group><contrib-group><contrib id="A1" contrib-type="author"><name><surname>Jin</surname><given-names>Bo</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>jinbo@musc.edu </email></contrib><contrib id="A2" contrib-type="author"><name><surname>Muller</surname><given-names>Brian</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>mullerb@musc.edu</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Zhai</surname><given-names>Chengxiang</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>czhai@cs.uiuc.edu</email></contrib><contrib id="A4" corresp="yes" contrib-type="author"><name><surname>Lu</surname><given-names>Xinghua</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>lux@musc.edu</email></contrib></contrib-group><aff id="I1"><label>1</label>Department of Biostatistics, Bioinformatics and Epidemiology, Medical University of South Carolina, 135 Cannon Street, Charleston, SC 29425, USA</aff><aff id="I2"><label>2</label>Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA</aff><pub-date pub-type="collection"><year>2008</year></pub-date><pub-date pub-type="epub"><day>8</day><month>12</month><year>2008</year></pub-date><volume>9</volume><fpage>525</fpage><lpage>525</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/9/525"/><history><date date-type="received"><day>28</day><month>7</month><year>2008</year></date><date date-type="accepted"><day>8</day><month>12</month><year>2008</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2008 Jin et al; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2008</copyright-year><copyright-holder>Jin et al; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>               Jin               Bo                              jinbo@musc.edu             </dc:author><dc:title>            Multi-label literature classification based on the Gene Ontology graph         </dc:title><dc:date>2008</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 9(1): 525-. (2008)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2008)9:1&#x0003c;525&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>The Gene Ontology is a controlled vocabulary for representing knowledge related to genes and proteins in a computable form. The current effort of manually annotating proteins with the Gene Ontology is outpaced by the rate of accumulation of biomedical knowledge in literature, which urges the development of text mining approaches to facilitate the process by automatically extracting the Gene Ontology annotation from literature. The task is usually cast as a text classification problem, and contemporary methods are confronted with unbalanced training data and the difficulties associated with multi-label classification.</p></sec><sec><title>Results</title><p>In this research, we investigated the methods of enhancing automatic multi-label classification of biomedical literature by utilizing the structure of the Gene Ontology graph. We have studied three graph-based multi-label classification algorithms, including a novel stochastic algorithm and two top-down hierarchical classification methods for multi-label literature classification. We systematically evaluated and compared these graph-based classification algorithms to a conventional flat multi-label algorithm. The results indicate that, through utilizing the information from the structure of the Gene Ontology graph, the graph-based multi-label classification methods can significantly improve predictions of the Gene Ontology terms implied by the analyzed text. Furthermore, the graph-based multi-label classifiers are capable of suggesting Gene Ontology annotations (to curators) that are closely related to the true annotations even if they fail to predict the true ones directly. A software package implementing the studied algorithms is available for the research community.</p></sec><sec><title>Conclusion</title><p>Through utilizing the information from the structure of the Gene Ontology graph, the graph-based multi-label classification methods have better potential than the conventional flat multi-label classification approach to facilitate protein annotation based on the literature.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>A thrust in bioinformatics is to acquire and transform contemporary knowledge from biomedical literature into computable forms, so that computers can be used to efficiently organize, retrieve and discover the knowledge. The Gene Ontology (GO) [<xref ref-type="bibr" rid="B1">1</xref>] is a controlled vocabulary used to represent molecular biology concepts, which is the <italic>de facto </italic>standard for annotating genes/proteins. The concepts in GO, referred to as GO terms, are organized in directed acyclic graphs (DAGs) to reflect hierarchical relationships among concepts. Currently, the process of extracting biological concepts from biomedical literature to annotate genes/proteins is manually performed by domain experts, whose roles are indispensable to ensure the accuracy of the acquired knowledge. However, the rate of manual annotation is outpaced by the growth of information in the biomedical literature [<xref ref-type="bibr" rid="B2">2</xref>]. Automatically performing literature-based GO annotation has drawn wide attention from the biomedical text mining community [<xref ref-type="bibr" rid="B3">3</xref>-<xref ref-type="bibr" rid="B8">8</xref>]. In addition to numerous publications by individual researchers, a special track was devoted to the task in the BioCreative conference in the form of a challenge from the biomedical text mining community [<xref ref-type="bibr" rid="B3">3</xref>]. Similar tasks were also investigated in the genomic track of the Text REtrieval Conference (TREC) [<xref ref-type="bibr" rid="B4">4</xref>].</p><p>Generally, the task of GO annotation based on free text of the literature can be cast as a text classification problem. Given a protein and the literature associated with it, one can potentially annotate the protein according to the classification (labeling) of the literature, for which various supervised classifiers can be trained, with the GO terms as target classes and the tokens in the training texts as input features. Due to the hierarchical nature of the GO concepts, GO annotation is also intrinsically a multi-label classification problem in that, when a protein is annotated with a GO term <italic>t</italic>, it is also considered to be annotated with all ancestors of <italic>t</italic>. A common approach to deal with multi-label classification in the machine learning field is to train multiple one-vs-rest binary classifiers, such that each classifier learns to discriminate cases of one class from the remaining classes [<xref ref-type="bibr" rid="B9">9</xref>]. Given a test case, all classifiers in such a system are invoked to make calls, and the case is labeled with the classes which turn out to be positive. Although such an approach can be adopted to perform GO annotation, it ignores the structure of GO and suffers from the following shortcomings. Firstly, the unbalanced training cases make learning difficult. This is because the number of training cases for an individual class is usually much smaller than the number of cases of all other classes combined in a multi-label classification scenario. Secondly, the outputs of such a system might not be compatible to the existing structure of classes, e.g., a case is labeled with a class, <italic>c</italic>, but not the parents of <italic>c</italic>.</p><p>Hierarchical classification takes into account the relationships among the target classes during training and outputs multi-labels that comply with the class relations. Hierarchical classification has received growing attention in the machine learning field in recent years [<xref ref-type="bibr" rid="B10">10</xref>-<xref ref-type="bibr" rid="B13">13</xref>]. In the bioinformatics domain, the hierarchical structure of GO was utilized to classify proteins based on various biological data, e.g., gene sequences and microarray [<xref ref-type="bibr" rid="B10">10</xref>,<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B15">15</xref>]. With respect to literature-based GO annotation, reports from text mining workshops have explored hierarchical text classification for GO annotation, e.g., BioLink [<xref ref-type="bibr" rid="B16">16</xref>] and BioCreative [<xref ref-type="bibr" rid="B3">3</xref>,<xref ref-type="bibr" rid="B17">17</xref>]. In the study by Kiritchenko <italic>et al </italic>[<xref ref-type="bibr" rid="B16">16</xref>], a hierarchical classification system was built with AdaBoost algorithms as base classifiers. On the other hand, Verspoor <italic>et al </italic>[<xref ref-type="bibr" rid="B17">17</xref>] attempted to classify documents by utilizing the GO hierarchy structure to identify a set of candidate GO terms. In our study, we investigated and evaluated the performance of hierarchical classification systems built with state-of-the-art text classification methods, namely the support vector machine (SVM) and na&#x000ef;ve Bayes classifier. In addition to conventional hierarchical classification, we also introduced a novel stochastic classification algorithm, referred to as random GO walk (RGOW), to perform probabilistic, graph-based multi-label classification. The motivation for RGOW is, by employing a stochastic mechanism, to alleviate the potential local maximum problem that results from the greedy search of top-down hierarchical classification.</p><p>The main goal of this study is to systematically investigate and evaluate the advantage, or lack of it, of a general class of graph-based multi-label classification methods (based on directed or undirected graphs). More specifically, we have studied the conventional non-hierarchical multi-label classification for GO annotation, the RGOW algorithm, and two top-down hierarchical classification algorithms. Our results show that graph-based multi-label classification methods significantly enhance the classification performance evaluated with metrics that measure exact matches. In addition, our methods are also capable of suggesting GO annotations closely related to the original annotations on the GO graph, even when they fail to predict them directly.</p></sec><sec><title>Results</title><sec><title>PubMed augmented GO graph</title><p>In this study, the task of literature-based gene/protein annotation was cast as a graph-based classification problem. We constructed a PubMed augmented GO graph (see the Methods section) using the Biological Process branch of the GO combined with the Gene Ontology Annotation (GOA) [<xref ref-type="bibr" rid="B18">18</xref>] corpus. In this graph, a node represents a GO term, an edge represents the semantic relationship between a pair of GO term, and the structure of the graph follows the definition of the Biological Process ontology from the Gene Ontology Consortium. In addition, we further augmented the information of the graph by adding sets of PubMed identification numbers to each GO node as attributes of the object. This enables us to further associate each GO node with a text classifier to perform graph-based classification. Although we only studied the performance of graph-based classification on the Biological Process domain of the GO, the results would likely generalize to the Molecular Function and Cellular Component domains because the tasks are essentially the same.</p><p>Figure <xref ref-type="fig" rid="F1">1</xref> shows a subgraph of the PubMed augmented GO graph, illustrating hierarchical relationships between GO terms (nodes) organized as a DAG. Each node is associated with two sets of PMIDs: a set of PMIDs explicitly associated with the node, referred to as <italic>nodeUniqPMIDs</italic>; and a set consisting of all PMIDs associated with the node and its descendants, referred to as <italic>nodeTotalPMIDs</italic>. The cardinalities (sizes) of the <italic>nodeUniqPMIDs </italic>and <italic>nodeTotalPMIDs </italic>sets are shown (in Figure <xref ref-type="fig" rid="F1">1</xref>) as numbers within the parentheses next to the GO terms; the definitions of nodes are shown in the text boxes below the nodes.</p><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold>A subgraph of the PubMed augmented GO graph constructed using the GOA data set</bold>.</p></caption><graphic xlink:href="1471-2105-9-525-1"/></fig><p>We further investigated the distribution of PubMed documents over the GO graph, which provides information on the state of current manual GO annotation processes, the degree of difficulty of training a literature-based GO annotation algorithm, and the motivation for graph-based classification. In Figure <xref ref-type="fig" rid="F2">2</xref>, Panel A shows the histogram of the unique GO terms grouped according to the number of training documents associated with each term (the cardinality of the unique GO terms' <italic>nodeTotalPMID</italic>s). It can be seen that many GO terms are associated with fewer than 10 training documents. One may reason that it is very difficult (if possible at all) to train accurate and generalizable text classifiers for the GO terms with so few training documents. Therefore, a more effective approach is to pool the training cases from these nodes to their ancestors and train more reliable classifiers at the ancestor nodes, which naturally leads to the graph-based multi-label classification approach. Panel B of Figure <xref ref-type="fig" rid="F2">2</xref> shows the count of annotation instances of the GO terms, grouped according to the number of training documents associated with them. It can be seen that, although a relatively small number of GO terms have more than 20 training cases, the instances of observing these GO terms constitute a fairly large portion of all observed GO annotations. Thus, enhancing the capability of correctly predicting these GO terms will have a great impact on the overall performance of the classification systems.</p><fig position="float" id="F2"><label>Figure 2</label><caption><p><bold>Summaries of GO terms with respect to the number of training documents</bold>. Panel A. The histogram of the unique GO terms grouped according to the number of training documents associated with each GO term. Panel B. The count of annotation instances of the GO terms grouped according to the number of training documents associated with them.</p></caption><graphic xlink:href="1471-2105-9-525-2"/></fig></sec><sec><title>Performance evaluation</title><sec><title>Evaluation of multi-label classification</title><p>Since the Gene Ontology Consortium adopts a principle of annotating proteins with GO terms that are as specific as possible, the observed GO terms in the GOA documents are usually the leaves of multi-label subgraphs. In order to evaluate multi-label classification, we reconstructed a multi-label subgraph for each test document based on its true/predicted GO annotations. The steps for constructing such a subgraph are as follows: 1) map a test document's GO annotations onto the PubMed augmented GO graph; 2) find the shortest path between the root and each of the true/predicted GO annotations; 3) join the paths using a union of the edges of the paths to make a subgraph of GO.</p><p>For graph-based multi-label algorithms, we used the outputs of each classification system as leaves to reconstruct the multi-label subgraph. For flat-SVM, we used two ways to evaluate its outputs: one is directly using the system outputs in multi-label evaluation; the other is treating its outputs as leaves (same as other systems) and building the multi-label subgraphs. Using the metrics specifically designed for graph-based multi-label classification described in the Methods Section, we evaluated the performance of different classification algorithms, and the results are shown in Figure <xref ref-type="fig" rid="F3">3</xref>. In Figure <xref ref-type="fig" rid="F3">3</xref>, the first four groups represent the performance of the flat-SVM evaluated with the direct outputs, the top-down SVM (TD-SVM), the top-down naive Bayes (TD-NB), and the random GO walk (RGOW). From these four groups, it can be seen that the TD-SVM, TD-NB, and RGOW systems significantly outperform the flat-SVM, with folds of increase in recall and F-score. The last group (Flat-SVM2) in the figure is the performance of the flat-SVM evaluated on the multi-label subgraphs built based on its outputs. This procedure is equivalent to evaluating the result from a flat SVM classifier as if it is from a hierarchical classifier, even though it does not utilize the GO graph during training. It is interesting to see that, although its performance is better than that of the flat-SVM, the flat-SVM2 is outperformed by the two top-down algorithms and the RGOW in terms of recall and F-score. These results indicate that the better performances by the graph-based classifiers indeed resulted from utilizing information from the GO graph structure during training the classifiers, rather than due to the differences in evaluation procedures.</p><fig position="float" id="F3"><label>Figure 3</label><caption><p><bold>The performance of flat-SVM, TD-SVM, TD-NB, RGOW and flat-SVM2 evaluated with multi-label classification evaluation (graph-to-graph) in terms of recall, precision and F-score</bold>.</p></caption><graphic xlink:href="1471-2105-9-525-3"/></fig></sec><sec><title>Leaf-to-leaf evaluation</title><p>The multi-label evaluation measures the accuracy of the systems by comparing subgraphs, such that it evaluates the overall capability of predicting both specific and general terms on the graph. In practice, protein annotation requires predicting the GO terms that are as specific as possible, and therefore we evaluated how accurately the predicted leaves (specific GO terms) matched the true annotations, a procedure referred to as leaf-to-leaf evaluation. The results are shown in Figure <xref ref-type="fig" rid="F4">4</xref>. Again, the results show that the graph-based multi-label classification methods significantly outperform the flat-SVM. TD-NB achieves a recall of around 17%; this recall represents that ~6,800 out of 40,000 instances of GO annotation in the GOA corpus were correctly predicted. It is interesting to note that precision for the flat-SVM decreases significantly in the leaf-to-leaf evaluation when compared to that in the graph-to-graph evaluation. This difference indicates that many of the correct predictions by the flat-SVM are general GO terms at the top levels of the GO graph, which can be detected in graph-to-graph evaluation. However, the flat-SVM is less capable of predicting more specific GO terms observed in the test cases, and thus it performs much worse in the leaf-to-leaf evaluation.</p><fig position="float" id="F4"><label>Figure 4</label><caption><p><bold>Systems' performance evaluated with leaf-to-leaf evaluation in terms of recall, precision and F-score</bold>.</p></caption><graphic xlink:href="1471-2105-9-525-4"/></fig></sec><sec><title>Evaluating performance using graph-based metrics</title><p>As shown in Figure <xref ref-type="fig" rid="F2">2</xref>, a large number of observed GO terms in the GOA corpus have only a few training documents, so it is almost impossible to train reliable classifiers for them. We conjectured that the misclassification of these cases (classes) constituted the majority of the test errors in multi-label classification. Instead of treating the misclassification of these terms as complete losses, it would be interesting to quantify and evaluate how closely the predicted and observed labels are located in the GO graph. One may argue that the loss incurred from predicting a label only one step away from the true label is more acceptable compared to predicting a label 5 steps apart from the true label. Indeed, one motivation of graph-based multi-label classification is to pool the training cases through training case propagation, so that it is possible to train more <italic>reliable </italic>classifiers associated with the ancestors of a GO node that has sparse training cases. Therefore we would like to evaluate how closely the predictions by these relatively reliable ancestor classifiers relate to the true classes. To this end, we devised graph-based metrics to evaluate results.</p><p>During graph-based evaluation, for each true GO term in testing cases, we searched for the shortest path from the true label to the leaves of the predicted subgraph, and the number of edges in the path was used as a metric to reflect how close to the true label the predicted labels were. The shorter the path, the better the performance. Panel A of Figure <xref ref-type="fig" rid="F5">5</xref> shows the distribution of the shortest distances of the predicted labels to the true GO annotations in the test set. Note that the paths with the length of zero reflect the correct predictions, and thus these numbers essentially agree with the recall of classification systems. It is interesting to note that many observed GO annotations are within one or two steps from the predicted multi-labels, and all graph-based classification systems perform better than the flat-SVM multi-label classification system. Panel B of Figure <xref ref-type="fig" rid="F5">5</xref> plots the cumulative percentile of GO terms (y axis) with respect to the number of steps from the predicted labels. It can be seen that 33% &#x02013; 42% of the true GO annotations are within only two steps from the labels predicted by the TD-NB, TD-SVM and RGOW. The results indicate that these graph-based classification systems are capable of predicting GO annotations very close to the true annotations, yet they are treated as misclassifications according to the conventional evaluation methods for multi-label classification. If we relax the criteria for correct predictions to include the predictions within two steps from the true labels, the graph-based systems can achieve even better performance (see Figure <xref ref-type="fig" rid="F6">6</xref>): 29% &#x02013; 35% in recall, 20% &#x02013; 31% in precision, and 24% &#x02013; 32% in F-score. The results are encouraging given the difficulty of the classification problem for GO annotation.</p><fig position="float" id="F5"><label>Figure 5</label><caption><p><bold>Systems' performance evaluated with graph-based metrics</bold>. Panel A. The distribution of the shortest distances of the predicted labels to the true GO annotations in the test set. Panel B. Cumulative percentile of GO terms with respect to the number of steps from the predicted labels. If a true class is missing from the predicted labels, the distance is set to 30.</p></caption><graphic xlink:href="1471-2105-9-525-5"/></fig><fig position="float" id="F6"><label>Figure 6</label><caption><p><bold>Systems' performance in terms of recall, precision, and F-score for relaxed hits (within two steps)</bold>.</p></caption><graphic xlink:href="1471-2105-9-525-6"/></fig></sec><sec><title>Enhanced classification for classes with fewer training cases</title><p>One of the motivations of employing graph-based classification methods is to address the problem associated with the training case imbalance that plagues flat classifiers. The assumption is that, by performing one-vs-rest classification locally rather than globally, the training case imbalance can be alleviated. To illustrate the impact of the size of training set on the prediction, we plotted the number of correctly predicted instances for each classification algorithm, grouped according to the number of training documents associated with each GO term in Figure <xref ref-type="fig" rid="F7">7</xref>. The figure illustrates that, for the GO classes with fewer than 50 training documents, the graph-based multi-label classification systems significantly outperform the flat multi-label classification method. As the number of training cases increases, the differences between the classification algorithms begin to diminish. These results indicate that the graph-based multi-label classification algorithms improve the performance on the classes with small training sets. These results are highly encouraging because GO terms with few training documents are the most difficult to predict.</p><fig position="float" id="F7"><label>Figure 7</label><caption><p><bold>The number of correctly predicted instances with training sets of different sizes</bold>. For each method, the sum of these numbers is shown in Panel A of Figure <xref ref-type="fig" rid="F5">5</xref> at edge distance equal to 0.</p></caption><graphic xlink:href="1471-2105-9-525-7"/></fig></sec></sec></sec><sec><title>Discussion</title><p>In this study, we transformed the problem of literature-based prediction of GO annotation to a graph-based multi-label classification problem. Our results indicate that, through utilizing the structure of the GO graph, the graph-based multi-label classification algorithms significantly outperform the conventional flat multi-label classification approach. Furthermore, our results demonstrate that graph-based classification is capable of suggesting annotations that are semantically close to the true annotations. These results indicate that the graph-based multi-label classification methods have better potential than the conventional flat multi-label classification approach to facilitate protein annotation based on the literature.</p><p>Controlled vocabularies such as the GO and the Unified Medical Language System (UMLS) [<xref ref-type="bibr" rid="B19">19</xref>,<xref ref-type="bibr" rid="B20">20</xref>] provide computable forms of biomedical concepts, which are critically important in knowledge representation and are widely used in molecular biology and medicine. Interconnections between biological concepts can often be best represented as DAGs rather than trees. Although there have been many investigations on tree-based hierarchical text classification, studies of utilizing a graph structure for multi-label classification of text are few. Recently, Barutcuoglu <italic>et al</italic>. have proposed a sophisticated Bayesian network framework to perform graph-based hierarchical multi-label classification and employed it to predict GO annotations of proteins based on biological data, e.g., gene expression and protein-protein interactions [<xref ref-type="bibr" rid="B10">10</xref>]. Their framework requires a relatively large number of training cases in order to train their model, such that they limited the target classes to about 100 GO terms with at least 20 training cases. This requirement would have eliminated most biologically specific GO terms in our case. In contrast, our methods can be applied on the full graph of the Biological Process domain of GO.</p><p>Our work is closely related to that by Kiritchenko <italic>et al </italic>[<xref ref-type="bibr" rid="B16">16</xref>] in terms of problem formulation and evaluation. In their work, the investigators employed a global hierarchical classification system with an AdaBoost algorithm as the base classifier. In this study, we further investigated the performance of systems consisting of SVM and na&#x000ef;ve Bayes classifiers, which are well established as the best text categorization classifiers [<xref ref-type="bibr" rid="B21">21</xref>]. In terms of evaluation, our graph-to-graph evaluation is essentially equivalent to the hierarchical recall and precision from Kiritchenko <italic>et al</italic>, in that they all evaluated the performance of overall multiple-label classification. In addition, we also performed the leaf-to-leaf evaluation which is more relevant to the real world evaluation from biologists' point of view. Furthermore, their evaluation concentrated on exact matches, which may not fully reflect the benefit of graph-based classification revealed by our relaxed graph-based evaluation. Thus, our evaluation methods demonstrated additional advantages of graph-based multiple-label classification to previous studies. Although it would be ideal to include their method in our evaluation, the lack of available software makes it difficult to perform a fair comparison due to potential minute variances in re-implementation.</p><p>Graph-based multi-label classification from this study is readily carried out as a series of localized classifications. For the TD-SVM and TD-NB algorithms, the localized classification is performed in a breadth-first-search manner, which is guaranteed to stop when all feasible paths are visited. In addition, to improve classification accuracy, employing top-down classification algorithms is more efficient due to their branch-and-bound nature. On the other hand, the RGOW algorithm transforms the DAG into an undirected graph and traverses the graph following the most probable paths. In addition to a more thorough search of the graph, the advantages of this algorithm also include the probabilistic outputs that accommodate the uncertainty of the predictions. Our results indicate that the probabilistic outputs by RGOW correctly reflect the uncertainty of predictions and can be further utilized to determine the decision threshold of classification.</p><p>The more important advantage of the graph-based multi-label classification algorithms lies in the fact that, even when not exactly matching the true target annotations, many of the predicted GO annotations are semantically close to the target annotations. This is the underpinning characteristic and motivation of our approach &#x02013; suggesting and predicting annotations that are as close as possible to the GO terms with few training cases, and the classification on these GO terms would be impossible otherwise. Note that, since most of the observed GO annotations are very specific per the guidelines of the Gene Ontology Consortium, the predicted GO annotations that are only one step away from the true annotation should be fairly specific too. If these predictions are counted as correct, the systems can achieve around 0.4 in recall, which may potentially be helpful to human annotators during annotation processes.</p><p>Although outperforming the flat classification system, the current graph-based multiple-label classification methods need further improvement in order to meet the requirements of real-world literature-based annotation. Reasonable directions for improvement include, first, further fine-tuning the base classifiers. For example, one may fine tune and vary the classification threshold based on the level of the node. Second, a refined approach would use more specific training data. Ideally, the most relevant part of a document related to the GO terms should be identified through semantic analysis [<xref ref-type="bibr" rid="B22">22</xref>] and used for training classifiers.</p></sec><sec><title>Conclusion</title><p>In this paper, we investigated and studied the methods of enhancing automatic multi-label classification of biomedical literature by utilizing the structure of the Gene Ontology graph. We systematically evaluated and compared three graph-based classification algorithms to a conventional flat multi-label algorithm and concluded that through utilizing the information from the structure of the Gene Ontology graph, the graph-based multi-label classification methods have better potential than the conventional flat multi-label classification approach to facilitate protein annotation based on the literature.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>Data set</title><p>The Uniprot [<xref ref-type="bibr" rid="B23">23</xref>] gene-GO association file, version 47, was downloaded from the website of the Gene Ontology Annotation (GOA) [<xref ref-type="bibr" rid="B18">18</xref>] project of the European Bioinformatics Institute. Each entry in the association files contains a gene identification number, the associated GO term, and the PubMed identification number (PMID) for the annotation if available, and thus the data provide the link between the GO annotation and the literature. A corpus consisting of the titles and abstracts of 36,423 MEDLINE entries was downloaded from the National Center for Biotechnology Information (NCBI) using the Entrez E-utility service. The corpus was processed as follows: (1) common words from a standard English "stop words" list were removed; (2) words were stemmed using the Porter stemmer algorithm [<xref ref-type="bibr" rid="B24">24</xref>]; (3) words with fewer than 5 occurrences in the corpus were discarded, resulting in a vocabulary of 33,230 unique words. In this study, we only used the Biological Process branch of the GO to study the performance of the graph-based multi-label classification methods, and the approaches are readily extendable to other GO domains.</p></sec><sec><title>Constructing the PubMed augmented GO graph</title><p>The GO definition file released in April 2007 was downloaded from the GO website and used to construct a GO graph. We have developed a Python software package referred to as GOGrapher (manuscript in preparation), which contains a set of application programming interfaces for building a GO graph and performing various graph-based queries. In the GO graph, each node (vertex) represents a GO term, and each directed edge corresponds to the IS_A relationship between a parent-child GO term pair. In the GOA corpus, each node is associated with a set of PMIDs, referred to as <italic>nodeUniqPMIDs</italic>. The GO graph was topologically sorted [<xref ref-type="bibr" rid="B25">25</xref>], and the PMIDs associated with each GO node were propagated from all children to their parents in a bottom-up fashion. At this stage, each GO node was associated with an additional set of PMIDs referred to as <italic>nodeTotalPMIDs</italic>, consisting of the union of its own <italic>nodeUniqPMIDs </italic>and its children's <italic>nodeTotalPMIDs </italic>sets. After propagation of PMIDs, the nodes with an empty set of <italic>nodeTotalPMIDs </italic>were pruned from the graph, which resulted in a graph with a total of 5,797 nodes (target classes). Based on the <italic>nodeTotalPMIDs</italic>, a word-vector was constructed for each GO node, of which each element was the count of the word associated with the GO term in the corpus. We refer to this graph as the PubMed augmented GO graph. A sub graph of the PubMed augmented GO graph is shown in Figure <xref ref-type="fig" rid="F1">1</xref>.</p></sec><sec><title>Classification methods</title><sec><title>Flat multi-label classification system</title><p>As a baseline reference classification system that would not utilize the structure of GO, a flat one-vs-rest multi-label classification system was constructed. SVM was chosen as the base binary classifier because it is the state-of-the-art classifier for text categorization [<xref ref-type="bibr" rid="B26">26</xref>-<xref ref-type="bibr" rid="B28">28</xref>]. In this model, the GO structure was flattened after propagation of PMIDs, and each class (node) was associated with a binary SVM classifier [<xref ref-type="bibr" rid="B26">26</xref>-<xref ref-type="bibr" rid="B28">28</xref>] to discriminate this class from the other classes. We refer to such a classification system as flat-SVM. A Python wrapper for LibSVM [<xref ref-type="bibr" rid="B29">29</xref>] with a linear kernel and default parameter settings were employed. Given a GO node, <italic>g</italic>, all PubMed documents in its <italic>nodeTotalPMIDs</italic><sub><italic>g </italic></sub>set were labeled as positive training data and all other documents not covered by <italic>nodeTotalPMIDs</italic><sub><italic>g </italic></sub>were labeled as negative training data.</p></sec><sec><title>Top-down hierarchical classification system</title><p>We designed and compared two classification systems for GO annotation with either SVM or naive Bayes as a base classifier. The classification procedure of the system is similar to top-down, tree-based hierarchical classification [<xref ref-type="bibr" rid="B12">12</xref>,<xref ref-type="bibr" rid="B30">30</xref>] but is generalized to deal with the more complicated GO graph structure. The idea underlying the top-down system was to perform localized one-vs-rest, rather than overall one-vs-rest classification at each level to overcome the training case imbalance problem. Given a GO node, <italic>g</italic>, a base classifier was trained with the documents of <italic>nodeTotalPMIDs</italic><sub><italic>g </italic></sub>as positive training cases and the documents of <italic>negTrainingSet</italic><sub><italic>g </italic></sub>defined in Equation (1) as negative training cases. Here, <italic>negTrainingSet</italic><sub><italic>g </italic></sub>is the set of the union of all <italic>g</italic>'s parents total PMIDs excluding <italic>g's nodeTotalPMIDs</italic>.</p><p><disp-formula id="bmcM1"><label>(1)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" name="1471-2105-9-525-i1" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x0222a;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>P</mml:mi><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>P</mml:mi><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>Naive Bayes is a well-studied probabilistic algorithm with robust performance on text classification. In this study, a multinomial version of naive Bayes [<xref ref-type="bibr" rid="B31">31</xref>] was implemented. Let <italic>V </italic>be the set of vocabulary of the corpus and <italic>W</italic><sub><italic>d </italic></sub>be a sequence of words in document <italic>d</italic>. For the binary naive Bayes classifier of node <italic>g</italic>, the prior probability, <italic>p</italic>(<italic>c</italic><sub><italic>g</italic></sub>), the conditional probability of observing a word, <italic>p</italic>(<italic>w</italic>|<italic>c</italic><sub><italic>g</italic></sub>), and the posterior probability for a class are defined as follows:</p><p><disp-formula id="bmcM2"><label>(2)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M2" name="1471-2105-9-525-i2" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>P</mml:mi><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x0222a;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>P</mml:mi><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM3"><label>(3)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M3" name="1471-2105-9-525-i3" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mi>V</mml:mi><mml:mo>|</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM4"><label>(4)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M4" name="1471-2105-9-525-i4" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>In Equation (3), <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M5" name="1471-2105-9-525-i5" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is the count of <italic>w </italic>in the training documents for a given class <italic>c</italic><sub><italic>g</italic></sub>; <italic>&#x003b2; </italic>is the Laplace smoothing parameter [<xref ref-type="bibr" rid="B31">31</xref>], which was set to 0.001 in this study. With individual base-classifiers trained at each GO node, classification of a new document was performed according to Algorithm 1 in a top-down, breadth-first-search manner as shown in Table <xref ref-type="table" rid="T1">1</xref>.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Algorithm 1 Top-down classification algorithm</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left">1</td><td align="left"><bold>inputs</bold></td></tr><tr><td></td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>d</italic>, a new document</td></tr><tr><td></td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>G</italic>, a GO graph with trained base-classifiers</td></tr><tr><td align="left">2</td><td align="left"><bold>initialize</bold>:</td></tr><tr><td align="left">3</td><td align="left">&#x000a0;&#x000a0;&#x000a0;<italic>PA </italic>&#x02190; {<italic>root</italic>} //Predicted GO annotation set</td></tr><tr><td align="left">4</td><td align="left">&#x000a0;&#x000a0;&#x000a0;<italic>Q </italic>&#x02190; {<italic>root</italic>} //Queue for breadth first search</td></tr><tr><td align="left">5</td><td align="left"><bold>while </bold><italic>Q </italic>not empty</td></tr><tr><td align="left">6</td><td align="left">&#x000a0;&#x000a0;&#x000a0;<italic>n </italic>&#x02190; <italic>Q</italic>.pop()</td></tr><tr><td align="left">7</td><td align="left">&#x000a0;&#x000a0;&#x000a0;<italic>S </italic>&#x02190; <italic>G</italic>.children(<italic>n</italic>)</td></tr><tr><td align="left">8</td><td align="left">&#x000a0;&#x000a0;&#x000a0;<bold>for </bold>each <italic>c </italic>in <italic>S</italic>:</td></tr><tr><td align="left">9</td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>y </italic>&#x02190; <italic>c</italic>.predict (<italic>d</italic>)</td></tr><tr><td></td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;//Classify <italic>d </italic>using the base-classifier of <italic>c</italic></td></tr><tr><td align="left">10</td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;if y == 1:</td></tr><tr><td></td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;//if prediction is positive</td></tr><tr><td align="left">11</td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>Q </italic>&#x02190; union(<italic>Q</italic>, <italic>c</italic>)</td></tr><tr><td align="left">12</td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>PA </italic>&#x02190; union(<italic>PA</italic>, <italic>c</italic>)</td></tr><tr><td align="left">13</td><td align="left">&#x000a0;&#x000a0;&#x000a0;<bold>end</bold></td></tr><tr><td align="left">14</td><td align="left"><bold>end</bold></td></tr><tr><td align="left">15</td><td align="left"><bold>outputs: </bold><italic>PA</italic></td></tr></tbody></table></table-wrap></sec><sec><title>Random GO walk (RGOW)</title><p>RGOW performs a stochastic search of the best multiple-labels for a given document, based on the Metropolis-Hastings algorithm [<xref ref-type="bibr" rid="B32">32</xref>] with a simulated annealing procedure. We designed RGOW to explore if stochastic procedures can be used to alleviate the local maximum problem due to the greedy search nature of the top-down SVM and na&#x000ef;ve Bayes classifiers. In addition, the system also outputs a probability distribution over the leaf labels reflecting the posterior probability of the multiple-labels.</p><p>An intuitive explanation for the algorithm is as follows: imagine that an undirected version of the PubMed augmented GO graph constitutes a landscape, and a new test document <italic>d </italic>is allowed to stochastically traverse the landscape to search for the most probable labels for it. At each step, the document stays at current node <italic>g </italic>and looks for the next node <italic>g</italic>*. A candidate node <italic>g</italic>* is stochastically selected according to a proposal distribution <italic>q</italic>(<italic>g</italic>* | <italic>g, d</italic>) defined as Equation (5) and accepted according to Algorithm 2 in Table <xref ref-type="table" rid="T2">2</xref>. Furthermore, a simulated annealing procedure enables the algorithm to search for the global maximum of the landscape&#x02013;the most probable labels for the document. If an affinity function is chosen such that it reflects the likelihood of the GO term being used to annotate the document <italic>d</italic>, a probability distribution over the multi-labels of the graph can be obtained by counting the samples that stop at each GO node followed by a normalization procedure. A posterior multinomial distribution guiding the next step from <italic>g </italic>(line 9 and 10 in Table <xref ref-type="table" rid="T2">2</xref>) is constructed locally through a Bayesian approach, in which the probability of the document reaching node <italic>g* </italic>in the next step is defined as Equation (5). The term <italic>p(g|d) </italic>in Algorithm 2 (at line 12 in Table <xref ref-type="table" rid="T2">2</xref>) is defined as Equation (6).</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Algorithm 2 Random GO walk</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left">1</td><td align="left"><bold>inputs:</bold></td></tr><tr><td></td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>d</italic>, a new document</td></tr><tr><td></td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>G</italic>, a GO graph with training cases</td></tr><tr><td></td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>nSample</italic>, the sample size</td></tr><tr><td></td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>nMaxSteps</italic>, the number of maximum steps</td></tr><tr><td align="left">2</td><td align="left"><bold>initialize</bold>:</td></tr><tr><td align="left">3</td><td align="left">&#x000a0;&#x000a0;&#x000a0;<italic>finalLeaves </italic>&#x02190; {}</td></tr><tr><td align="left">4</td><td align="left">&#x000a0;&#x000a0;&#x000a0;<italic>finalLeavesProbs </italic>&#x02190; {}</td></tr><tr><td align="left">5</td><td align="left"><bold>for </bold><italic>n </italic>in 1: <italic>nSamples</italic></td></tr><tr><td align="left">6</td><td align="left">&#x000a0;&#x000a0;&#x000a0;<italic>g </italic>&#x02190; initialize()</td></tr><tr><td align="left">7</td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;//Select initial GO node randomly</td></tr><tr><td></td><td align="left">&#x000a0;&#x000a0;&#x000a0;<bold>for </bold><italic>s </italic>in 1: <italic>nMaxSteps</italic>:</td></tr><tr><td align="left">8</td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>T </italic>&#x02190; TempFunc(s)</td></tr><tr><td align="left">9</td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>nbrs </italic>&#x02190; <italic>G</italic>.neighbors(<italic>g</italic>)</td></tr><tr><td align="left">10</td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>g</italic>* &#x02190; <italic>q</italic>(<italic>g</italic>*|<italic>g, d</italic>)</td></tr><tr><td></td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;//Sample from proposal distribution, <italic>g</italic>*&#x02208;{<italic>g</italic>, <italic>nbrs</italic>}</td></tr><tr><td align="left">11</td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>u </italic>&#x02190; uniform [0, 1]</td></tr><tr><td align="left">12</td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>if </bold><italic>u </italic>&#x0003c;<italic>A </italic>&#x02190; <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M6" name="1471-2105-9-525-i6" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>min</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle scriptlevel="+1"><mml:mfrac bevelled="true"><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo>|</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle scriptlevel="+1"><mml:mfrac bevelled="true"><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula></td></tr><tr><td align="left">13</td><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<italic>g </italic>&#x02190; <italic>g</italic>*</td></tr><tr><td align="left">14</td><td align="left">&#x000a0;&#x000a0;&#x000a0;<bold>end</bold></td></tr><tr><td align="left">15</td><td align="left">&#x000a0;&#x000a0;&#x000a0;<italic>finalLeaves </italic>&#x02190; union(<italic>finalLeaves, curNode</italic>)</td></tr><tr><td align="left">16</td><td align="left"><bold>end</bold></td></tr><tr><td align="left">17</td><td align="left"><italic>finaLeavesProbs </italic>&#x02190; calProbFromSample(<italic>finalLeaves</italic>)</td></tr><tr><td align="left">18</td><td align="left"><bold>outputs: </bold><italic>finalLeaves, finaLeavesProbs</italic></td></tr></tbody></table></table-wrap><p><disp-formula id="bmcM5"><label>(5)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M7" name="1471-2105-9-525-i7" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM6"><label>(6)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M8" name="1471-2105-9-525-i8" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo>|</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where the probability quantities in Equation (5) are defined as follows:</p><p><disp-formula id="bmcM7"><label>(7)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M9" name="1471-2105-9-525-i9" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>q</mml:mi><mml:mi>P</mml:mi><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>q</mml:mi><mml:mi>P</mml:mi><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM8"><label>(8)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M10" name="1471-2105-9-525-i10" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>q</mml:mi><mml:mi>P</mml:mi><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>P</mml:mi><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM9"><label>(9)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M11" name="1471-2105-9-525-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>g</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x02208;</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>g</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mi>V</mml:mi><mml:mo>|</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM10"><label>(10)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M12" name="1471-2105-9-525-i12" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>In the above equations, <italic>V</italic>, <italic>W</italic><sub><italic>d </italic></sub>and <italic>&#x003b2; </italic>are the same as defined in binary naive Bayes, <italic>count(w)</italic><sub><italic>g </italic></sub>is the number of words taking the value of <italic>w </italic>in the document set of <italic>nodeTotalPMIDs</italic><sub><italic>g</italic></sub>, and <italic>neighbors</italic>(<italic>g</italic>) is the set of neighbor nodes of <italic>g</italic>. With the local proposal distribution determined, a new document can traverse the GO graph through sampling (random walk) to search for the most likely GO annotation (see Algorithm 2 in Table <xref ref-type="table" rid="T2">2</xref>)</p><p>In Algorithm 2, the function <italic>calProbFromSample </italic>calculates the probability that document <italic>d </italic>stops at node <italic>g </italic>by dividing the number of samples whose final visited node is <italic>g </italic>by the total number of samples. We set the sample size to 40 and the number of steps of the random walk to 30. The simulated temperature is defined as Equation (11):</p><p><disp-formula id="bmcM11"><label>(11)</label><italic>TempFunc</italic>(<italic>i</italic>) = (<italic>C </italic>ln(<italic>i </italic>+ <italic>T</italic><sub>0</sub>))<sup>-1</sup>,</disp-formula></p><p>where <italic>T</italic><sub><italic>0 </italic></sub>is the initial temperature and <italic>C </italic>is a constant. <italic>T</italic><sub><italic>0 </italic></sub>and C were set to 1.1 and 4, respectively.</p></sec></sec><sec><title>Evaluation</title><sec><title>Semantic distance</title><p>In this study, we adopted a commonly used method to measure the semantic distance between a pair of GO terms, in which the difference between the information contents (IC) [<xref ref-type="bibr" rid="B33">33</xref>-<xref ref-type="bibr" rid="B36">36</xref>] of the GO terms was employed as a measure of the semantic distance. Here, the IC of a GO term <italic>t </italic>was calculated as: IC(<italic>t</italic>) = -ln <italic>P</italic>(<italic>t</italic>), where <italic>P</italic>(<italic>t</italic>) was the probability of observing the term, calculated as the number of annotation instances by the term divided by the total number of annotation instances. Then the semantic distance between a parent-child pair of GO terms, <italic>t</italic><sub><italic>p </italic></sub>and <italic>t</italic><sub><italic>c</italic></sub>, was determined as follows,</p><p><disp-formula id="bmcM12"><label>(12)</label><italic>dist</italic>(<italic>t</italic><sub><italic>p</italic></sub>, <italic>t</italic><sub><italic>c</italic></sub>) = |<italic>IC</italic>(<italic>t</italic><sub><italic>p</italic></sub>) - <italic>IC</italic>(<italic>t</italic><sub><italic>c</italic></sub>)|.</disp-formula></p><p>Note, that the IC-based semantic distance is not a metric distance in that it does not satisfy the triangle inequality, which potentially introduces errors during a search for the shortest path between a pair of GO terms. However, the operations of searching for the shortest paths between GO terms were performed in a consistent manner during the evaluation of all classification algorithms, and therefore we believe this characteristic of the IC-based semantic distance had no significant impact on the comparison of the results.</p></sec><sec><title>Multi-label evaluation metrics</title><p>Since abundant training and testing data are available, we employed a four-fold cross validation procedure in evaluation. Evaluation of multi-label classification is different from that of conventional binary classification. In this study, we adopted the information retrieval metrics that were modified for evaluating multi-label classification [<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B16">16</xref>,<xref ref-type="bibr" rid="B37">37</xref>]. Let <italic>D </italic>denote the test corpus and <italic>Y</italic><sub><italic>d </italic></sub>and <italic>Z</italic><sub><italic>d </italic></sub>be the true and predicted label sets, respectively, for document <italic>d</italic>. The precision, recall and F-score for a classification system are determined as follows,</p><p><disp-formula id="bmcM13"><label>(13)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M13" name="1471-2105-9-525-i13" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM14"><label>(14)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M14" name="1471-2105-9-525-i14" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM15"><label>(15)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M15" name="1471-2105-9-525-i15" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>F</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p></sec><sec><title>Graph-based evaluation metrics</title><p>When the predicted labels do not match exactly with the true labels, the above metrics consider such an error as a complete loss. However, in the graph-based classification scenario, we wanted to know whether the predicted classes were closely related to the true classes even if they were not direct matches. We used the length (in number of edges) of the shortest path (measured with IC) between true and predicted labels as a metric for evaluating the closeness of the predicted and true labels. The shortest paths between all pairs of true and predicted labels were found using Dijkstra's algorithm [<xref ref-type="bibr" rid="B38">38</xref>].</p></sec></sec><sec><title>Software</title><p>A Python package is available at:</p><p><ext-link ext-link-type="uri" xlink:href="http://projects.dbbe.musc.edu/public/GOHClassification/trunk/"/></p></sec></sec><sec><title>Authors' contributions</title><p>XL conceived the project, BJ carried out experiments, BM contributed to coding of the library. All authors contributed to experiment designs and manuscript writing.</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>This research is partially supported by NIH grant 1R01LM009153-01A1.</p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Camon</surname><given-names>E</given-names></name><name><surname>Magrane</surname><given-names>M</given-names></name><name><surname>Barrell</surname><given-names>D</given-names></name><name><surname>Lee</surname><given-names>V</given-names></name><name><surname>Dimmer</surname><given-names>E</given-names></name><name><surname>Maslen</surname><given-names>J</given-names></name><name><surname>Binns</surname><given-names>D</given-names></name><name><surname>Harte</surname><given-names>N</given-names></name><name><surname>Lopez</surname><given-names>R</given-names></name><name><surname>Apweiler</surname><given-names>R</given-names></name></person-group><article-title>The Gene Ontology Annotation (GOA) Database: sharing knowledge in Uniprot with Gene Ontology</article-title><source>Nucleic Acids Res</source><year>2004</year><fpage>D262</fpage><lpage>266</lpage><pub-id pub-id-type="pmid">14681408</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Baumgartner</surname><given-names>WA</given-names><suffix>Jr</suffix></name><name><surname>Cohen</surname><given-names>KB</given-names></name><name><surname>Fox</surname><given-names>LM</given-names></name><name><surname>Acquaah-Mensah</surname><given-names>G</given-names></name><name><surname>Hunter</surname><given-names>L</given-names></name></person-group><article-title>Manual curation is not sufficient for annotation of genomic databases</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><fpage>i41</fpage><lpage>48</lpage><pub-id pub-id-type="pmid">17646325</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Camon</surname><given-names>EB</given-names></name><name><surname>Barrell</surname><given-names>DG</given-names></name><name><surname>Dimmer</surname><given-names>EC</given-names></name><name><surname>Lee</surname><given-names>V</given-names></name><name><surname>Magrane</surname><given-names>M</given-names></name><name><surname>Maslen</surname><given-names>J</given-names></name><name><surname>Binns</surname><given-names>D</given-names></name><name><surname>Apweiler</surname><given-names>R</given-names></name></person-group><article-title>An evaluation of GO annotation retrieval for BioCreAtIvE and GOA</article-title><source>BMC Bioinformatics</source><year>2005</year><volume>6</volume><fpage>S17</fpage><pub-id pub-id-type="pmid">15960829</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>AM</given-names></name><name><surname>Hersh</surname><given-names>WR</given-names></name></person-group><article-title>The TREC 2004 genomics track categorization task: classifying full text biomedical documents</article-title><source>J Biomed Discov Collab</source><year>2006</year><volume>1</volume><fpage>4</fpage><pub-id pub-id-type="pmid">16722582</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Couto</surname><given-names>FM</given-names></name><name><surname>Silva</surname><given-names>MJ</given-names></name><name><surname>Lee</surname><given-names>V</given-names></name><name><surname>Dimmer</surname><given-names>E</given-names></name><name><surname>Camon</surname><given-names>E</given-names></name><name><surname>Apweiler</surname><given-names>R</given-names></name><name><surname>Kirsch</surname><given-names>H</given-names></name><name><surname>Rebholz-Schuhmann</surname><given-names>D</given-names></name></person-group><article-title>GOAnnotator: linking protein GO annotations to evidence text</article-title><source>J Biomed Discov Collab</source><year>2006</year><volume>1</volume><fpage>19</fpage><pub-id pub-id-type="pmid">17181854</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Daraselia</surname><given-names>N</given-names></name><name><surname>Yuryev</surname><given-names>A</given-names></name><name><surname>Egorov</surname><given-names>S</given-names></name><name><surname>Mazo</surname><given-names>I</given-names></name><name><surname>Ispolatov</surname><given-names>I</given-names></name></person-group><article-title>Automatic extraction of gene ontology annotation and its correlation with clusters in protein networks</article-title><source>BMC Bioinformatics</source><year>2007</year><volume>8</volume><fpage>243</fpage><pub-id pub-id-type="pmid">17620146</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Jelier</surname><given-names>R</given-names></name><name><surname>Schuemie</surname><given-names>MJ</given-names></name><name><surname>Roes</surname><given-names>PJ</given-names></name><name><surname>van Mulligen</surname><given-names>EM</given-names></name><name><surname>Kors</surname><given-names>JA</given-names></name></person-group><article-title>Literature-based concept profiles for gene annotation: The issue of weighting</article-title><source>Int J Med Inform</source><year>2007</year><volume>77</volume><fpage>354</fpage><lpage>362</lpage><pub-id pub-id-type="pmid">17827057</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Daraselia</surname><given-names>N</given-names></name><name><surname>Yuryev</surname><given-names>A</given-names></name><name><surname>Egorov</surname><given-names>S</given-names></name><name><surname>Mazo</surname><given-names>I</given-names></name><name><surname>Ispolatov</surname><given-names>I</given-names></name></person-group><article-title>Automatic extraction of gene ontology annotation and its correlation with clusters in protein networks</article-title><source>BMC Bioinformatics</source><year>2007</year><volume>8</volume><fpage>243</fpage><pub-id pub-id-type="pmid">17620146</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tsoumakas</surname><given-names>G</given-names></name><name><surname>Katakis</surname><given-names>I</given-names></name></person-group><article-title>Multi-Label Classification: An Overview</article-title><source>International Journal of Data Warehousing and Mining</source><year>2007</year><volume>3</volume><fpage>1</fpage><lpage>13</lpage></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Barutcuoglu</surname><given-names>Z</given-names></name><name><surname>Schapire</surname><given-names>RE</given-names></name><name><surname>Troyanskaya</surname><given-names>OG</given-names></name></person-group><article-title>Hierarchical multi-label prediction of gene function</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>830</fpage><lpage>836</lpage><pub-id pub-id-type="pmid">16410319</pub-id></citation></ref><ref id="B11"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>L</given-names></name><name><surname>Hofmann</surname><given-names>T</given-names></name></person-group><article-title>Hierarchical document categorization with support vector machines</article-title><source>ACM 13th Conference on Information Management:2004</source><year>2004</year></citation></ref><ref id="B12"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Dumais</surname><given-names>ST</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name></person-group><article-title>Hierarchical classification of web content</article-title><source>ACM Special Interest Group on Information Retrieval (SIGIR): 2000</source><year>2000</year><fpage>256</fpage><lpage>263</lpage></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rousu</surname><given-names>J</given-names></name><name><surname>Saunders</surname><given-names>C</given-names></name><name><surname>Shawe-Taylor</surname><given-names>J</given-names></name></person-group><article-title>Kernel-based learning of hierarchical multilabel classification models</article-title><source>Journal of Machine Learning Research</source><year>2006</year><volume>7</volume><fpage>1601</fpage><lpage>1626</lpage></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Verspoor</surname><given-names>K</given-names></name><name><surname>Cohn</surname><given-names>J</given-names></name><name><surname>Mniszewski</surname><given-names>S</given-names></name><name><surname>Joslyn</surname><given-names>C</given-names></name></person-group><article-title>A categorization approach to automated ontological function annotation</article-title><source>Protein Sci</source><year>2006</year><volume>15</volume><fpage>1544</fpage><lpage>1549</lpage><pub-id pub-id-type="pmid">16672243</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wolstencroft</surname><given-names>K</given-names></name><name><surname>Lord</surname><given-names>P</given-names></name><name><surname>Tabernero</surname><given-names>L</given-names></name><name><surname>Brass</surname><given-names>A</given-names></name><name><surname>Stevens</surname><given-names>R</given-names></name></person-group><article-title>Protein classification using ontology classification</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>e530</fpage><lpage>538</lpage><pub-id pub-id-type="pmid">16873517</pub-id></citation></ref><ref id="B16"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Kiritchenko</surname><given-names>S</given-names></name><name><surname>Matwin</surname><given-names>S</given-names></name><name><surname>Famili</surname><given-names>FID</given-names></name></person-group><article-title>Functional Annotation of Genes Using Hierarchical Text Categorization</article-title><source>BioLINK SIG: Linking Literature, Information and Knowledge for Biology, a Joint Meeting of The ISMB BioLINK Special Interest Group on Text Data Mining and The ACL Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics</source><year>2005</year></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Verspoor</surname><given-names>K</given-names></name><name><surname>Cohn</surname><given-names>J</given-names></name><name><surname>Joslyn</surname><given-names>C</given-names></name><name><surname>Mniszewski</surname><given-names>S</given-names></name><name><surname>Rechtsteiner</surname><given-names>A</given-names></name><name><surname>Rocha</surname><given-names>LM</given-names></name><name><surname>Simas</surname><given-names>T</given-names></name></person-group><article-title>Protein annotation as term categorization in the gene ontology using word proximity networks</article-title><source>BMC bioinformatics</source><year>2005</year><volume>6</volume><fpage>S20</fpage><pub-id pub-id-type="pmid">15960833</pub-id></citation></ref><ref id="B18"><citation citation-type="other"><person-group person-group-type="author"><collab>GOA</collab></person-group><article-title>Gene Ontology Annotation Project</article-title><ext-link ext-link-type="uri" xlink:href="http://www.ebi.ac.uk/GOA/"/></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lindberg</surname><given-names>C</given-names></name></person-group><article-title>The Unified Medical Language System (UMLS) of the National Library of Medicine</article-title><source>Journal (American Medical Record Association)</source><year>1990</year><volume>61</volume><fpage>40</fpage><lpage>42</lpage><pub-id pub-id-type="pmid">10104531</pub-id></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bodenreider</surname><given-names>O</given-names></name></person-group><article-title>The Unified Medical Language System (UMLS): integrating biomedical terminology</article-title><source>Nucleic Acids Res</source><year>2004</year><fpage>D267</fpage><lpage>270</lpage><pub-id pub-id-type="pmid">14681409</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>DD</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Rose</surname><given-names>TG</given-names></name><name><surname>Li</surname><given-names>F</given-names></name></person-group><article-title>RCV1: A new benchmark collection for text categorization research</article-title><source>Journal of Machine Learning Research</source><year>2004</year><volume>5</volume><fpage>361</fpage><lpage>397</lpage></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>B</given-names></name><name><surname>McLean</surname><given-names>DC</given-names><suffix>Jr</suffix></name><name><surname>Lu</surname><given-names>X</given-names></name></person-group><article-title>Identifying biological concepts from a protein-related corpus with a probabilistic topic model</article-title><source>BMC Bioinformatics</source><year>2006</year><volume>7</volume><fpage>58</fpage><pub-id pub-id-type="pmid">16466569</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Camon</surname><given-names>E</given-names></name><name><surname>Barrell</surname><given-names>D</given-names></name><name><surname>Lee</surname><given-names>V</given-names></name><name><surname>Dimmer</surname><given-names>E</given-names></name><name><surname>Apweiler</surname><given-names>R</given-names></name></person-group><article-title>The Gene Ontology Annotation (GOA) Database&#x02013;an integrated resource of GO annotations to the UniProt Knowledgebase</article-title><source>In Silico Biol</source><year>2004</year><volume>4</volume><fpage>5</fpage><lpage>6</lpage><pub-id pub-id-type="pmid">15089749</pub-id></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Porter</surname><given-names>MF</given-names></name></person-group><article-title>An algorithm for suffix stripping</article-title><source>Program</source><year>1980</year><volume>14</volume><fpage>130</fpage><lpage>137</lpage></citation></ref><ref id="B25"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Cormen</surname><given-names>T</given-names></name><name><surname>Leiserson</surname><given-names>CE</given-names></name><name><surname>Rivest</surname><given-names>RL</given-names></name><name><surname>Stein</surname><given-names>C</given-names></name></person-group><source>Introduction to algorithms</source><year>2001</year><edition>2</edition><publisher-name>Cambridge, MA: MIT Press</publisher-name></citation></ref><ref id="B26"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Boser</surname><given-names>B</given-names></name><name><surname>Guyon</surname><given-names>I</given-names></name><name><surname>Vapnik</surname><given-names>VN</given-names></name></person-group><article-title>A training algorithm for optimal margin classifiers</article-title><source>Proc 5th Annual Workshop on Computational Learning Theory</source><year>1992</year><publisher-name>New York, ACM Press</publisher-name><fpage>144</fpage><lpage>152</lpage></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>X</given-names></name><name><surname>Zheng</surname><given-names>B</given-names></name><name><surname>Velivelli</surname><given-names>A</given-names></name><name><surname>Zhai</surname><given-names>C</given-names></name></person-group><article-title>Enhancing text categorization with semantic-enriched representation and training data augmentation</article-title><source>J Am Med Inform Assoc</source><year>2006</year><volume>13</volume><fpage>526</fpage><lpage>535</lpage><pub-id pub-id-type="pmid">16799127</pub-id></citation></ref><ref id="B28"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>VN</given-names></name></person-group><source>Statistical Learning Theory</source><year>1998</year><publisher-name>New York: John Wiley and Sons</publisher-name></citation></ref><ref id="B29"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>C-C</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><article-title>LIBSVM: a library for support vector machines</article-title><year>2001</year></citation></ref><ref id="B30"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Koller</surname><given-names>D</given-names></name><name><surname>Sahami</surname><given-names>M</given-names></name></person-group><article-title>Hierarchically classifying documents using very few words</article-title><source>the 14th International Conference on Machine Learning (ICML)</source><year>1997</year></citation></ref><ref id="B31"><citation citation-type="book"><person-group person-group-type="author"><name><surname>McCallum</surname><given-names>A</given-names></name><name><surname>Nigam</surname><given-names>K</given-names></name></person-group><article-title>A Comparison of Event Models for Naive Bayes Text Classification</article-title><source>AAAI/ICML-98 Workshop on Learning for Text Categorization: 1998</source><year>1998</year><publisher-name>AAAI Press</publisher-name><fpage>41</fpage><lpage>48</lpage></citation></ref><ref id="B32"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hastings</surname><given-names>WK</given-names></name></person-group><article-title>Monte Carlo sampling methods using Markov chains and their applications</article-title><source>Biometrika</source><year>1970</year><volume>57</volume><fpage>97</fpage><lpage>109</lpage></citation></ref><ref id="B33"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>J</given-names></name><name><surname>Conrath</surname><given-names>D</given-names></name></person-group><article-title>Semantic similarity based on corpus statistics and lexical taxonomy</article-title><source>Proceedings on International Conference on Research in Computational Linguistics: 1998; Taiwan</source><year>1998</year></citation></ref><ref id="B34"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>D</given-names></name></person-group><article-title>An information-theoretic definition of similarity</article-title><source>Proc 15th International Conf on Machine Learning</source><year>1998</year><publisher-name>San Francisco, CA: Morgan Kaufmann</publisher-name><fpage>296</fpage><lpage>304</lpage></citation></ref><ref id="B35"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lord</surname><given-names>P</given-names></name><name><surname>Stevens</surname><given-names>R</given-names></name><name><surname>Brass</surname><given-names>A</given-names></name><name><surname>Goble</surname><given-names>C</given-names></name></person-group><article-title>Investigating semantic similarity measures across the Gene Ontology: the relationship between sequence and annotation</article-title><source>Bioinformatics</source><year>2003</year><volume>19</volume><fpage>1275</fpage><lpage>1283</lpage><pub-id pub-id-type="pmid">12835272</pub-id></citation></ref><ref id="B36"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Resnik</surname><given-names>P</given-names></name></person-group><article-title>Using information content to evaluate semantic similarity in a taxonomy</article-title><source>Proceedings of the 14th International Joint Conference on Artificial Intelligence</source><year>1995</year><fpage>448</fpage><lpage>453</lpage></citation></ref><ref id="B37"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Costa</surname><given-names>EP</given-names></name><name><surname>Lorena</surname><given-names>AC</given-names></name><name><surname>Carvalho</surname><given-names>AeCPLF</given-names></name><name><surname>Freitas</surname><given-names>AA</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Drummond WE C, Japkowicz N, Macskassy SA</surname></name></person-group><article-title>A Review of Performance Evaluation Measures for Hierarchical Classifiers</article-title><source>Evaluation Methods for Machine Learning II: papers from the AAAI-2007 Workshop</source><year>2007</year><fpage>1</fpage><lpage>6</lpage></citation></ref><ref id="B38"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>EW</given-names></name></person-group><article-title>A note on two problems in connection with graphs</article-title><source>Numerische Mathematic</source><year>1959</year><volume>1</volume><fpage>269</fpage><lpage>271</lpage></citation></ref></ref-list></back></article>