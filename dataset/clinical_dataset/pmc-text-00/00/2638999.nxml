<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article" xml:lang="EN"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id><journal-id journal-id-type="publisher-id">bioinformatics</journal-id><journal-id journal-id-type="hwp">bioinfo</journal-id><journal-title>Bioinformatics</journal-title><issn pub-type="ppub">1367-4803</issn><issn pub-type="epub">1460-2059</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19038986</article-id><article-id pub-id-type="pmc">2638999</article-id><article-id pub-id-type="doi">10.1093/bioinformatics/btn618</article-id><article-id pub-id-type="publisher-id">btn618</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Papers</subject><subj-group><subject>Structural Bioinformatics</subject></subj-group></subj-group></article-categories><title-group><article-title>Identification of structurally conserved residues of proteins in absence of structural homologs using neural network ensemble</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pugalenthi</surname><given-names>Ganesan</given-names></name><xref ref-type="aff" rid="AFF1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Ke</given-names></name><xref ref-type="aff" rid="AFF1"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Suganthan</surname><given-names>P. N.</given-names></name><xref ref-type="aff" rid="AFF1"><sup>1</sup></xref><xref ref-type="corresp" rid="COR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Chakrabarti</surname><given-names>Saikat</given-names></name><xref ref-type="aff" rid="AFF1"><sup>3</sup></xref><xref ref-type="corresp" rid="COR1">*</xref></contrib></contrib-group><aff id="AFF1"><sup>1</sup>School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798, <sup>2</sup>Department of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China and <sup>3</sup>National Center for Biotechnology Information, National Library of Medicine, National Institutes of Health, Bethesda, MD 20894, USA</aff><author-notes><corresp id="COR1">*To whom correspondence should be addressed.</corresp><fn><p>Associate Editor: Anna Tramontano</p></fn></author-notes><pub-date pub-type="ppub"><day>15</day><month>1</month><year>2009</year></pub-date><pub-date pub-type="epub"><day>27</day><month>11</month><year>2008</year></pub-date><pub-date pub-type="pmc-release"><day>27</day><month>11</month><year>2008</year></pub-date><volume>25</volume><issue>2</issue><fpage>204</fpage><lpage>210</lpage><history><date date-type="received"><day>22</day><month>8</month><year>2008</year></date><date date-type="rev-recd"><day>25</day><month>11</month><year>2008</year></date><date date-type="accepted"><day>25</day><month>11</month><year>2008</year></date></history><permissions><copyright-statement>&#x000a9; 2008 The Author(s)</copyright-statement><copyright-year>2008</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc/2.0/uk/"><p><!--CREATIVE COMMONS-->This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/2.0/uk/">http://creativecommons.org/licenses/by-nc/2.0/uk/</ext-link>) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.</p></license></permissions><abstract><p><bold>Motivation:</bold> So far various bioinformatics and machine learning techniques applied for identification of sequence and functionally conserved residues in proteins. Although few computational methods are available for the prediction of structurally conserved residues from protein structure, almost all methods require homologous structural information and structure-based alignments, which still prove to be a bottleneck in protein structure comparison studies. In this work, we developed a neural network approach for identification of structurally important residues from a single protein structure without using homologous structural information and structural alignment.</p><p><bold>Results:</bold> A neural network ensemble (NNE) method that utilizes negative correlation learning (NCL) approach was developed for identification of structurally conserved residues (SCRs) in proteins using features that represent amino acid conservation and composition, physico-chemical properties and structural properties. The NCL-NNE method was applied to 6042 SCRs that have been extracted from 496 protein domains. This method obtained high prediction sensitivity (92.8%) and quality (Matthew's correlation coefficient is 0.852) in identification of SCRs. Further benchmarking using 60 protein domains containing 1657 SCRs that were not part of the training and testing datasets shows that the NCL-NNE can correctly predict SCRs with &#x0223c; 90% sensitivity. These results suggest the usefulness of NCL-NNE for facilitating the identification of SCRs utilizing information derived from a single protein structure. Therefore, this method could be extremely effective in large-scale benchmarking studies where reliable structural homologs and alignments are limited.</p><p><bold>Availability:</bold> The executable for the NCL-NNE algorithm is available at <ext-link ext-link-type="uri" xlink:href="http://www3.ntu.edu.sg/home/EPNSugan/index_files/SCR.htm">http://www3.ntu.edu.sg/home/EPNSugan/index_files/SCR.htm</ext-link></p><p><bold>Contact:</bold> <email>epnsugan@ntu.edu.sg</email>; <email>chakraba@ncbi.nlm.nih.gov</email>.</p><p><bold>Supplementary information:</bold> <ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/cgi/content/full/btn618/DC1">Supplementary data</ext-link> are available at <italic>Bioinformatics</italic> online.</p></abstract></article-meta></front><body><sec sec-type="intro" id="SEC1"><title>1 INTRODUCTION</title><p>The overall fold is very important to maintain a suitable framework for protein function. It is well established that the structure of proteins is determined by their amino acid sequences (Anfinsen, <xref ref-type="bibr" rid="B2">1973</xref>). Although there is an exponential increase in the available protein structures, the number of protein folds is still very limited in nature (Chothia, <xref ref-type="bibr" rid="B8">1992</xref>). In other words, many evolutionary and functionally related proteins with well-diverged sequences still keep the same folding pattern. This suggests that the protein folding pattern depends not only on the whole sequence but also on some small segments of residues that are conserved during evolution in both sequence and structural aspects. These residues, which might have important implications in maintenance of protein folds, are termed as SCR.</p><p>Sequence-based motifs and conserved residues are useful in understanding the conservational variation and have been successfully linked to functionally important sites indicating higher selection pressure on them (Neuwald et al., <xref ref-type="bibr" rid="B19">1995</xref>; Saqi and Sternberg, <xref ref-type="bibr" rid="B26">1994</xref>). However, SCRs identified at 3D structure level provide more meaningful information towards understanding the structure&#x02013;function relationship of proteins (Paiardini et al., <xref ref-type="bibr" rid="B20">2005</xref>; Peters et al., <xref ref-type="bibr" rid="B21">2006</xref>; Shapiro and Brutlag, <xref ref-type="bibr" rid="B27">2004</xref>). In our earlier works (Chakrabarti and Sowdhamini, <xref ref-type="bibr" rid="B5">2003</xref>; Chakrabarti et al., <xref ref-type="bibr" rid="B6">2003</xref>, <xref ref-type="bibr" rid="B7">2006</xref>; Pugalenthi et al., <xref ref-type="bibr" rid="B22">2007</xref>), we identified structurally invariant segments at superfamily level where proteins are distantly related but retain similar fold and biological functions. These structural motifs were recognized on the basis of both sequence conservation and preservation of important structural properties, such as solvent accessibility, secondary structural content, hydrogen-bonding pattern and residue compactness. They are also found to maintain a similar spatial orientation pattern, when compared across different proteins belonging to the same family or superfamily. Therefore, these SCRs might be crucial for the formation of common structural core that provides optimal environment for the protein to perform its molecular or biological function. SCRs might also provide important clues as sequence&#x02013;structural signature of multiple folding units for each protein fold, and therefore can be extremely useful in protein engineering and design experiments.</p><p>Identification of SCRs is a difficult and challenging task as it requires careful examination of 3D structural homologs and development of reliable structural alignments. Additionally, many protein structures are reported to have limited or no structural homologs. For example, 566 out of 1194 superfamilies in PASS2 database have only single structural entry (Bhaduri et al.,<xref ref-type="bibr" rid="B4">2004</xref>). Therefore, identification of SCRs from protein structure that has limited or no homologous structural information is very important and poses a challenging task.</p><p>In this study, we propose a neural network ensemble (NNE) method that utilizes negative correlation learning (NCL) for classification and prediction of SCRs. As the availability of 3D structures and structural alignments are still limited in protein comparison studies, the NCL-NNE prediction approach provides a useful option that can successfully predict important structural residues utilizing a single protein structure.</p></sec><sec sec-type="methods" id="SEC2"><title>2 METHODS</title><sec id="SEC2.1"><title>2.1 The dataset</title><p>The dataset used for training and testing our algorithm was obtained from MegaMotifBase database (Pugalenthi et al., <xref ref-type="bibr" rid="B23">2008a</xref>), which contains protein structural motifs for structurally aligned protein domains related at the superfamily level. These structural motifs were identified by screening the superfamily alignment (structural alignment) positions for conservation of important structural properties, such as solvent accessibility, secondary structural content, hydrogen-bonding pattern and residue compactness. In addition to the structural motif definition for the superfamily, this database also provides structural motif information for each individual structure by consulting the structural alignments. Thus, SCRs for individual domain can be extracted from the structural motif definitions provided by the MegaMotifBase database (Pugalenthi et al.,<xref ref-type="bibr" rid="B23">2008a</xref>).</p><p>In this study, we used 191 superfamilies for classification. Out of 191, 131 superfamilies belonging to 14 All-&#x003b1;, 25 All-&#x003b2;, 47 &#x003b1;/&#x003b2;, 37 &#x003b1;+&#x003b2;, 4 small domains, 3 multidomain protein and 1 membrane/cell surface protein classes were selected for training and testing. From 131 superfamilies, 496 domains were chosen for training and testing. From the remaining 60 superfamilies that do not overlap with training and testing datasets, 60 protein domains were used for benchmarking study. Each protein domain sequence in our dataset has &#x0003c;40% sequence identity to any other sequences in the training, testing and benchmarking datasets (<ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/cgi/content/full/btn618/DC1">Supplementary Material 1 and 2</ext-link>).</p><p>We used 6042 SCRs (positive dataset) and 105 204 non-SCRs (negative dataset) that were obtained from the selected 496 domains for training and testing. To avoid imbalance between positive and negative (residues) datasets, we randomly selected 3021 SCRs from 6042 positive samples and 3021 non-SCRs from the negative samples for training. In the same way, the test data were constructed from the remaining 3021 positive samples and 3021 residues randomly chosen from the remaining negative samples (<ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/cgi/content/full/btn618/DC1">Supplementary Material 3 and 4</ext-link>). In addition, 1657 SCRs were obtained from 60 protein domains for benchmarking.</p></sec><sec id="SEC2.2"><title>2.2 Feature set</title><p>Each residue in the SCR dataset is represented by 212 features that include sequence and structural information extracted from the homologous alignment and 3D structure (<ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/cgi/content/full/btn618/DC1">Supplementary Material 5</ext-link>). Homologous sequences for each protein domain were obtained using five rounds of PSI-BLAST (Altschul et al., <xref ref-type="bibr" rid="B1">1997</xref>) against NCBI non-redundant protein database, with an <italic>E</italic>-value cutoff of 0.001. CLUSTALW (Thompson et al., <xref ref-type="bibr" rid="B28">1994</xref>) was used to align the homologous sequences. Sequences having &#x0003c;80% of the length of the query structure were removed from the alignment. Secondary structural information was assigned for all sequence homologs in the alignment using PSIPRED (McGuffin et al., <xref ref-type="bibr" rid="B16">2000</xref>). The details of the features used in this study are briefly mentioned below.</p><p>Conservation score: sequence conservation score for each alignment position was evaluated consulting a standard 20 &#x000d7; 20 substitution matrix (Johnson and Overington, <xref ref-type="bibr" rid="B10">1993</xref>).</p><p>Amino acid type and functional groups: we categorized 20 amino acids into 10 functional groups based on the presence of side chain chemical group, such as phenyl (F/W/Y), carboxyl (D/E), imidazole (H), primary amine (K), guanidino (R), thiol (C), sulfur (M), amido (Q/N), hydroxyl (S/T) and non-polar (A/G/I/L/V/P). The compositional diversities of each SCR were evaluated by calculating the frequency of 20 amino acids and 10 functional groups within the SCR alignment positions.</p><p>Structural features: structural features, such as solvent accessibility, secondary structures, hydrogen bonds and residue compactness were computed from the individual protein structure using the JOY package (Mizuguchi et al., <xref ref-type="bibr" rid="B18">1998</xref>).</p><p>Physico-chemical properties: matrices containing quantitative values for amino acids' physico-chemical properties scaled between 0 and 1 were obtained from the UMBC AAIndex database (Kawashima et al., <xref ref-type="bibr" rid="B11">1999</xref>). The selected physico-chemical properties include molecular weight, hydrophobicity, hydrophilicity, hydration potential, refractivity, average accessible surface area, free energy transfer, flexibility, residue volume, mutability, melting point, optical activity, side chain volume, polarity and isoelectric points.</p><p>Sequence and structural features from spatial neighbors: spatially neighboring residues were shown to have positive influence in identification of critical sites in proteins (Pugalenthi et al., <xref ref-type="bibr" rid="B24">2008b</xref>). The residues whose C<sup>&#x003b2;</sup> atoms were found within 5 &#x000c5; distance from the C<sup>&#x003b2;</sup> of a SCR were considered as spatial neighbors of the SCR. In case of glycine, a virtual C<sup>&#x003b2;</sup> atom was considered. Content of amino acid type and functional groups, structural features and physico-chemical property values were computed from all spatial neighbors for each SCR.</p></sec><sec id="SEC2.3"><title>2.3 Classification protocol</title><p>The classification model presented in this article was built through three steps. First, all the input features of training data were normalized to be between -1 and 1 by a linear function. Then, a NNE was trained by a method called NCL (Liu and Yao, <xref ref-type="bibr" rid="B12">1997</xref>, <xref ref-type="bibr" rid="B13">1999a</xref>, <xref ref-type="bibr" rid="B14">b</xref>; Yao et al., <xref ref-type="bibr" rid="B31">2001</xref>). Finally, &#x02018;feature selection&#x02019; was conducted to investigate whether we can still achieve good prediction performance with only a subset of features.</p><p>NCL approach is widely used for training the NN ensembles. Below, we briefly describe the basic ideas and steps of the NCL and readers are requested to refer the original publications (Liu and Yao, <xref ref-type="bibr" rid="B12">1997</xref>, <xref ref-type="bibr" rid="B13">1999a</xref>, <xref ref-type="bibr" rid="B14">b</xref>; Yao et al., <xref ref-type="bibr" rid="B31">2001</xref>) for full details.</p><p>Suppose that we have a training set of size <italic>N</italic>, denoted by<disp-formula><graphic xlink:href="btn618um1.jpg" position="float"/></disp-formula>where x &#x02208; <bold>R</bold><sup><italic>d</italic></sup> is the <italic>d</italic>-dimensional training samples and y is the corresponding class labels. NCL is designed to train a NN ensemble of the form:<disp-formula id="M1"><label>(1)</label><graphic xlink:href="btn618m1.jpg" position="float"/></disp-formula>where <italic>M</italic> is the number of the individual NNs in the ensemble. <italic>F</italic><sub><italic>i</italic></sub>(<italic>n</italic>) is the output of the <italic>i</italic>-th NN on the <italic>n</italic>-th training sample and <italic>F</italic>(<italic>n</italic>) is the output of the ensemble on the <italic>n</italic>-th training sample.</p><p>NCL employs the standard back-propagation algorithm to train the individual NNs in parallel. The key to the success of NCL is the use of the error function. NCL uses the sum of the mean squared error (MSE) and a penalty term as the error function during the learning process. When the <italic>n</italic>-th training sample is presented, the <italic>i</italic>-th NN is trained to minimize the error function:<disp-formula id="M2"><label>(2)</label><graphic xlink:href="btn618m2.jpg" position="float"/></disp-formula>(where &#x003bb; is a positive parameter controlling the tradeoff between the MSE (accuracy) and <italic>y</italic> is the class label and the penalty term (diversity) can be calculated by: <disp-formula id="M3"><label>(3)</label><graphic xlink:href="btn618m3.jpg" position="float"/></disp-formula>It can be seen from Equation (<xref ref-type="disp-formula" rid="M3">3</xref>) that the penalty term explicitly encourages the <italic>i</italic>-th NN to be negatively correlated with the remaining NNs in the ensemble. By this means, diversity among the individual NNs is achieved. It can also be seen that with &#x003bb; =0 we would have an ensemble exactly equivalent to training a set of NNs independently of one another. When &#x003bb; is increased, more and more emphasis would be placed on seeking the negative correlation.</p><p>The NN ensemble used in this work has five NNs. Each individual NN is a feed forward network with one hidden layer. The number of hidden neurons is set to five for all individual NNs. When employing NCL to train the NN ensemble &#x003bb; is set to 1 and the number of learning epochs is set to 100.</p></sec><sec id="SEC2.4"><title>2.4 Feature selection</title><p>Since the number of features in this study is high, we conducted feature selection to decrease the size of the features by omitting the non-effective features. We designed a wrapper approach to conduct feature selection for our dataset. A feature selection method typically consists of two main components: a selection criterion and a search scheme. The selection criterion measures the usefulness of any feature subset, and feature selection seeks the feature subset that optimizes the selection criterion. The search scheme determines how to search for the optimal feature subset among all possible combinations of features. In this work, the Matthew's Correlation Coefficient (MCC) defined in Equation (<xref ref-type="disp-formula" rid="M7">7</xref>) was used as the selection criterion, and we adopted a sequential backward elimination (or recursive feature elimination) (Webb, <xref ref-type="bibr" rid="B30">2002</xref>) search scheme in this study. The feature selection procedure is described briefly in the following.</p><p>We trained the NN ensemble using the whole feature set (i.e. the original training dataset). After that, the trained NN ensemble was preserved and the MCC was calculated. Then, starting from the whole feature set, features were iteratively pruned. For each individual NN, we removed the input neurons (and the weights associated to them) that correspond to the omitted features, while keeping all the other structure of the NN unchanged. The output of the NN ensemble was obtained using Equation (<xref ref-type="disp-formula" rid="M1">1</xref>) and the MCC was calculated based on it. At each iteration, the feature whose omission led to the largest MCC was pruned. The feature selection procedure terminates when a predefined number of features have been pruned.</p></sec><sec id="SEC2.5"><title>2.5 Performance measures</title><p>Four different parameters have been used to measure the performance of the prediction method. These four parameters can be derived from the four scalar values: TP (true positives: number of correctly classified SCR), TN (true negatives: number of correctly classified non-SCR), FP (false positives: number of non-SCR incorrectly classified as SCR) and FN (false negatives: number of SCR incorrectly classified as non-SCR). Using the following formulas, we calculated sensitivity, specificity, positive prediction value (PPV) and MCC.<disp-formula id="M4"><label>(4)</label><graphic xlink:href="btn618m4.jpg" position="float"/></disp-formula><disp-formula id="M5"><label>(5)</label><graphic xlink:href="btn618m5.jpg" position="float"/></disp-formula><disp-formula id="M6"><label>(6)</label><graphic xlink:href="btn618m6.jpg" position="float"/></disp-formula><disp-formula id="M7"><label>(7)</label><graphic xlink:href="btn618m7.jpg" position="float"/></disp-formula></p></sec></sec><sec sec-type="results" id="SEC3"><title>3 RESULTS</title><sec id="SEC3.1"><title>3.1 Distribution of SCRs</title><p>We collected 6042 SCRs from 131 protein superfamilies enlisted in MegaMotifBase database (Pugalenthi <italic>et al</italic>., <xref ref-type="bibr" rid="B23">2008a</xref>). Generally SCRs maintain a basal level of sequence conservation (&#x02265;30% sequence identity), but there could be more conserved residues in proteins than the SCRs. As shown in <xref ref-type="table" rid="T1">Table 1</xref>, there are totally 54 958 residues from 496 domains that fall within the residue conservation range of 30&#x02013;100%. Out of 54 958 residues, only 6042 are SCRs. Therefore, only 11% of sequentially conserved residues account for the SCRs. This observation suggests that although the SCRs are conserved in the sequence, it is difficult to specifically identify the SCRs just by looking at the residue conservation score.<table-wrap id="T1" position="float"><label>Table 1.</label><caption><p>Residue conservation between SCR and non-SCR residues</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Residue conservation (%)</th><th rowspan="1" colspan="1">No. of residues</th><th rowspan="1" colspan="1">No. of SCR</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">30&#x02013;40</td><td rowspan="1" colspan="1">31648</td><td rowspan="1" colspan="1">2518</td></tr><tr><td rowspan="1" colspan="1">41&#x02013;50</td><td rowspan="1" colspan="1">15848</td><td rowspan="1" colspan="1">1976</td></tr><tr><td rowspan="1" colspan="1">51&#x02013;60</td><td rowspan="1" colspan="1">5226</td><td rowspan="1" colspan="1">1122</td></tr><tr><td rowspan="1" colspan="1">61&#x02013;70</td><td rowspan="1" colspan="1">1620</td><td rowspan="1" colspan="1">254</td></tr><tr><td rowspan="1" colspan="1">71&#x02013;80</td><td rowspan="1" colspan="1">276</td><td rowspan="1" colspan="1">88</td></tr><tr><td rowspan="1" colspan="1">81&#x02013;90</td><td rowspan="1" colspan="1">238</td><td rowspan="1" colspan="1">58</td></tr><tr><td rowspan="1" colspan="1">91&#x02013;100</td><td rowspan="1" colspan="1">102</td><td rowspan="1" colspan="1">26</td></tr><tr><td rowspan="1" colspan="1">Total</td><td rowspan="1" colspan="1">54958</td><td rowspan="1" colspan="1">6042</td></tr></tbody></table></table-wrap></p><p>Further, we examined the spatial distances between all pairs of 6042 SCRs calculated from each protein domain (<xref ref-type="fig" rid="F1">Fig. 1</xref>). <xref ref-type="fig" rid="F1">Figure 1</xref> provides the distance distribution of SCRs along with distances (C<sup>&#x003b2;</sup>&#x02013;C<sup>&#x003b2;</sup> distances) calculated from all non-SCR pair as well as randomly selected non-SCR pairs (numbers equal to the SCR pairs). From this distance distribution, it can be seen that higher number SCR pair distances fall in lower distance bins (&#x0003c;20 &#x000c5;) compared with that of non-SCRs. Therefore, it is reasonable to state that SCRs prefer a probable requirement of spatial proximity.<fig id="F1" position="float"><label>Fig. 1.</label><caption><p>Distribution of spatial distances between pairs of SCRs. Spatial distance between two SCRs was calculated utilizing the C<sup>&#x003b2;</sup>&#x02013;C<sup>&#x003b2;</sup> atom coordinates supplied in the individual PDB (Berman et al., <xref ref-type="bibr" rid="B3">2000</xref>) file.</p></caption><graphic xlink:href="btn618f1"/></fig></p></sec><sec id="SEC3.2"><title>3.2 Prediction of SCR</title><p>We employed the NCL-NNE for classification and subsequent prediction of SCRs in proteins. The NCL-NNE was trained using the training dataset containing 3021 SCRs (positive samples) and 3021 non-SCRs (negative samples), while the performance of the classifier was tested on the testing dataset containing the remaining 3021 SCRs and 3021 randomly selected negative samples. Our NCL-NNE method achieved 92.8% sensitivities with MCC score of 0.852 in the testing data using all the 212 features that represents the compositional and conservational properties of SCRs (<xref ref-type="table" rid="T2">Table 2</xref>).<table-wrap id="T2" position="float"><label>Table 2.</label><caption><p>Classification results achieved for the testing data using different feature subsets</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">No. of features</th><th rowspan="1" colspan="1">Sensitivity (%)</th><th rowspan="1" colspan="1">Specificity (%)</th><th rowspan="1" colspan="1">PPV (%)</th><th rowspan="1" colspan="1">MCC</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">92.59</td><td rowspan="1" colspan="1">85.95</td><td rowspan="1" colspan="1">86.83</td><td rowspan="1" colspan="1">0.787</td></tr><tr><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">90.47</td><td rowspan="1" colspan="1">91.30</td><td rowspan="1" colspan="1">91.23</td><td rowspan="1" colspan="1">0.818</td></tr><tr><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">92.19</td><td rowspan="1" colspan="1">91.63</td><td rowspan="1" colspan="1">91.68</td><td rowspan="1" colspan="1">0.836</td></tr><tr><td rowspan="1" colspan="1">50</td><td rowspan="1" colspan="1">91.03</td><td rowspan="1" colspan="1">93.52</td><td rowspan="1" colspan="1">93.35</td><td rowspan="1" colspan="1">0.846</td></tr><tr><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">91.72</td><td rowspan="1" colspan="1">92.73</td><td rowspan="1" colspan="1">92.66</td><td rowspan="1" colspan="1">0.845</td></tr><tr><td rowspan="1" colspan="1">150</td><td rowspan="1" colspan="1">92.06</td><td rowspan="1" colspan="1">92.57</td><td rowspan="1" colspan="1">92.53</td><td rowspan="1" colspan="1">0.846</td></tr><tr><td rowspan="1" colspan="1">200</td><td rowspan="1" colspan="1">91.00</td><td rowspan="1" colspan="1">93.20</td><td rowspan="1" colspan="1">93.05</td><td rowspan="1" colspan="1">0.842</td></tr><tr><td rowspan="1" colspan="1">212</td><td rowspan="1" colspan="1">92.82</td><td rowspan="1" colspan="1">92.50</td><td rowspan="1" colspan="1">92.52</td><td rowspan="1" colspan="1">0.852</td></tr></tbody></table></table-wrap></p><p>We applied a feature reduction protocol utilizing seven feature subsets to eliminate the redundant features. As seen in <xref ref-type="table" rid="T2">Table 2</xref>, feature selection (reduction) generally does not deteriorate the classification performance much until the number of features decreases below 10. Before that, the usage of smaller number of features only leads to a very little decrease in the sensitivity and specificity rates. We also investigated the influence of the feature reduction by plotting receiver operating characteristic (ROC) curves (<xref ref-type="fig" rid="F2">Fig. 2</xref>) derived from the sensitivity (TP rate) and specificity (FP rate) values for the classifiers using all the features and the 10 best performing features, respectively. <xref ref-type="fig" rid="F2">Figure. 2</xref> shows that the classifiers built with the 10 features and the whole feature set perform comparably. Such observation is also supported by the similar values (0.9682 for 10 features and 0.9762 for all features) of the area under curve (AUC) obtained from the ROC curves.<fig id="F2" position="float"><label>Fig. 2.</label><caption><p>ROC curves were plotted utilizing the fractions of TP and FP values derived using top 10 features and all features.</p></caption><graphic xlink:href="btn618f2"/></fig></p><p>Although the trained NCL-NNE shows good performance on the testing data, it is natural to ask whether the performance of NCL-NNE depends on any specific split of training and testing data. To verify this issue, we also conducted 5-fold cross-validation procedures on the training data. For each feature subsets presented in the <xref ref-type="table" rid="T2">Table 2</xref>, the corresponding performance measures achieved in the 5-fold cross-validation procedure are also provided in <xref ref-type="table" rid="T3">Table 3</xref>. If NCL-NNE exhibits significantly different performance in the cross-validation and testing procedure, then its performance highly depends on the training data. If NCL-NNE shows similar performance in the two scenarios, we may expect the NN ensemble trained with it generalizes well to unseen data. It can be observed from <xref ref-type="table" rid="T2">Tables 2</xref> and <xref ref-type="table" rid="T3">3</xref> that NCL-NNE performed more or less similar in the two scenarios. Therefore, we can conclude that NCL-NNE is not very sensitive to different training data, and thus our final NN ensemble generalizes well.<table-wrap id="T3" position="float"><label>Table 3.</label><caption><p>Classification results achieved for the training data using 5-fold cross-validation on different feature subsets</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">No. of features</th><th rowspan="1" colspan="1">Sensitivity (%)</th><th rowspan="1" colspan="1">Specificity(%)</th><th rowspan="1" colspan="1">PPV (%)</th><th rowspan="1" colspan="1">MCC</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">93.81 (1.60)</td><td rowspan="1" colspan="1">79.84 (3.37)</td><td rowspan="1" colspan="1">82.70 (2.26)</td><td rowspan="1" colspan="1">0.747 (0.014)</td></tr><tr><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">92.09 (1.49)</td><td rowspan="1" colspan="1">85.44 (2.77)</td><td rowspan="1" colspan="1">86.68 (2.12)</td><td rowspan="1" colspan="1">0.780 (0.012)</td></tr><tr><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">94.34 (1.10)</td><td rowspan="1" colspan="1">87.62 (2.24)</td><td rowspan="1" colspan="1">88.62 (1.77)</td><td rowspan="1" colspan="1">0.823 (0.012)</td></tr><tr><td rowspan="1" colspan="1">50</td><td rowspan="1" colspan="1">94.17 (0.95)</td><td rowspan="1" colspan="1">90.10 (0.99)</td><td rowspan="1" colspan="1">90.54 (0.79)</td><td rowspan="1" colspan="1">0.844 (0.006)</td></tr><tr><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">93.41 (1.36)</td><td rowspan="1" colspan="1">90.60 (1.23)</td><td rowspan="1" colspan="1">90.94 (0.98)</td><td rowspan="1" colspan="1">0.842 (0.008)</td></tr><tr><td rowspan="1" colspan="1">150</td><td rowspan="1" colspan="1">92.68 (0.51)</td><td rowspan="1" colspan="1">91.99 (0.70)</td><td rowspan="1" colspan="1">92.07 (0.62)</td><td rowspan="1" colspan="1">0.847 (0.006)</td></tr><tr><td rowspan="1" colspan="1">200</td><td rowspan="1" colspan="1">92.68 (0.33)</td><td rowspan="1" colspan="1">92.02 (0.41)</td><td rowspan="1" colspan="1">92.08 (0.36)</td><td rowspan="1" colspan="1">0.847 (0.004)</td></tr><tr><td rowspan="1" colspan="1">212</td><td rowspan="1" colspan="1">93.03 (0.10)</td><td rowspan="1" colspan="1">91.48 (0.13)</td><td rowspan="1" colspan="1">91.61 (0.12)</td><td rowspan="1" colspan="1">0.845 (0.002)</td></tr></tbody></table><table-wrap-foot><fn><p>Statistical errors (standard error) associated with the average sensitivity, specificity, PPV and MCC are provided within the parenthesis.</p></fn></table-wrap-foot></table-wrap></p><p>In order to check whether the high accuracy is due to the NCL-NNE classifier or the quality of the selected features, we applied a linear model on our datasets. We obtained good prediction rate using all features (sensitivity 83.45% and specificity 83.28%) and 10 features (sensitivity 90.90% and specificity 89.47%). This result shows that the quality of the best performing features selected by our &#x02018;feature selection&#x02019; approach play an important role in successful classification. However, NCL-NNE reported higher sensitivity and specificity rates than the linear model signifying its importance for better performance.</p></sec><sec id="SEC3.3"><title>3.3 Influence of structural features and spatial neighbors</title><p><xref ref-type="table" rid="T4">Table 4</xref> shows the list of 10 best performing features. Eight out of the 10 best performing features that were automatically selected by the classifier involve features that represent structural properties, such as solvent accessibility, secondary structures, hydrogen bonding and residue compactness for a given SCR and its spatial neighbors. This finding emphasizes that important structural properties retrieved from a single protein can be successfully used in machine learning classification for identification of sites that are conserved for such properties across similar protein structures. Our finding also indicates that the environment of neighboring residues of the SCRs can be an important factor towards better classification and identification of SCRs.<table-wrap id="T4" position="float"><label>Table 4.</label><caption><p>List of best performing features</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Feature</th><th rowspan="1" colspan="1">SCR related</th><th rowspan="1" colspan="1">SCR neighbors related</th><th rowspan="1" colspan="1">Structural feature</th><th rowspan="1" colspan="1">Sequence feature</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">Helix content in SCR</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td></tr><tr><td rowspan="1" colspan="1">Strand content in SCR</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td></tr><tr><td rowspan="1" colspan="1">Coil content in the SCR</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td></tr><tr><td rowspan="1" colspan="1">Helix content in the spatial neighbor</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td></tr><tr><td rowspan="1" colspan="1">Solvent accessibility in SCR</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td></tr><tr><td rowspan="1" colspan="1">Hydrogen bonding information in SCR</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td></tr><tr><td rowspan="1" colspan="1">Residue compactness in SCR</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td></tr><tr><td rowspan="1" colspan="1">Residue compactness in the spatial neighbor</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td></tr><tr><td rowspan="1" colspan="1">Leucine content in spatial neighbor</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">Yes</td></tr><tr><td rowspan="1" colspan="1">Cysteine content in SCR</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td></tr></tbody></table></table-wrap></p><p>As shown in <xref ref-type="fig" rid="F3">Figure 3</xref>a, we found that significantly higher number of SCRs prefer aliphatic, hydrophobic residues (73.06%, 76.61%, 86.58% and 80.75% of SCRs has at least one alanine, isoleucine, leucine and valine, respectively, as their structural neighbor) surrounding themselves compared with that of non-SCRs. Similarly, lower fractions of SCRs prefer charged residues (aspartic acid: 37.32%, glutamic acid: 35.50%, histidine: 24.20% and arginine: 35.37%) as their structural neighbor. <xref ref-type="fig" rid="F3">Figure 3</xref>b compares the fraction of each amino acid within the spatial neighbors of SCRs and non-SCRs where fraction of each amino acid is normalized by the overall background frequency of that particular residue. Higher preference of aliphatic, hydrophobic residues as spatial neighbors for the SCRs is also observed in <xref ref-type="fig" rid="F3">Figure 3</xref>b.<fig id="F3" position="float"><label>Fig. 3.</label><caption><p>Distribution of 20 amino acid type within the spatial neighbors of SCRs. (a) Shows the percentage of residues having at least one of the 20 amino acids within their spatial neighbor whereas (b) provides the fraction of each amino acid within the spatial neighbor. White bars with standard error from five trials provide data obtained from randomly selected non-SCRs.</p></caption><graphic xlink:href="btn618f3"/></fig></p><p>We also trained our NCL-NNE on different feature subsets that were formed by grouping the qualitatively similar features together. We categorized all the 212 features into four different groups. Group 1 and group 2 contain features that represent the amino acids' composition and conservation for a given SCR and its spatial neighbors, respectively. Group 3 contains physico-chemical property features computed for a given SCR and its spatial neighbors. Similarly, group 4 represents the structural property features for a given SCR and its spatial neighbors. We test the performance of the classifier utilizing these feature groups separately as well as mixing them in all possible combinations. The performance of the NCL-NNE utilizing various combinations of feature groups is summarized in <xref ref-type="table" rid="T5">Table 5</xref>. <xref ref-type="table" rid="T5">Table 5</xref> presents the results obtained via conducting 5-fold cross-validation on the training set and the results obtained on the testing set. As can be observed, NCL-NNE again performed similar in the two cases. Among the feature groups, group 4 alone performs quite well, in fact achieved the highest sensitivity (97.38%). This result further ascertains the importance of structural properties of the SCRs and its neighbors, and thereby supports their (structural property features) selection as best performing ones by the automated feature reduction protocol used in this study. However, the specificity of the prediction is compromised when only group 4 features were used. It can also be noticed that combination of structural property features (group 4) together with features belonging to other three groups significantly improves the specificity (92.50%), while marginally decreasing the sensitivity (92.82%) value.<table-wrap id="T5" position="float"><label>Table 5.</label><caption><p>Evaluation of performance of different feature groups</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">No. of features</th><th rowspan="1" colspan="1">Sensitivity using 5-fold CV (%)</th><th rowspan="1" colspan="1">Sensitivity without CV (%)</th><th rowspan="1" colspan="1">Specificity using 5-fold CV (%)</th><th rowspan="1" colspan="1">Specificity without CV (%)</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">Group 1</td><td rowspan="1" colspan="1">33.79 (10.03)</td><td rowspan="1" colspan="1">30.22</td><td rowspan="1" colspan="1">89.57 (2.79)</td><td rowspan="1" colspan="1">93.77</td></tr><tr><td rowspan="1" colspan="1">Group 2</td><td rowspan="1" colspan="1">11.35 (3.46)</td><td rowspan="1" colspan="1">7.55</td><td rowspan="1" colspan="1">97.85 (1.06)</td><td rowspan="1" colspan="1">98.83</td></tr><tr><td rowspan="1" colspan="1">Group 3</td><td rowspan="1" colspan="1">41.90 (9.90)</td><td rowspan="1" colspan="1">41.31</td><td rowspan="1" colspan="1">90.41 (4.79)</td><td rowspan="1" colspan="1">85.48</td></tr><tr><td rowspan="1" colspan="1">Group 4</td><td rowspan="1" colspan="1">97.91 (0.26)</td><td rowspan="1" colspan="1">97.38</td><td rowspan="1" colspan="1">79.11 (1.87)</td><td rowspan="1" colspan="1">79.13</td></tr><tr><td rowspan="1" colspan="1">Group 1+2</td><td rowspan="1" colspan="1">13.01 (3.66)</td><td rowspan="1" colspan="1">10.76</td><td rowspan="1" colspan="1">98.94 (0.28)</td><td rowspan="1" colspan="1">99.07</td></tr><tr><td rowspan="1" colspan="1">Group 1+3</td><td rowspan="1" colspan="1">44.15 (8.53)</td><td rowspan="1" colspan="1">32.17</td><td rowspan="1" colspan="1">93.12 (2.43)</td><td rowspan="1" colspan="1">96.12</td></tr><tr><td rowspan="1" colspan="1">Group 1+4</td><td rowspan="1" colspan="1">96.62 (0.36)</td><td rowspan="1" colspan="1">96.46</td><td rowspan="1" colspan="1">85.57 (1.14)</td><td rowspan="1" colspan="1">85.31</td></tr><tr><td rowspan="1" colspan="1">Group 2+3</td><td rowspan="1" colspan="1">26.45 (6.88)</td><td rowspan="1" colspan="1">12.45</td><td rowspan="1" colspan="1">96.79 (1.07)</td><td rowspan="1" colspan="1">99.17</td></tr><tr><td rowspan="1" colspan="1">Group 2+4</td><td rowspan="1" colspan="1">95.13 (0.61)</td><td rowspan="1" colspan="1">94.97</td><td rowspan="1" colspan="1">88.25 (1.62)</td><td rowspan="1" colspan="1">88.80</td></tr><tr><td rowspan="1" colspan="1">Group 3+4</td><td rowspan="1" colspan="1">97.85 (0.26)</td><td rowspan="1" colspan="1">97.25</td><td rowspan="1" colspan="1">79.48 (2.19)</td><td rowspan="1" colspan="1">80.47</td></tr><tr><td rowspan="1" colspan="1">Group 1+2+3</td><td rowspan="1" colspan="1">22.28 (4.36)</td><td rowspan="1" colspan="1">14.86</td><td rowspan="1" colspan="1">98.54 (0.51)</td><td rowspan="1" colspan="1">99.22</td></tr><tr><td rowspan="1" colspan="1">Group 1+2+4</td><td rowspan="1" colspan="1">92.85 (0.22)</td><td rowspan="1" colspan="1">93.11</td><td rowspan="1" colspan="1">91.89 (0.78)</td><td rowspan="1" colspan="1">92.18</td></tr><tr><td rowspan="1" colspan="1">Group 1+3+4</td><td rowspan="1" colspan="1">96.29 (0.63)</td><td rowspan="1" colspan="1">96.19</td><td rowspan="1" colspan="1">85.67 (1.28)</td><td rowspan="1" colspan="1">86.28</td></tr><tr><td rowspan="1" colspan="1">Group 2+3+4</td><td rowspan="1" colspan="1">95.10 (0.65)</td><td rowspan="1" colspan="1">94.74</td><td rowspan="1" colspan="1">88.28 (1.60)</td><td rowspan="1" colspan="1">89.73</td></tr><tr><td rowspan="1" colspan="1">All Groups</td><td rowspan="1" colspan="1">93.03 (0.10)</td><td rowspan="1" colspan="1">92.82</td><td rowspan="1" colspan="1">91.48 (0.13)</td><td rowspan="1" colspan="1">92.50</td></tr></tbody></table><table-wrap-foot><fn><p>Statistical errors (standard error) associated with the average sensitivity are provided within the parenthesis. CV, cross-validation.</p></fn></table-wrap-foot></table-wrap></p></sec><sec id="SEC3.4"><title>3.4 Benchmarking studies</title><p>To test the capability, we applied the NCL-NNE to 60 protein domains obtained from 60 superfamilies for the prediction of SCRs. These 60 domains contain 1657 SCRs that do not overlap with the training and test datasets. The NCL-NNE prediction module correctly predicts 1497 SCRs (out of 1657 SCRs) with 90.3% sensitivity and 89.2% specificity. Further, the performance of our approach was compared with recently reported CUSP algorithm (Sandhya et al., <xref ref-type="bibr" rid="B25">2008</xref>) that utilizes protein's structural homologs and structural alignments to distinguish structurally conserved regions. CUSP method correctly predicts 1485 SCRs with 89.6% sensitivity (<ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/cgi/content/full/btn618/DC1">Supplementary Material 6</ext-link>). This suggests that the result obtained from our method is very similar to CUSP result obtained from high quality structural alignments. Importantly, our method achieves high sensitivity rate in absence of homologous structural information and structural alignments.</p><p>In order to show the structural and functional importance of SCR, we applied NCL-NNE for the prediction of SCRs from three protein structures (<xref ref-type="fig" rid="F4">Fig 4</xref>). <xref ref-type="fig" rid="F4">Figure 4</xref>a shows three-dimensional structure of wild-type CheY from <italic>Escherichia coli</italic> (PDB code: 3chy) (Lopez-Hernandez and Serrano, <xref ref-type="bibr" rid="B15">1996</xref>). Our approach predicts 15 residues as SCRs (F8, L9, V10, V11, A42, F53, V54, I55, S56, D57, L68, V83, L84, M85 and V86). Out of 15 predicted SCRs, five residues V10, V11, A42, V54 and V57 (shown in purple ball and stick model in <xref ref-type="fig" rid="F4">Fig. 4</xref>a) were reported in the previous studies as a part of folding nuclei, which play important role in folding of the protein (Mirny and Shakhnovich, <xref ref-type="bibr" rid="B17">2001</xref>).<fig id="F4" position="float"><label>Fig. 4.</label><caption><p>Example of successful prediction of SCRs. SCRs predicted by NCL-NNE are shown in purple. Predicted SCRs that are experimentally verified are shown in ball and stick model. (a) Wild-type CheY from <italic>Escherichia coli</italic> (PDB code: 3CHY); (b) serum RBP (PDB code: 1JYD) and c) Cu&#x02013;Zn superoxide dismutase (SOD) (PDB code: 2SOD). Regular secondary structures are colored in blue (helix), green (strand) and yellow (loops).</p></caption><graphic xlink:href="btn618f4"/></fig></p><p>The structural importance of SCRs can be further explained by serum retinol binding protein (RBP), a member of the lipocalin family (PDB code: 1JYD) (<xref ref-type="fig" rid="F4">Fig. 4</xref>b). Our algorithm predicts 27 SCRs (W24, A26, K29, A43, E44, F45, M53, A55, G75, H104, W105, I106, V107, T109, Y114, A115, V116, Q117, Y118, S119, C120, Y133, S134, F135, V136, F137 and S138). This structure has four conserved tryptophans (W24, W67, W91 and W105) and W24 and W105 were predicted as SCRs by NCL-NNE. Greene et al. (<xref ref-type="bibr" rid="B9">2001</xref>) conducted conservative substitutions for the four tryptophans and observed that substitutions at W67 and W91 positions do not affect the overall structural integrity. Substitution of W105, which is largely buried in the wall of the <italic>&#x003b2;</italic> -barrel, has minor effect on the structure. Further they reported that mutation at W24 position leads to large losses in stability and lower yields of native protein generated by <italic>in vitro</italic> folding.</p><p>Though the SCRs are generally associated with structural stability, some of them might have functional role or provide optimal environment for the protein to perform its function. For example, H41 plays both catalytic and structural role in Cu&#x02013;Zn superoxide dismutase (SOD) (PDB code: 2SOD; <xref ref-type="fig" rid="F4">Fig. 4</xref>c). Twenty-one SCRs are predicted (A4, C6, L8, I18, V29, I33, H41, G42, F43, H44, V45, H46, D81, L82, V85, T114, M115, V116 and V117) for the Cu&#x02013;Zn SOD by NCL-NNE. Previous analysis by Toyama et al. (<xref ref-type="bibr" rid="B29">2004</xref>) suggest that H41 involves in hydrogen bonding with T37 and H118 and this H41-mediated hydrogen bonds (T37-H41-H118) play crucial role in keeping the protein structure suitable for its efficient catalytic reactions.</p></sec><sec id="SEC3.5"><title>3.5 Execution time</title><p>The execution time for our algorithm is reasonably faster. The procedure involves formulation of the features and prediction of SCRs using NCL-NNE model. In order to provide a flavor of the computation time for NCL-NNE, we randomly selected seven proteins with varying lengths and measured the user CPU time (<xref ref-type="table" rid="T6">Table 6</xref>) spent for the feature generation followed by prediction on a Pentium4 machine having 3 GHz CPU and 2 GB memory.<table-wrap id="T6" position="float"><label>Table 6.</label><caption><p>Execution time for NCL-NNE method</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Protein PDB code</th><th rowspan="1" colspan="1">Chain identifier</th><th rowspan="1" colspan="1">Length</th><th rowspan="1" colspan="1">Execution time (in s)</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">1BY5</td><td rowspan="1" colspan="1">A</td><td rowspan="1" colspan="1">698</td><td rowspan="1" colspan="1">74</td></tr><tr><td rowspan="1" colspan="1">1EZ0</td><td rowspan="1" colspan="1">A</td><td rowspan="1" colspan="1">504</td><td rowspan="1" colspan="1">57</td></tr><tr><td rowspan="1" colspan="1">1CPT</td><td rowspan="1" colspan="1">&#x02013;</td><td rowspan="1" colspan="1">412</td><td rowspan="1" colspan="1">43</td></tr><tr><td rowspan="1" colspan="1">1EZF</td><td rowspan="1" colspan="1">A</td><td rowspan="1" colspan="1">323</td><td rowspan="1" colspan="1">39</td></tr><tr><td rowspan="1" colspan="1">1A7T</td><td rowspan="1" colspan="1">A</td><td rowspan="1" colspan="1">227</td><td rowspan="1" colspan="1">31</td></tr><tr><td rowspan="1" colspan="1">1DOI</td><td rowspan="1" colspan="1">&#x02013;</td><td rowspan="1" colspan="1">128</td><td rowspan="1" colspan="1">25</td></tr><tr><td rowspan="1" colspan="1">2HPQ</td><td rowspan="1" colspan="1">P</td><td rowspan="1" colspan="1">79</td><td rowspan="1" colspan="1">21</td></tr></tbody></table></table-wrap></p></sec></sec><sec sec-type="conclusions" id="SEC4"><title>4 CONCLUSION</title><p>SCRs are crucial for the overall protein fold and can play important role in maintaining the suitable scaffold for the function of a protein. Identification of SCRs from single structure is a challenging task. Here, we implemented a NNE method that utilizes NCL approach for prediction of SCRs using features that represent the amino acid conservation and composition, physico-chemical properties and structural properties, such as solvent accessibility, secondary structures, hydrogen bonding and residue compactness. Validation of the NCL-NNE on the test dataset provided high sensitivity and quality of prediction (sensitivity: 92.8%, MCC: 0.852). Additional large-scale benchmarking using alignments of separate 60 protein domains shows 90.3% prediction sensitivity for the NCL-NNE. We also found that utilization of the structural features derived from the SCRs and their spatial neighbors are beneficial for successful classification and prediction. Our NCL-NNE prediction approach utilizes information derived from a single protein structure and its sequence homologs. Therefore, this method could be extremely useful for identification of SCRs in large-scale benchmarking studies where structural homologs and reliable structural alignments are still limited.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="PMC_1" content-type="local-data"><caption><title>[Supplementary Data]</title></caption><media mimetype="text" mime-subtype="html" xlink:href="btn618_index.html"/><media xlink:role="associated-file" mimetype="application" mime-subtype="vnd.ms-excel" xlink:href="btn618_bioinf-2008-1351-File002.xls"/><media xlink:role="associated-file" mimetype="application" mime-subtype="vnd.ms-excel" xlink:href="btn618_bioinf-2008-1351-File003.xls"/><media xlink:role="associated-file" mimetype="application" mime-subtype="vnd.ms-excel" xlink:href="btn618_bioinf-2008-1351-File004.xls"/><media xlink:role="associated-file" mimetype="application" mime-subtype="vnd.ms-excel" xlink:href="btn618_bioinf-2008-1351-File005.xls"/><media xlink:role="associated-file" mimetype="application" mime-subtype="vnd.ms-excel" xlink:href="btn618_bioinf-2008-1351-File006.xls"/><media xlink:role="associated-file" mimetype="application" mime-subtype="vnd.ms-excel" xlink:href="btn618_bioinf-2008-1351-File007.xls"/></supplementary-material></sec></body><back><ack><title>ACKNOWLEDGEMENTS</title><p>We thank Dr R. Sowdhamini and Sandhya Sankaran for providing CUSP results.</p><p><italic>Funding:</italic> A*Star (Agency for Science, Technology and Research to G.P. and P.N.S.); National Natural Science Foundation of China grant (No. 60802036 to K.T.); Intramural Research Program of the National Library of Medicine at National Institutes of Health/DHHS (to S.C.).</p><p><italic>Conflict of Interest:</italic> none declared.</p></ack><ref-list><title>REFERENCES</title><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Altschul</surname><given-names>SF</given-names></name><etal/></person-group><article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title><source>Nucleic Acids Res</source><year>1997</year><volume>25</volume><fpage>3389</fpage><lpage>3402</lpage><pub-id pub-id-type="pmid">9254694</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Anfinsen</surname><given-names>CB</given-names></name></person-group><article-title>Principles that govern the folding of protein chains</article-title><source>Science</source><year>1973</year><volume>181</volume><fpage>223</fpage><lpage>230</lpage><pub-id pub-id-type="pmid">4124164</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname><given-names>HM</given-names></name><etal/></person-group><article-title>The Protein Data Bank</article-title><source>Nucleic Acids Res.</source><year>2000</year><volume>28</volume><fpage>235</fpage><lpage>242</lpage><pub-id pub-id-type="pmid">10592235</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bhaduri</surname><given-names>A</given-names></name><etal/></person-group><article-title>PASS2: an automated database of protein alignments organised as structural superfamilies</article-title><source>BMC Bioinformatics</source><year>2004</year><volume>5</volume><fpage>35</fpage><pub-id pub-id-type="pmid">15059245</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chakrabarti</surname><given-names>S</given-names></name><name><surname>Sowdhamini</surname><given-names>R</given-names></name></person-group><article-title>Regions of minimal structural variation among members of protein domain superfamilies application to remote homology detection and modeling using distant relationships</article-title><source>FEBS Lett.</source><year>2003</year><volume>569</volume><fpage>31</fpage><lpage>36</lpage><pub-id pub-id-type="pmid">15225604</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chakrabarti</surname><given-names>S</given-names></name><etal/></person-group><article-title>SMoS: a database of structural motifs of superfamily</article-title><source>Protein Eng</source><year>2003</year><volume>16</volume><fpage>791</fpage><lpage>793</lpage><pub-id pub-id-type="pmid">14631067</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chakrabarti</surname><given-names>S</given-names></name><etal/></person-group><article-title>SSToSS - sequence-structural templates of single-member superfamilies</article-title><source>In Sillico Biol</source><year>2006</year><volume>6</volume><fpage>0029</fpage></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chothia</surname><given-names>C</given-names></name></person-group><article-title>Proteins. One thousand families for the molecular biologist</article-title><source>Nature</source><year>1992</year><volume>357</volume><fpage>543</fpage><lpage>544</lpage><pub-id pub-id-type="pmid">1608464</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Greene</surname><given-names>LH</given-names></name><etal/></person-group><article-title>Role of conserved residues in structure and stability: Tryptophans of human serum retinol-binding protein, a model for the lipocalin superfamily</article-title><source>Protein Sci</source><year>2001</year><volume>10</volume><fpage>2301</fpage><lpage>2316</lpage><pub-id pub-id-type="pmid">11604536</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>MS</given-names></name><name><surname>Overington</surname><given-names>JP</given-names></name></person-group><article-title>A structural basis for sequence comparisons. An evaluation of scoring methodologies</article-title><source>J. Mol. Biol.</source><year>1993</year><volume>233</volume><fpage>716</fpage><lpage>738</lpage><pub-id pub-id-type="pmid">8411177</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kawashima</surname><given-names>S</given-names></name><etal/></person-group><article-title>AAindex: amino acid index database</article-title><source>Nucleic Acids Res</source><year>1999</year><volume>27</volume><fpage>368</fpage><lpage>369</lpage><pub-id pub-id-type="pmid">9847231</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Yao</surname><given-names>X</given-names></name></person-group><article-title>Negatively correlated neural networks can produce best ensembles</article-title><source>Aust. J. Intell. Inf. Process. Syst</source><year>1997</year><volume>4</volume><fpage>176</fpage><lpage>185</lpage></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Yao</surname><given-names>X</given-names></name></person-group><article-title>Ensemble learning via negative correlation</article-title><source>Neural Netw.</source><year>1999a</year><volume>12</volume><fpage>1399</fpage><lpage>1404</lpage><pub-id pub-id-type="pmid">12662623</pub-id></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Yao</surname><given-names>X</given-names></name></person-group><article-title>Simultaneous training of negatively correlated neural networks in an ensemble</article-title><source>IEEE Trans. Syst. Man Cybern. B Cybern.</source><year>1999b</year><volume>29</volume><fpage>716</fpage><lpage>725</lpage><pub-id pub-id-type="pmid">18252352</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lopez-Hernandez</surname><given-names>E</given-names></name><name><surname>Serrano</surname><given-names>L</given-names></name></person-group><article-title>Structure of the transition state for folding of the 129 aa protein chey resembles that of a smaller protein, ci2</article-title><source>Fold. Des</source><year>1996</year><volume>1</volume><fpage>43</fpage><lpage>55</lpage><pub-id pub-id-type="pmid">9079363</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>McGuffin</surname><given-names>LJ</given-names></name><etal/></person-group><article-title>The PSIPRED protein structure prediction server</article-title><source>Bioinformatics</source><year>2000</year><volume>16</volume><fpage>404</fpage><lpage>405</lpage><pub-id pub-id-type="pmid">10869041</pub-id></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mirny</surname><given-names>L</given-names></name><name><surname>Shakhnovich</surname><given-names>E</given-names></name></person-group><article-title>Evolutionary conservation of the folding nucleus</article-title><source>J. Mol. Biol</source><year>2001</year><volume>308</volume><fpage>123</fpage><lpage>129</lpage><pub-id pub-id-type="pmid">11327757</pub-id></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mizuguchi</surname><given-names>K</given-names></name><etal/></person-group><article-title>JOY: protein sequence-structure representation and analysis</article-title><source>Bioinformatics</source><year>1998</year><volume>14</volume><fpage>617</fpage><lpage>623</lpage><pub-id pub-id-type="pmid">9730927</pub-id></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Neuwald</surname><given-names>AF</given-names></name><etal/></person-group><article-title>Gibbs motif sampling: detection of bacterial outer membrane protein repeats</article-title><source>Protein Sci</source><year>1995</year><volume>4</volume><fpage>1618</fpage><lpage>1632</lpage><pub-id pub-id-type="pmid">8520488</pub-id></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Paiardini</surname><given-names>A</given-names></name><etal/></person-group><article-title>CAMPO, SCR_FIND and CHC_FIND: a suite of web tools for computational structural biology</article-title><source>Nucleic Acids Res</source><year>2005</year><volume>33</volume><fpage>W50</fpage><lpage>W55</lpage><pub-id pub-id-type="pmid">15980521</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>B</given-names></name><etal/></person-group><article-title>Identification of similar regions of protein structures using integrated sequence and structure analysis tools</article-title><source>BMC Struct. Biol</source><year>2006</year><volume>6</volume><issue>4</issue></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pugalenthi</surname><given-names>G</given-names></name><etal/></person-group><article-title>SMotif: a server for structural motifs in proteins</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><fpage>637</fpage><lpage>638</lpage><pub-id pub-id-type="pmid">17237055</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pugalenthi</surname><given-names>G</given-names></name><etal/></person-group><article-title>MegaMotifBase: a database of structural motifs in protein families and superfamilies</article-title><source>Nucleic Acid Res</source><year>2008a</year><volume>36</volume><fpage>D218</fpage><lpage>D221</lpage><pub-id pub-id-type="pmid">17933773</pub-id></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pugalenthi</surname><given-names>G</given-names></name><etal/></person-group><article-title>Identification of catalytic residues from protein structure using support vector machine with sequence and structural features</article-title><source>Biochem. Biophys. Res. Commun</source><year>2008b</year><volume>367</volume><fpage>630634</fpage></citation></ref><ref id="B25"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sandhya</surname><given-names>S</given-names></name><etal/></person-group><article-title>CUSP: an algorithm to distinguish structurally conserved and unconserved regions in protein domain alignments and its application in the study of large length variations</article-title><source>BMC Struct. Biol</source><year>2008</year><volume>8</volume><fpage>28</fpage><pub-id pub-id-type="pmid">18513436</pub-id></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Saqi</surname><given-names>MA</given-names></name><name><surname>Sternberg</surname><given-names>MJ</given-names></name></person-group><article-title>Identification of sequence motifs from a set of proteins with related function</article-title><source>Protein Eng</source><year>1994</year><volume>7</volume><fpage>165</fpage><lpage>171</lpage><pub-id pub-id-type="pmid">8170920</pub-id></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shapiro</surname><given-names>J</given-names></name><name><surname>Brutlag</surname><given-names>D</given-names></name></person-group><article-title>FoldMiner: structural motif discovery using an improved superposition algorithm</article-title><source>Protein Sci</source><year>2004</year><volume>13</volume><fpage>278</fpage><lpage>294</lpage><pub-id pub-id-type="pmid">14691242</pub-id></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>JD</given-names></name><etal/></person-group><article-title>CLUSTAL W: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice</article-title><source>Nucleic Acids Res</source><year>1994</year><volume>22</volume><fpage>4673</fpage><lpage>4680</lpage><pub-id pub-id-type="pmid">7984417</pub-id></citation></ref><ref id="B29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Toyama</surname><given-names>A</given-names></name></person-group><article-title>Catalytic and structural role of a metal-free histidine residue in bovine Cu-Zn Superoxide dismutase</article-title><source>Biochemistry</source><year>2004</year><volume>43</volume><fpage>4670</fpage><lpage>4679</lpage><pub-id pub-id-type="pmid">15096035</pub-id></citation></ref><ref id="B30"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Webb</surname><given-names>AR</given-names></name></person-group><source>Statistical Pattern Recognition.</source><year>2002</year><publisher-loc>London</publisher-loc><publisher-name>John Wiley and Sons</publisher-name></citation></ref><ref id="B31"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>X</given-names></name><etal/></person-group><article-title>Neural network ensembles and their application to traffic flow prediction in telecommunications networks.</article-title><source>Proceedings of International Joint Conference on Neural Networks.</source><year>2001</year><publisher-loc>Washington DC 1</publisher-loc><publisher-name>IEEE Press</publisher-name><fpage>693</fpage><lpage>698</lpage></citation></ref></ref-list></back></article>