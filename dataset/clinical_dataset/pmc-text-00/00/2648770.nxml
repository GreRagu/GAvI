<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">19208149</article-id><article-id pub-id-type="pmc">2648770</article-id><article-id pub-id-type="publisher-id">1471-2105-10-S1-S47</article-id><article-id pub-id-type="doi">10.1186/1471-2105-10-S1-S47</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Semi-supervised protein subcellular localization</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Xu</surname><given-names>Qian</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>fleurxq@ust.hk</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Hu</surname><given-names>Derek Hao</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>derekhh@cse.ust.hk</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Xue</surname><given-names>Hong</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I3">3</xref><email>hxue@ust.hk</email></contrib><contrib id="A4" contrib-type="author"><name><surname>Yu</surname><given-names>Weichuan</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I4">4</xref><email>eeyu@ust.hk</email></contrib><contrib id="A5" contrib-type="author"><name><surname>Yang</surname><given-names>Qiang</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><email>qyang@cse.ust.hk</email></contrib></contrib-group><aff id="I1"><label>1</label>Program of Bioengineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong</aff><aff id="I2"><label>2</label>Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong</aff><aff id="I3"><label>3</label>Department of Biochemistry, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong</aff><aff id="I4"><label>4</label>Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong</aff><pub-date pub-type="collection"><year>2009</year></pub-date><pub-date pub-type="epub"><day>30</day><month>1</month><year>2009</year></pub-date><volume>10</volume><issue>Suppl 1</issue><supplement><named-content content-type="supplement-title">Selected papers from the Seventh Asia-Pacific Bioinformatics Conference (APBC 2009)</named-content><named-content content-type="supplement-editor">Michael Q Zhang, Michael S Waterman and Xuegong Zhang</named-content></supplement><fpage>S47</fpage><lpage>S47</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10/S1/S47"/><permissions><copyright-statement>Copyright &#x000a9; 2009 Xu et al; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2009</copyright-year><copyright-holder>Xu et al; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an open access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>               Xu               Qian                              fleurxq@ust.hk            </dc:author><dc:title>            Semi-supervised protein subcellular localization         </dc:title><dc:date>2009</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 10(Suppl 1): S47-. (2009)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2009)10:Suppl 1&#x0003c;S47&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>Protein subcellular localization is concerned with predicting the location of a protein within a cell using computational method. The location information can indicate key functionalities of proteins. Accurate predictions of subcellular localizations of protein can aid the prediction of protein function and genome annotation, as well as the identification of drug targets. Computational methods based on machine learning, such as support vector machine approaches, have already been widely used in the prediction of protein subcellular localization. However, a major drawback of these machine learning-based approaches is that a large amount of data should be labeled in order to let the prediction system learn a classifier of good generalization ability. However, in real world cases, it is laborious, expensive and time-consuming to experimentally determine the subcellular localization of a protein and prepare instances of labeled data.</p></sec><sec><title>Results</title><p>In this paper, we present an approach based on a new learning framework, semi-supervised learning, which can use much fewer labeled instances to construct a high quality prediction model. We construct an initial classifier using a small set of labeled examples first, and then use unlabeled instances to refine the classifier for future predictions.</p></sec><sec><title>Conclusion</title><p>Experimental results show that our methods can effectively reduce the workload for labeling data using the unlabeled data. Our method is shown to enhance the state-of-the-art prediction results of SVM classifiers by more than 10%.</p></sec></abstract><conference><conf-date>13&#x02013;16 January 2009</conf-date><conf-name>The Seventh Asia Pacific Bioinformatics Conference (APBC 2009)</conf-name><conf-loc>Beijing, China</conf-loc></conference></article-meta></front><body><sec><title>Background</title><p>Organelles with different functions are the specialized subunits in a cell. (See Figure <xref ref-type="fig" rid="F1">1</xref>.) Most organelles are closed compartments separated by lipid membranes, such as mitochondria, chloroplasts, peroxisomes, lysosomes, endoplasmic reticulum, cell nucleus and Golgi apparatus. These compartments play different roles, for instance, mitochondria supply chemical energy ATP for cell survive; chloroplasts transform light energy to chemical energy using photosynthesis; peroxisomes participate metabolism process; lysosomes degrade engulfed viruses or bacteria, and destroyed organelles; cell nucleus contains almost genetic information, carried by DNA together with variable proteins to form chromosomes; Golgi apparatus is responsible to package proteins and lipids and modify chemicals to make them functional [<xref ref-type="bibr" rid="B1">1</xref>].</p><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold>Organelles with different functions in a cell</bold>. This figure shows the organelles with different functions are the specialized subunits in a cell. Most organelles are closed compartments separated by lipid membranes, such as mitochondria, chloroplasts, peroxisomes, lysosomes, endoplasmic reticulum, cell nucleus and Golgi apparatus. Protein subcellular located within different organelles plays different role.</p></caption><graphic xlink:href="1471-2105-10-S1-S47-1"/></fig><p>Protein subcellular localization is crucial for genome annotation, protein function prediction, and drug discovery [<xref ref-type="bibr" rid="B2">2</xref>]. Proteins perform their appropriate functions as, and only when, they localize in the correct subcellular compartments. Take prokaryotic and eukaryotic proteins as examples, for prokaryotes, many proteins that are synthesized in the cytoplasm are ultimately found noncytoplasmic locations [<xref ref-type="bibr" rid="B3">3</xref>], such as to a cell membrane or the extracellular environment, while most eukaryotic proteins are encoded in the nuclear and transported to the cytosol for further synthesis.</p><p>The annotations of protein subcellular localization can be detected by various wet-lab experiments. Cell fractionation, electron microscopy and fluorescence microscopy are three major experimental methods for the study of protein subcellular localization. However, the experimental approaches are time-consuming and expensive, so that there is a wide gap between the number of known protein subcellular localizations and the number of uncovered ones. For instance, according to the Swiss-Prot database version 50.0 related on 30-May-2006 the number of protein sequences with localization annotations is just about 14% of total eukaryotic protein entries [<xref ref-type="bibr" rid="B1">1</xref>]. This means that there are about 86% of eukaryotic protein entries without localization labels, which motivates us to find computational methods to predict the protein subcellular localization automatically and accurately.</p><p>Many methods have been developed and applied in an attempt to predict protein subcellular localization. Methods in [<xref ref-type="bibr" rid="B4">4</xref>-<xref ref-type="bibr" rid="B17">17</xref>] are based on amino acid composition to predict localization. Furthermore, scientists took account of sequence order together with amino acid composition to overcome information missing problem [<xref ref-type="bibr" rid="B18">18</xref>-<xref ref-type="bibr" rid="B21">21</xref>]. In addition, supervised learning algorithms such neural networks [<xref ref-type="bibr" rid="B22">22</xref>], K-NN algorithm [<xref ref-type="bibr" rid="B23">23</xref>], SVM [<xref ref-type="bibr" rid="B7">7</xref>,<xref ref-type="bibr" rid="B24">24</xref>,<xref ref-type="bibr" rid="B25">25</xref>] are widely applied to solve this problem. Among these learning-based approaches, SVM is popularly adopted in bioinformatics and is shown to perform relatively better compared to many others. There are also a large number of specialized databases are exploited such as DBSubLoc [<xref ref-type="bibr" rid="B26">26</xref>], ESLPred [<xref ref-type="bibr" rid="B27">27</xref>], HSLpred [<xref ref-type="bibr" rid="B28">28</xref>], LOCSVMPSI [<xref ref-type="bibr" rid="B29">29</xref>], LOC3d [<xref ref-type="bibr" rid="B30">30</xref>], PSORTb [<xref ref-type="bibr" rid="B31">31</xref>], PSORT [<xref ref-type="bibr" rid="B32">32</xref>], LOCtree [<xref ref-type="bibr" rid="B33">33</xref>], BaCelLo [<xref ref-type="bibr" rid="B34">34</xref>], TargetP [<xref ref-type="bibr" rid="B35">35</xref>], SecretomeP [<xref ref-type="bibr" rid="B36">36</xref>], PredictNLS [<xref ref-type="bibr" rid="B37">37</xref>], WoLF PSORT [<xref ref-type="bibr" rid="B38">38</xref>], Proteome Analyst [<xref ref-type="bibr" rid="B39">39</xref>], and CELLO [<xref ref-type="bibr" rid="B40">40</xref>].</p><p>In this paper, we present a novel approach to exploit the use of unlabeled data to aid the overall accuracy of protein subcellular localization and reduce the labeling effort. The existence of the relative large amount of unlabeled data provides us with a chance to mine useful information about the statistical distributions. We resort to two classical machine learning approaches, namely semi-supervised learning and ensemble learning. Experimental results on real biological data sets demonstrate that our efforts can effectively improve the accuracy of the state-of-the-art SVM classifiers with fewer labeled instances.</p></sec><sec><title>Results and discussion</title><sec sec-type="materials"><title>Materials</title><p>This protein dataset includes 7,579 eukaryotic proteins with determined subcellular localizations, which were extracted from SWISS-PROT release 39.0 by Park and Kanehisa [<xref ref-type="bibr" rid="B41">41</xref>] and 34,521 eukaryotic proteins without subcellular localization information also extracted from SWISS-PROT. Within 7,579 proteins, there are 12 localizations: Chloroplast, Cytoplasmic, Cytoskeleton, Endoplasmic reticulum, Extracellular, Golgi apparatus, Lysosomal, Mitochondrial, Nuclear, Peroxisomal, Plasma membrane, Vacuolar. Detailed statistics of this protein dataset is shown in the following Table <xref ref-type="table" rid="T1">1</xref>.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Distribution of protein subcellular localizations</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">Subcellular Localizations</td><td align="center">Number of proteins</td></tr></thead><tbody><tr><td align="center">Chloroplast</td><td align="center">671</td></tr><tr><td align="center">Cytoplasmic</td><td align="center">1241</td></tr><tr><td align="center">Cytoskeletion</td><td align="center">40</td></tr><tr><td align="center">Endoplasmic reticulum</td><td align="center">114</td></tr><tr><td align="center">Extracellular</td><td align="center">861</td></tr><tr><td align="center">Golgi apparatus</td><td align="center">47</td></tr><tr><td align="center">Lysosomal</td><td align="center">93</td></tr><tr><td align="center">Mitochondrial</td><td align="center">727</td></tr><tr><td align="center">Nuclear</td><td align="center">1932</td></tr><tr><td align="center">Peroxisomal</td><td align="center">125</td></tr><tr><td align="center">Plasma membrane</td><td align="center">1674</td></tr><tr><td align="center">Vacuolar</td><td align="center">54</td></tr><tr><td colspan="2"><hr></hr></td></tr><tr><td align="center">Total</td><td align="center">7579</td></tr></tbody></table></table-wrap><p>We adopt the 2-gram protein encoding method to generate feature of amino acid compositions, which is widely used in many existing protein subcellular localization protein systems [<xref ref-type="bibr" rid="B42">42</xref>].</p></sec><sec><title>Empirical evaluations</title><p>We conducted extensive experiments to compare the CoForest approach with other state-of-the art prediction algorithms based on evaluation measurement 'accuracy'. In this paper, accuracy is defined as the proportion of true results, namely,</p><p><disp-formula id="bmcM1"><label>(1)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" name="1471-2105-10-S1-S47-i1" overflow="scroll">                     <mml:semantics>                        <mml:mrow>                           <mml:mi>A</mml:mi>                           <mml:mi>c</mml:mi>                           <mml:mi>c</mml:mi>                           <mml:mi>u</mml:mi>                           <mml:mi>r</mml:mi>                           <mml:mi>a</mml:mi>                           <mml:mi>c</mml:mi>                           <mml:mi>y</mml:mi>                           <mml:mo>=</mml:mo>                           <mml:mfrac>                              <mml:mrow>                                 <mml:mi>T</mml:mi>                                 <mml:mi>P</mml:mi>                                 <mml:mo>+</mml:mo>                                 <mml:mi>T</mml:mi>                                 <mml:mi>N</mml:mi>                              </mml:mrow>                              <mml:mrow>                                 <mml:mi>T</mml:mi>                                 <mml:mi>P</mml:mi>                                 <mml:mo>+</mml:mo>                                 <mml:mi>F</mml:mi>                                 <mml:mi>P</mml:mi>                                 <mml:mo>+</mml:mo>                                 <mml:mi>F</mml:mi>                                 <mml:mi>N</mml:mi>                                 <mml:mo>+</mml:mo>                                 <mml:mi>T</mml:mi>                                 <mml:mi>N</mml:mi>                              </mml:mrow>                           </mml:mfrac>                        </mml:mrow>                                             </mml:semantics>                  </mml:math></disp-formula></p><p>TP means: the number of True Positives</p><p>TN means: the number of True Negatives</p><p>FP means: the number of False Positives</p><p>FN means: the number of False Negatives</p><sec><title>Can we achieve same or better prediction with fewer labeled data?</title><p>We first demonstrate that our semi-supervised learning approach is indeed useful. In the next method section, we will demonstrate that two parameters will affect the overall performance of CoForest. We have chosen different values of <italic>F </italic>and <italic>N </italic>and also different numbers of labeled instances. The labeled instances are drawn randomly from the 12 localization classes in the labeled dataset. We sample the number of labeled instances from 1,000 to 3,000 and also change the number of classifiers from 60 to 200. As a result, the corresponding prediction accuracy on the whole set of 7,579 labeled instances are computed. The results in terms of prediction accuracy are described in Table <xref ref-type="table" rid="T2">2</xref>, Table <xref ref-type="table" rid="T3">3</xref> and Table <xref ref-type="table" rid="T4">4</xref>.</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Performance of Co-Forest Algorithm on the selected datasets(<italic>F </italic>= 20)</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">N</td><td align="center">N = 60</td><td align="center">N = 100</td><td align="center">N = 150</td><td align="center">N = 200</td></tr></thead><tbody><tr><td align="center">Labeled = 1000</td><td align="center">0.5863</td><td align="center">0.604</td><td align="center">0.616</td><td align="center">0.6289</td></tr><tr><td align="center">Labeled = 1200</td><td align="center">0.6285</td><td align="center">0.6539</td><td align="center">0.6716</td><td align="center">0.6936</td></tr><tr><td align="center">Labeled = 1400</td><td align="center">0.6498</td><td align="center">0.674</td><td align="center">0.6907</td><td align="center">0.7018</td></tr><tr><td align="center">Labeled = 1600</td><td align="center">0.703</td><td align="center">0.7251</td><td align="center">0.7352</td><td align="center">0.7425</td></tr><tr><td align="center">Labeled = 1800</td><td align="center">0.7218</td><td align="center">0.7441</td><td align="center">0.7525</td><td align="center">0.7589</td></tr><tr><td align="center">Labeled = 2000</td><td align="center">0.7416</td><td align="center">0.769</td><td align="center">0.78</td><td align="center">0.7613</td></tr><tr><td align="center">Labeled = 2200</td><td align="center">0.7703</td><td align="center">0.7898</td><td align="center">0.7935</td><td align="center">0.7627</td></tr><tr><td align="center">Labeled = 2400</td><td align="center">0.7896</td><td align="center">0.8045</td><td align="center">0.8125</td><td align="center">0.8158</td></tr><tr><td align="center">Labeled = 2600</td><td align="center">0.8035</td><td align="center">0.8181</td><td align="center">0.8208</td><td align="center">0.8209</td></tr><tr><td align="center">Labeled = 2800</td><td align="center">0.8162</td><td align="center">0.8294</td><td align="center">0.8318</td><td align="center">0.8307</td></tr><tr><td align="center">Labeled = 3000</td><td align="center">0.83</td><td align="center">0.834</td><td align="center">0.847</td><td align="center">0.8503</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T3"><label>Table 3</label><caption><p>Performance of Co-Forest Algorithm on the selected datasets(<italic>F </italic>= 40)</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">N</td><td align="center">N = 60</td><td align="center">N = 100</td><td align="center">N = 150</td><td align="center">N = 200</td></tr></thead><tbody><tr><td align="center">Labeled = 1000</td><td align="center">0.5916</td><td align="center">0.6114</td><td align="center">0.6277</td><td align="center">0.631</td></tr><tr><td align="center">Labeled = 1200</td><td align="center">0.6478</td><td align="center">0.6656</td><td align="center">0.6707</td><td align="center">0.6818</td></tr><tr><td align="center">Labeled = 1400</td><td align="center">0.667</td><td align="center">0.6919</td><td align="center">0.6924</td><td align="center">0.6962</td></tr><tr><td align="center">Labeled = 1600</td><td align="center">0.7071</td><td align="center">0.726</td><td align="center">0.7391</td><td align="center">0.7416</td></tr><tr><td align="center">Labeled = 1800</td><td align="center">0.7215</td><td align="center">0.7467</td><td align="center">0.7502</td><td align="center">0.7638</td></tr><tr><td align="center">Labeled = 2000</td><td align="center">0.7423</td><td align="center">0.7598</td><td align="center">0.7613</td><td align="center">0.7707</td></tr><tr><td align="center">Labeled = 2200</td><td align="center">0.7715</td><td align="center">0.7849</td><td align="center">0.7913</td><td align="center">0.8052</td></tr><tr><td align="center">Labeled = 2400</td><td align="center">0.7952</td><td align="center">0.8054</td><td align="center">0.8132</td><td align="center">0.8208</td></tr><tr><td align="center">Labeled = 2600</td><td align="center">0.8092</td><td align="center">0.8341</td><td align="center">0.8244</td><td align="center">0.8303</td></tr><tr><td align="center">Labeled = 2800</td><td align="center">0.8157</td><td align="center">0.8312</td><td align="center">0.8338</td><td align="center">0.8394</td></tr><tr><td align="center">Labeled = 3000</td><td align="center">0.8298</td><td align="center">0.8407</td><td align="center">0.8525</td><td align="center">0.8595</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T4"><label>Table 4</label><caption><p>Performance of Co-Forest Algorithm on the selected dataset (<italic>F </italic>= 60)</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">N</td><td align="center">N = 60</td><td align="center">N = 100</td><td align="center">N = 150</td><td align="center">N = 200</td></tr></thead><tbody><tr><td align="center">Labeled = 1000</td><td align="center">0.5969</td><td align="center">0.6226</td><td align="center">0.6208</td><td align="center">0.632</td></tr><tr><td align="center">Labeled = 1200</td><td align="center">0.6521</td><td align="center">0.6736</td><td align="center">0.6745</td><td align="center">0.6928</td></tr><tr><td align="center">Labeled = 1400</td><td align="center">0.6637</td><td align="center">0.6832</td><td align="center">0.6933</td><td align="center">0.7049</td></tr><tr><td align="center">Labeled = 1600</td><td align="center">0.7075</td><td align="center">0.7228</td><td align="center">0.7337</td><td align="center">0.7483</td></tr><tr><td align="center">Labeled = 1800</td><td align="center">0.7351</td><td align="center">0.7532</td><td align="center">0.7591</td><td align="center">0.7658</td></tr><tr><td align="center">Labeled = 2000</td><td align="center">0.7526</td><td align="center">0.7618</td><td align="center">0.7913</td><td align="center">0.7956</td></tr><tr><td align="center">Labeled = 2200</td><td align="center">0.7736</td><td align="center">0.7888</td><td align="center">0.7873</td><td align="center">0.8027</td></tr><tr><td align="center">Labeled = 2400</td><td align="center">0.7942</td><td align="center">0.8099</td><td align="center">0.8174</td><td align="center">0.823</td></tr><tr><td align="center">Labeled = 2600</td><td align="center">0.812</td><td align="center">0.8235</td><td align="center">0.8252</td><td align="center">0.8295</td></tr><tr><td align="center">Labeled = 2800</td><td align="center">0.8219</td><td align="center">0.831</td><td align="center">0.834</td><td align="center">0.8367</td></tr><tr><td align="center">Labeled = 3000</td><td align="center">0.836</td><td align="center">0.838</td><td align="center">0.8493</td><td align="center">0.8504</td></tr></tbody></table></table-wrap><p>From the results, we can see that by using only about 20% of the labeled instances, we can achieve a prediction accuracy of more than 75%. As a rule of thumb, we can see that the prediction accuracy increases as <italic>F </italic>and <italic>N </italic>increase. This follows from our intuition of the algorithm description in the last section.</p></sec><sec><title>Comparison with baseline algorithms</title><p>We also compared CoForest with a number of machine learning algorithms, such as Decision Tree, AdaBoost and SVM. The reason for us to choose these classifiers as baseline algorithms are as follows: Since the weak learners we use in CoForest algorithm are in fact decision trees, we want to demonstrate the effectiveness of ensemble learning in our approach. Furthermore, since AdaBoost is also one of the most effective ensemble learning algorithms, we want to show that by using AdaBoost one could not achieve the same performance as our classifier does, where AdaBoost did not use unlabeled data to help refine the accuracy. A third choice of our baseline classifiers is the Support Vector Machine (SVM), which is the state-of-the-art algorithm in protein subcellular localization. We use this algorithm to show that our algorithm can perform better by using even fewer labeled instances.</p><p>For all the three baseline algorithms, we did not use any unlabeled instance since they are supervised machine learning algorithms and did not use the information from unlabeled data. We also ranged the number of training instances from 1,000 to 7,579 to show different levels of prediction accuracy as a function of labeled training data.</p><p>For decision tree, we used the C4.5 package implemented in Weka [<xref ref-type="bibr" rid="B43">43</xref>] and tested the algorithm accuracy in two settings. One setting is the ten-fold cross validation, where we randomly split the labeled data into ten folds, where one is used for testing and the other nine for training. This process is iterated ten times and the resulting ten classification accuracy values are averaged to get the final result of ten-fold cross validation. Another test setting is to simply use the whole set of 7,579 labeled instances for testing. For AdaBoost, we applied the AdaBoost package in Weka, and used decision stump as weak learners. Again we use 10-fold cross validation and external testing for the two test settings. Experimental results for these two baseline algorithms are shown in Table <xref ref-type="table" rid="T5">5</xref>. We could see that by using only the tree-based approach on AdaBoost, the overall performance is relatively lower than the CoForest approach.</p><table-wrap position="float" id="T5"><label>Table 5</label><caption><p>Performance of baseline classifiers</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">Labeled Data</td><td align="center">Tree(10-CV)</td><td align="center">Tree(External)</td><td align="center">AdaBoost(10-CV)</td><td align="center">AdaBoost(External)</td></tr></thead><tbody><tr><td align="center">Labeled = 1000</td><td align="center">0.256</td><td align="center">0.3167</td><td align="center">0.124</td><td align="center">0.1124</td></tr><tr><td align="center">Labeled = 1200</td><td align="center">0.275</td><td align="center">0.3628</td><td align="center">0.1442</td><td align="center">0.1989</td></tr><tr><td align="center">Labeled = 1400</td><td align="center">0.2485</td><td align="center">0.3730</td><td align="center">0.1943</td><td align="center">0.2697</td></tr><tr><td align="center">Labeled = 1600</td><td align="center">0.2887</td><td align="center">0.4350</td><td align="center">0.1913</td><td align="center">0.2488</td></tr><tr><td align="center">Labeled = 1800</td><td align="center">0.2772</td><td align="center">0.4465</td><td align="center">0.1917</td><td align="center">0.2210</td></tr><tr><td align="center">Labeled = 2000</td><td align="center">0.2985</td><td align="center">0.4749</td><td align="center">0.2065</td><td align="center">0.2702</td></tr><tr><td align="center">Labeled = 2200</td><td align="center">0.3222</td><td align="center">0.5084</td><td align="center">0.1868</td><td align="center">0.2515</td></tr><tr><td align="center">Labeled = 2400</td><td align="center">0.2913</td><td align="center">0.5296</td><td align="center">0.1929</td><td align="center">0.3544</td></tr><tr><td align="center">Labeled = 2600</td><td align="center">0.3173</td><td align="center">0.5503</td><td align="center">0.1981</td><td align="center">0.2697</td></tr><tr><td align="center">Labeled = 2800</td><td align="center">0.3357</td><td align="center">0.5601</td><td align="center">0.2204</td><td align="center">0.3524</td></tr><tr><td align="center">Labeled = 3000</td><td align="center">0.3387</td><td align="center">0.5842</td><td align="center">0.2160</td><td align="center">0.3544</td></tr><tr><td align="center">Labeled = 7579</td><td align="center">0.4521</td><td align="center">0.6234</td><td align="center">0.3544</td><td align="center">0.4041</td></tr></tbody></table></table-wrap><p>Comparisons between our proposed and the baseline algorithms can be visualized directly in the following figures, figure <xref ref-type="fig" rid="F2">2</xref>, figure <xref ref-type="fig" rid="F3">3</xref> and figure <xref ref-type="fig" rid="F4">4</xref>. These three figures indicate performances of different algorithms based on the same number of labeled training data.</p><fig position="float" id="F2"><label>Figure 2</label><caption><p><bold>Accuracy comparison of different approaches</bold>. Accuracy comparison among Co-Forest F = 20, N = 100, Co-Forest F = 20, N = 150, Co-Forest F = 20, N = 200, decision tree with 10 cross validation, decision tree with external test, AdaBoost with 10 cross validation and AdaBoost with external test when training classifiers with different sample sizes.</p></caption><graphic xlink:href="1471-2105-10-S1-S47-2"/></fig><fig position="float" id="F3"><label>Figure 3</label><caption><p><bold>Accuracy comparison of different approaches</bold>. Accuracy comparison among Co-Forest F = 40, N = 100, Co-Forest F = 40, N = 150, Co-Forest F = 20, N = 200, decision tree with 10 cross validation, decision tree with external test, AdaBoost with 10 cross validation and AdaBoost with external test when training classifiers with different sample sizes.</p></caption><graphic xlink:href="1471-2105-10-S1-S47-3"/></fig><fig position="float" id="F4"><label>Figure 4</label><caption><p><bold>Accuracy comparison of different approaches</bold>. Accuracy comparison among Co-Forest F = 60, N = 100, Co-Forest F = 60, N = 150, Co-Forest F = 60, N = 200, decision tree with 10 cross validation, decision tree with external test, AdaBoost with 10 cross validation and AdaBoost with external test when training classifiers with different sample sizes.</p></caption><graphic xlink:href="1471-2105-10-S1-S47-4"/></fig><p>We next compared the prediction accuracy with Support Vector Machine, which is the state-of-the-art algorithm for protein subcellular localization. Due to time constraint, we did not consider different values of labeled instances when training the SVM classifier, we used the 7,579 labeled instances and did a ten-fold cross validation. We tuned the <italic>&#x003b3; </italic>parameter in RBF kernel, which is a typical setting in protein subcellular localization, and the different values of <italic>&#x003b3; </italic>will undoubtedly affect the overall prediction accuracy. The experimental results are shown in Table <xref ref-type="table" rid="T6">6</xref>.</p><table-wrap position="float" id="T6"><label>Table 6</label><caption><p>Performance of SVM</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center"><italic>&#x003b3;</italic></td><td align="center">Prediction Accuracy</td></tr></thead><tbody><tr><td align="center">0.00001</td><td align="center">0.5865</td></tr><tr><td align="center">0.00002</td><td align="center">0.6162</td></tr><tr><td align="center">0.00004</td><td align="center">0.7050</td></tr><tr><td align="center">0.00008</td><td align="center">0.7524</td></tr><tr><td align="center">0.0001</td><td align="center">0.7661</td></tr><tr><td align="center">0.0002</td><td align="center">0.7807</td></tr><tr><td align="center">0.0004</td><td align="center">0.7957</td></tr><tr><td align="center">0.0008</td><td align="center">0.7887</td></tr><tr><td align="center">0.001</td><td align="center">0.7772</td></tr><tr><td align="center">0.002</td><td align="center">0.7279</td></tr><tr><td align="center">0.004</td><td align="center">0.6048</td></tr><tr><td align="center">0.008</td><td align="center">0.4185</td></tr><tr><td align="center">0.01</td><td align="center">0.3666</td></tr><tr><td align="center">0.02</td><td align="center">0.2827</td></tr><tr><td align="center">0.04</td><td align="center">0.2577</td></tr><tr><td align="center">0.08</td><td align="center">0.2560</td></tr></tbody></table></table-wrap><p>From the results, we can see that SVM could almost achieve a 80% accuracy when <italic>&#x003b3; </italic>is set to 0.0004, and typically the prediction accuracy is between 75% and 80%. However, as shown in our CoForest approach, the prediction accuracy can be increased to 85% when we are using only 3000 labeled instances for training, thus, by using about 40% of labeled instances, one can achieve a 10% performance increase than the state-of-the-art algorithms. This result is very promising.</p></sec></sec></sec><sec><title>Conclusion</title><p>In this paper, we present a semi-supervised learning approach to solve protein subcellular localization problem. One particular feature of protein subcellular localization is that a large amount of unlabeled protein sequences are available but no literature tries to make use of these unlabeled instances. We used the CoForest algorithm and the large number of unlabeled protein sequences for predicting protein subcellular localization. Experimental results show that we can achieve more than 10% accuracy increase than SVM and moreover, we used only about 30% labeled instances to achieve this accuracy.</p><p>There are several possible directions for future research into this CoForest framework. The performance of CoForest may be better enhanced when we incorporate the active learning framework into CoForest, i.e. we could extract more useful information by selecting some representative unlabeled instances, instead of randomly choosing the unlabeled instances. Another possible solution is to further incorporate the transfer learning framework into this approach, where the distribution of unlabeled data may not follow the overall distribution of labeled data. Using a semi-supervised transfer learning approach may further improve the prediction accuracy.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>Related work</title><p>In this paper, our proposed approach is based on the co-training paradigm, which is a very important algorithm in semi-supervised learning. Also, we exploit the ideas from ensemble learning to help improve the overall accuracy. In the following, we briefly introduce some related work in semi-supervised learning and ensemble learning.</p><p>Machine learning, or classification in particular, is concerned with fitting a function that maps a pattern to its corresponding class label based on prior knowledge and a set of features describing the pattern. For a traditional two-class classification problem, we are given a set of samples, i.e. a number of input vectors <bold>x</bold><sub><italic>i </italic></sub>&#x02208; &#x0211d;<sup><italic>d</italic></sup>(<italic>i </italic>= 1, 2,..., <italic>N</italic>) with corresponding labels <italic>y</italic><sub><italic>i </italic></sub>&#x02208; {+1, -1}(<italic>i </italic>= 1, 2,..., <italic>N</italic>), where <italic>N </italic>is the number of labeled instances and <italic>d </italic>is the dimension cardinality of each training instance (that is, the number of features). The goal of a learning algorithm is to construct a binary classifier or a decision function which takes a new <bold>x </bold>as input and derives a corresponding label <italic>y </italic>&#x02208; {+1, -1} based on the given labeled data. Typically, features are manually chosen to quantitatively describe each training instance or extract the most important values that can distinguish one class with another. From the view of statistical machine learning, experimental results usually show that the larger the <italic>N </italic>is, the better the overall prediction accuracy will be. As mentioned in the last section, manually labeling the data is a time-consuming task. There exists a large amount of unlabeled proteins, which traditionally are not taken into account in overall prediction. However, we think this is a mistake.</p><p>In traditional classification, all training data should be labeled before learning and the learned classifiers depend on these labeled data. When a large portion of unlabeled data are also available, a new opportunity is presented to improve the learning performance. An effective approach that has been used by machine learning researchers is the <italic>semi-supervised learning framework</italic>, where an initial hypothesis is first learned from the labeled data and then this hypothesis is refined, using the unlabeled data by some automatic labeling strategies, in several iterations.</p><p>There have been many approaches or algorithms that fall into the semi-supervised framework. Interested readers can refer to Zhu's survey on semi-supervised learning [<xref ref-type="bibr" rid="B44">44</xref>] for a comprehensive explanation about what semi-supervised learning is and some latest results.</p><p>Typical semi-supervised algorithms include the EM algorithms to estimate the parameters of the generative model and the probability of unlabeled examples in each class [<xref ref-type="bibr" rid="B45">45</xref>]; transductive inference for support vector machines [<xref ref-type="bibr" rid="B46">46</xref>,<xref ref-type="bibr" rid="B47">47</xref>], and so on.</p><p>The co-training paradigm is one of the early proposed framework that was well studied and developed [<xref ref-type="bibr" rid="B48">48</xref>]. In co-training, two classifiers are trained on two sets of attributes/features respectively. Each classifier will choose to label some unlabeled data for which they feel they are most "confident" with. These newly labeled examples are then added to the labeled training set of the other classifier. After that, each classifier is retrained using the augmented labeled data set, hoping that the "most confident" instances labeled by the other classifier will improve the generalization ability of the classifier learnt in this iteration. This process is repeated till converge is reached, or the difference in the classifiers learned in previous two rounds is relatively small. Co-training has been successfully applied in many applications, including statistical parsing [<xref ref-type="bibr" rid="B49">49</xref>], visual detection [<xref ref-type="bibr" rid="B50">50</xref>], etc.</p><p>Therefore, we believe it would be interesting to apply semi-supervised algorithm based on the co-training framework to the problem of protein subcellular localization. To our best knowledge, there has been no work that tries to solve the protein subcellular localization problem via a semi-supervised learning approach.</p><p>Ensemble learning is a very important machine learning framework that was usually explained as "wisdom of the crowds". In ensemble learning, multiple learners are trained and then their predictions are combined in order to make more accurate predictions. Experiments in many real-world datasets across a large number of domains show that ensemble learning can effectively improve the accuracy or generalization ability of many classifiers.</p><p>An ensemble learning algorithm usually has two steps, in which the first is to generate multiple classifiers and the second is to combine their predictions. Current trends tend to categorize ensemble learning algorithms in two categories, considering whether they generate the classifiers in a parallel way or a sequential way.</p><p>For the first category, where the multiple classifiers are generated in a parallel way, some representative algorithms include Bagging [<xref ref-type="bibr" rid="B51">51</xref>], which generates each classifier based on a training set bootstrapped from the original training set, this generating process can be done in a parallel way since different bootstrapping process do not affect each other. The predictions of these classifiers are combined using a majority voting. Other algorithms that fall into this category include stacking predictors [<xref ref-type="bibr" rid="B52">52</xref>], random subspace [<xref ref-type="bibr" rid="B53">53</xref>], random forest [<xref ref-type="bibr" rid="B54">54</xref>], etc.</p><p>For the second category, the most important and representative algorithm is AdaBoost [<xref ref-type="bibr" rid="B55">55</xref>], which sequentially generates a number of classifiers. The subsequent classifiers are targeted on the misclassified examples by the former classifiers.</p><p>Ensemble learning has been successful in many fields, including the protein subcellular localization problem. Recently Shen et al. [<xref ref-type="bibr" rid="B1">1</xref>] presents an ensemble learning algorithm for protein subcellular localization. Our approach combines semi-supervised learning and ensemble learning in hopes of much better prediction results for the biological problem.</p></sec><sec><title>Proposed approach</title><p>In this paper, we use a new co-training style algorithm that was first proposed by Li and Zhou [<xref ref-type="bibr" rid="B56">56</xref>] which extends the co-training paradigm by an ensemble algorithm named Random Forest [<xref ref-type="bibr" rid="B54">54</xref>].</p><p>We let <italic>L </italic>and <italic>U </italic>denote the labeled set and unlabeled set. In co-training, two classifiers are trained from <italic>L </italic>and then each of them selects the most confident examples in <italic>U </italic>to label, from their own classifying function or separating hyperplane, respectively. Thus, an important part of co-training lies in how to estimate the confidence of prediction, in other words, how to estimate the confidence of an unlabeled example.</p><p>In Li and Zhou's proposed Co-Forest algorithm [<xref ref-type="bibr" rid="B56">56</xref>], an ensemble of <italic>N </italic>classifiers denoted as <italic>H* </italic>is used in co-training instead of two classifiers. In this way, we can estimate the confidence of each classifier efficiently. If we want to consider the most confidently labeled example by a certain component classifier of the ensemble <italic>h</italic><sub><italic>i</italic></sub>(<italic>i </italic>= 1, 2,..., <italic>N</italic>), we use all other component classifiers except <italic>h</italic><sub><italic>i</italic></sub>, called the <italic>concomitant ensemble </italic>of <italic>hi </italic>and denoted by <italic>H</italic><sub><italic>i</italic></sub>. Therefore, the confidence of labeling can be computed as the degree of agreements on the labeling, i.e. the number of classifiers that agree on the label assigned by <italic>H</italic><sub><italic>i</italic></sub>. The overall idea of CoForest is to firstly train an ensemble of classifiers from labeled dataset <italic>L </italic>and then refine each classifier with unlabeled data by its concomitant ensemble.</p><p>More specifically, in each learning iteration round of CoForest, the concomitant ensemble <italic>H</italic><sub><italic>i </italic></sub>will test each example in <italic>U</italic>. If the number of classifiers that agree on a particular label exceeds a pre-defined threshold <italic>&#x003b8;</italic>, the unlabeled example, labeled with this newly assigned label is copied into the newly labeled set <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M2" name="1471-2105-10-S1-S47-i2" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:msup><mml:mi>L</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> Then for this round, set <italic>L cup</italic><inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M3" name="1471-2105-10-S1-S47-i2" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:msup><mml:mi>L</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is used for refining <italic>h</italic><sub><italic>i </italic></sub>in this iteration. Note that the unlabeled examples are not removed from <italic>U</italic>, so they might be selected by other <italic>H</italic><sub><italic>j</italic></sub>(<italic>j </italic>&#x02260; <italic>i</italic>) in the following iterations. One problem that may affect the overall performance of CoForest is that all the unlabeled data whose prediction confidence that are above <italic>&#x003b8; </italic>will be added to <italic>L</italic><sub><italic>i</italic></sub>, thus making <italic>L</italic><sub><italic>i </italic></sub>rather large in the future. But in case the learned classifier cannot represent the underlying distribution, such a huge amount of labeled data will indeed hurt the performance, instead of helping the prediction accuracy. This phenomenon was discovered in several semi-supervised learning algorithms. Inspired by Nigam et al [<xref ref-type="bibr" rid="B45">45</xref>], CoForest also assigns a weight to each unlabeled example. An example is weighted by the predictive confidence of a concomitant ensemble. This approach makes the influence of <italic>&#x003b8; </italic>insensitive, even if <italic>&#x003b8; </italic>is small, the influences of examples with low predictive confidence can be limited.</p><p>In the CoForest algorithm, <italic>N </italic>random trees are firstly initiated from the bootstrapped training set from the labeled set <italic>L </italic>to create a random forest. Then in each iteration, each random tree will be refined with the newly labeled examples by its concomitant ensemble, where the confidence of the labeled example exceeds a certain threshold <italic>&#x003b8;</italic>. This method will reduce the chance of the trees in a random forest being biased when we utilize the unlabeled data.</p><p>For detailed descriptions of CoForest algorithm, interested readers could refer to [<xref ref-type="bibr" rid="B56">56</xref>] for details.</p></sec></sec><sec><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec><sec><title>Authors' contributions</title><p>Qian XU and Derek Hao Hu identified the data set, conducted the experiments, and analyzed the experimental results. Hannah Hong Xue, Weichuan Yu and Qiang Yang designed and directed the research. All author worked on the manuscript. All authors read and approved the final manuscript</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>We thank the support of HKUST Project RPC06/07.EG09.</p><p>This article has been published as part of <italic>BMC Bioinformatics </italic>Volume 10 Supplement 1, 2009: Proceedings of The Seventh Asia Pacific Bioinformatics Conference (APBC) 2009. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10?issue=S1"/></p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>HB</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Chou</surname><given-names>KC</given-names></name></person-group><article-title>Euk-PLoc: an ensemble classifier for large-scale eukaryotic protein subcellular location prediction</article-title><source>Amino Acids</source><year>2007</year><volume>33</volume><fpage>57</fpage><lpage>67</lpage><pub-id pub-id-type="pmid">17235453</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Su</surname><given-names>ECY</given-names></name><name><surname>Chiu</surname><given-names>HS</given-names></name><name><surname>Lo</surname><given-names>A</given-names></name><name><surname>Hwang</surname><given-names>JK</given-names></name><name><surname>yi Sung</surname><given-names>T</given-names></name><name><surname>Hsu</surname><given-names>WL</given-names></name></person-group><article-title>Protein subcellular localization prediction based on compartment-specific feature and structure conservation</article-title><source>BMC Bioinformatics</source><year>2007</year><volume>8</volume><fpage>330</fpage><pub-id pub-id-type="pmid">17825110</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Claros</surname><given-names>M</given-names></name><name><surname>Brunak</surname><given-names>S</given-names></name><name><surname>Heijne</surname><given-names>G</given-names></name></person-group><article-title>Prediction of N-terminal protein sorting signals</article-title><source>Current Opinion in Structural Biology</source><year>1997</year><volume>7</volume><fpage>394</fpage><lpage>398</lpage><pub-id pub-id-type="pmid">9204282</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Muskal</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name></person-group><article-title>Predicting protein secondary structure-content-a tandem neural network approach</article-title><source>J Mol Biol</source><year>1992</year><volume>225</volume><fpage>713</fpage><lpage>727</lpage><pub-id pub-id-type="pmid">1602478</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bahar</surname><given-names>I</given-names></name><name><surname>Atilgan</surname><given-names>A</given-names></name><name><surname>Jernigan</surname><given-names>R</given-names></name><name><surname>Erman</surname><given-names>B</given-names></name></person-group><article-title>Understanding the recognition of protein structural classes by amino acid composition</article-title><source>Proteins</source><year>1997</year><volume>29</volume><fpage>172</fpage><lpage>185</lpage><pub-id pub-id-type="pmid">9329082</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>G</given-names></name></person-group><article-title>Prediction of protein structural classes by neural network</article-title><source>Biochimie</source><year>2000</year><volume>82</volume><fpage>783</fpage><lpage>785</lpage><pub-id pub-id-type="pmid">11018296</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>G</given-names></name></person-group><article-title>Support vector machines for predicting protein structural class</article-title><source>BMC Bioinformatics</source><year>2001</year><volume>2</volume><fpage>3</fpage><pub-id pub-id-type="pmid">11483157</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>K</given-names></name></person-group><article-title>A novel approach to predicting protein structural classes in a (20-1)-D amino-acid-composition space</article-title><source>Proteins</source><year>1995</year><volume>21</volume><fpage>319</fpage><lpage>344</lpage><pub-id pub-id-type="pmid">7567954</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>K</given-names></name></person-group><article-title>Prediction of protein structural classes and subcellular locations</article-title><source>Curr Protein Pept Sci</source><year>2000</year><volume>1</volume><fpage>171</fpage><lpage>208</lpage><pub-id pub-id-type="pmid">12369916</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name></person-group><article-title>A correlation-coefficient method to predicting protein-structural classes from amino-acid compositions</article-title><source>Eur J Biochem</source><year>1992</year><volume>207</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="pmid">1633801</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name></person-group><article-title>Predicting protein-folding types by distance functions that make allowances for amino-acid interactions</article-title><source>J Biol Chem</source><year>1994</year><volume>269</volume><fpage>22014</fpage><lpage>22020</lpage><pub-id pub-id-type="pmid">8071322</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>R</given-names></name><name><surname>Feng</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><article-title>Prediction of protein structural class by amino acid and polypeptide composition</article-title><source>Eur J Biochem</source><year>2002</year><volume>269</volume><fpage>4219</fpage><lpage>4225</lpage><pub-id pub-id-type="pmid">12199700</pub-id></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Chou</surname><given-names>K</given-names></name><name><surname>Maggiora</surname><given-names>G</given-names></name></person-group><article-title>Predicting protein structural classes from amino-acid-composition-application of fuzzy clustering</article-title><source>Protein Eng</source><year>1995</year><volume>8</volume><fpage>425</fpage><lpage>435</lpage><pub-id pub-id-type="pmid">8532663</pub-id></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Chou</surname><given-names>K</given-names></name></person-group><article-title>An optimization approach to predicting protein structural class from amino-acid-composition</article-title><source>Protein Sci</source><year>1992</year><volume>1</volume><fpage>401</fpage><lpage>408</lpage><pub-id pub-id-type="pmid">1304347</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>G</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name></person-group><article-title>A weighting method for predicting protein structural class from amino-acid-composition</article-title><source>Eur J Biochem</source><year>1992</year><volume>210</volume><fpage>747</fpage><lpage>749</lpage><pub-id pub-id-type="pmid">1483458</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>G</given-names></name></person-group><article-title>An intriguing controversy over protein structural class prediction</article-title><source>J Protein Chem</source><year>1998</year><volume>17</volume><fpage>729</fpage><lpage>738</lpage><pub-id pub-id-type="pmid">9988519</pub-id></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>G</given-names></name><name><surname>Assa-Munt</surname><given-names>N</given-names></name></person-group><article-title>Some insights into protein structural class prediction</article-title><source>Proteins</source><year>2001</year><volume>44</volume><fpage>57</fpage><lpage>59</lpage><pub-id pub-id-type="pmid">11354006</pub-id></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>K</given-names></name></person-group><article-title>Prediction of protein cellular attributes using pseudo-amino acid composition</article-title><source>Proteins</source><year>2001</year><volume>43</volume><fpage>246</fpage><lpage>255</lpage><pub-id pub-id-type="pmid">11288174</pub-id></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Nakashima</surname><given-names>H</given-names></name><name><surname>Nishikawa</surname><given-names>K</given-names></name><name><surname>Ooi</surname><given-names>T</given-names></name></person-group><article-title>The folding type of a protein is relevant to the amino-acid-composition</article-title><source>J Biochem (Tokyo)</source><year>1986</year><volume>99</volume><fpage>153</fpage><lpage>162</lpage><pub-id pub-id-type="pmid">3957893</pub-id></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>K</given-names></name></person-group><article-title>Using pair-coupled amino acid composition to predict protein secondary structure content</article-title><source>J Protein Chem</source><year>1999</year><volume>18</volume><fpage>473</fpage><lpage>480</lpage><pub-id pub-id-type="pmid">10449044</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Chou</surname><given-names>K</given-names></name></person-group><article-title>Prediction of protein secondary structure content</article-title><source>Protein Eng</source><year>1999</year><volume>12</volume><fpage>1041</fpage><lpage>1050</lpage><pub-id pub-id-type="pmid">10611397</pub-id></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Reinhardt</surname><given-names>A</given-names></name><name><surname>Hubbard</surname><given-names>T</given-names></name></person-group><article-title>Using neural networks for prediction of the subcellular location of proteins</article-title><source>Nucleic Acid Res</source><year>1998</year><volume>26</volume><fpage>2230</fpage><lpage>2236</lpage><pub-id pub-id-type="pmid">9547285</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name></person-group><article-title>Prediction of protein subcellular locations using fuzzy k-NN method</article-title><source>Bioinformatics</source><year>2004</year><volume>20</volume><fpage>21</fpage><lpage>28</lpage><pub-id pub-id-type="pmid">14693804</pub-id></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hua</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>Z</given-names></name></person-group><article-title>Support vector machine approach for protein subcellular localization prediction</article-title><source>Bioinformatics</source><year>2001</year><volume>7</volume><fpage>721</fpage><lpage>728</lpage><pub-id pub-id-type="pmid">11524373</pub-id></citation></ref><ref id="B25"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chine-Sheng Yu</surname><given-names>CJL</given-names></name><name><surname>Hwang</surname><given-names>JK</given-names></name></person-group><article-title>Predicting subcellular localization of proteins for Gram-negative bacteria by support vector machine based on n-peptide compositions</article-title><source>Protein Sci</source><year>2004</year><volume>13</volume><fpage>1402</fpage><lpage>1406</lpage><pub-id pub-id-type="pmid">15096640</pub-id></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>T</given-names></name><name><surname>Hua</surname><given-names>S</given-names></name><name><surname>Ji</surname><given-names>X</given-names></name><name><surname>Sun</surname><given-names>Z</given-names></name></person-group><article-title>DBSubLoc: database of protein subcellular localization</article-title><source>Nucleic Acids Res</source><year>2004</year><volume>32</volume><fpage>D122</fpage><lpage>D124</lpage><pub-id pub-id-type="pmid">14681374</pub-id></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bhasin</surname><given-names>M</given-names></name><name><surname>Raghava</surname><given-names>GPS</given-names></name></person-group><article-title>ESLpred: SVM-based method for subcellular localization of eukaryotic proteins using dipeptide composition and PSI-BLAST</article-title><source>Nucleic Acids Res</source><year>2004</year><volume>32</volume><fpage>414</fpage><lpage>419</lpage></citation></ref><ref id="B28"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Garg</surname><given-names>A</given-names></name><name><surname>Bhasin</surname><given-names>M</given-names></name><name><surname>Raghava</surname><given-names>GPS</given-names></name></person-group><article-title>SVM-based method for subcellular localization of human proteins using amino acid compositions, their order and similarity search</article-title><source>J Biol Chem</source><comment></comment></citation></ref><ref id="B29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Fan</surname><given-names>Z</given-names></name><name><surname>Feng</surname><given-names>H</given-names></name></person-group><article-title>LOCSVMPSI: a web server for subcellular localization of eukaryotic proteins using SVM and profile of PSI-BLAST</article-title><source>Nucleic Acids Research</source><year>2005</year><volume>33</volume><fpage>105</fpage><lpage>110</lpage></citation></ref><ref id="B30"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>R</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name></person-group><article-title>LOC3D: annotate sub-cellular localization for protein structures</article-title><source>Nucleic Acids Research</source><year>2003</year><volume>31</volume><fpage>3337</fpage><lpage>3340</lpage><pub-id pub-id-type="pmid">12824321</pub-id></citation></ref><ref id="B31"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gardy</surname><given-names>J</given-names></name><name><surname>Spencer</surname><given-names>C</given-names></name></person-group><article-title>PSORT-B: Improving protein subcellular localization prediction for Gram-negative bacteria</article-title><source>Nucleic Acids Res</source><year>2003</year><volume>31</volume><fpage>3613</fpage><lpage>3617</lpage><pub-id pub-id-type="pmid">12824378</pub-id></citation></ref><ref id="B32"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Nakai</surname><given-names>K</given-names></name><name><surname>Kanehisa</surname><given-names>M</given-names></name></person-group><article-title>Expert system for predicting protein localization sites in gram-negative bacteria</article-title><source>Proteins</source><year>1991</year><volume>11</volume><fpage>95</fpage><lpage>110</lpage><pub-id pub-id-type="pmid">1946347</pub-id></citation></ref><ref id="B33"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>R</given-names></name><name><surname>Rost</surname><given-names>B</given-names></name></person-group><article-title>Mimicking cellular sorting improves prediction of Subcellular Localization</article-title><source>J Mol Biol</source><year>2005</year><volume>348</volume><fpage>85</fpage><lpage>100</lpage><pub-id pub-id-type="pmid">15808855</pub-id></citation></ref><ref id="B34"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pierleoni</surname><given-names>A</given-names></name><name><surname>Martelli</surname><given-names>PL</given-names></name><etal></etal></person-group><article-title>BaCelLo: a balanced subcellular localization predictor</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>e408</fpage><lpage>e416</lpage><pub-id pub-id-type="pmid">16873501</pub-id></citation></ref><ref id="B35"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Emanuelsson</surname><given-names>O</given-names></name><name><surname>Nielsen</surname><given-names>H</given-names></name><etal></etal></person-group><article-title>Predicting subcellular localization of proteins based on their N-terminal amino acid sequence</article-title><source>J Mol Biol</source><year>2000</year><volume>300</volume><fpage>1005</fpage><lpage>1016</lpage><pub-id pub-id-type="pmid">10891285</pub-id></citation></ref><ref id="B36"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>JD</surname><given-names>B</given-names></name><name><surname>Jensen</surname><given-names>L</given-names></name></person-group><article-title>Feature-based prediction of non-classical and leaderless protein secretion</article-title><source>Protein Eng Des Sel</source><year>2004</year><volume>17</volume><fpage>349</fpage><lpage>356</lpage><pub-id pub-id-type="pmid">15115854</pub-id></citation></ref><ref id="B37"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>RPC</given-names></name><etal></etal></person-group><article-title>NLSdb: database of nuclear localization signals</article-title><source>Nucleic Acids Res</source><year>2003</year><volume>31</volume><fpage>397</fpage><lpage>399</lpage><pub-id pub-id-type="pmid">12520032</pub-id></citation></ref><ref id="B38"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Horton</surname><given-names>P</given-names></name><name><surname>Park</surname><given-names>KJ</given-names></name><name><surname>Obayashi</surname><given-names>T</given-names></name><name><surname>Nakai</surname><given-names>K</given-names></name></person-group><article-title>Protein Subcellular Localization Prediction with WoLF PSORT</article-title><source>Proceedings of Asian Pacific Bioinformatics Conference: 13&#x02013;16 Feb 2006; Taiwan</source><year>2006</year><fpage>39</fpage><lpage>48</lpage></citation></ref><ref id="B39"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Szafron</surname><given-names>D</given-names></name><etal></etal></person-group><article-title>Predicting subcellular localization of proteins using machine-learned classifiers</article-title><source>Bioinformatics</source><year>2004</year><volume>20</volume><fpage>547</fpage><lpage>556</lpage><pub-id pub-id-type="pmid">14990451</pub-id></citation></ref><ref id="B40"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>C</given-names></name><name><surname>Lin</surname><given-names>C</given-names></name><name><surname>Hwang</surname><given-names>J</given-names></name></person-group><article-title>Predicting subcellular localization of proteins for Gram-negative bacteria by support vector machines based on n-peptide compositions</article-title><source>Protein Sci</source><year>2004</year><volume>13</volume><fpage>1402</fpage><lpage>1406</lpage><pub-id pub-id-type="pmid">15096640</pub-id></citation></ref><ref id="B41"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>K</given-names></name><name><surname>M</surname><given-names>K</given-names></name></person-group><article-title>Prediction of Protein Subcellular Locations by Support Vectors Machines using Compositions of Amino Acids an Amino Acid Pairs</article-title><source>Bioinformatics</source><year>2003</year><volume>19</volume><fpage>1656</fpage><lpage>1663</lpage><pub-id pub-id-type="pmid">12967962</pub-id></citation></ref><ref id="B42"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Sung</surname><given-names>WK</given-names></name><name><surname>Krishnan</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>KB</given-names></name></person-group><article-title>Protein subcellular localization prediction for Gram-negative bacteria using amino acid subalphabets and a combination of multiple support vector machines</article-title><source>Bioinformatics</source><year>2005</year><volume>6</volume><fpage>174</fpage><pub-id pub-id-type="pmid">16011808</pub-id></citation></ref><ref id="B43"><citation citation-type="other"><article-title>Weke package</article-title><ext-link ext-link-type="uri" xlink:href="http://www.cs.waikato.ac.nz/ml/weka/"/></citation></ref><ref id="B44"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>X</given-names></name></person-group><article-title>Semi-Supervised Learning Literature Survey</article-title><source>Tech Rep 1530</source><year>2005</year><publisher-name>Computer Sciences, University of Wisconsin-Madison</publisher-name></citation></ref><ref id="B45"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Nigam</surname><given-names>K</given-names></name><name><surname>McCallum</surname><given-names>A</given-names></name><name><surname>Thrun</surname><given-names>S</given-names></name><name><surname>Mitchell</surname><given-names>TM</given-names></name></person-group><article-title>Text Classification from Labeled and Unlabeled Documents using EM</article-title><source>Machine Learning</source><year>2000</year><volume>39</volume><fpage>103</fpage><lpage>134</lpage></citation></ref><ref id="B46"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Joachims</surname><given-names>T</given-names></name></person-group><article-title>Transductive Inference for Text Classification using Support Vector Machines</article-title><source>ICML</source><year>1999</year><fpage>200</fpage><lpage>209</lpage></citation></ref><ref id="B47"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Joachims</surname><given-names>T</given-names></name></person-group><article-title>Transductive Learning via Spectral Graph Partitioning</article-title><source>ICML</source><year>2003</year><fpage>290</fpage><lpage>297</lpage></citation></ref><ref id="B48"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Blum</surname><given-names>A</given-names></name><name><surname>Mitchell</surname><given-names>TM</given-names></name></person-group><article-title>Combining Labeled and Unlabeled Sata with Co-Training</article-title><source>COLT</source><year>1998</year><fpage>92</fpage><lpage>100</lpage></citation></ref><ref id="B49"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Sarkar</surname><given-names>A</given-names></name></person-group><article-title>Applying Co-Training Methods to Statistical Parsing</article-title><source>NAACL</source><year>2001</year><fpage>1</fpage><lpage>8</lpage></citation></ref><ref id="B50"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Levin</surname><given-names>A</given-names></name><name><surname>Viola</surname><given-names>PA</given-names></name><name><surname>Freund</surname><given-names>Y</given-names></name></person-group><article-title>Unsupervised Improvement of Visual Detectors using Co-Training</article-title><source>ICCV</source><year>2003</year><fpage>626</fpage><lpage>633</lpage></citation></ref><ref id="B51"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Bagging Predictors</article-title><source>Machine Learning</source><year>1996</year><volume>24</volume><fpage>123</fpage><lpage>140</lpage></citation></ref><ref id="B52"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Stacked Regressions</article-title><source>Machine Learning</source><year>1996</year><volume>24</volume><fpage>49</fpage><lpage>64</lpage></citation></ref><ref id="B53"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>TK</given-names></name></person-group><article-title>The Random Subspace Method for Constructing Decision Forests</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>1998</year><volume>20</volume><fpage>832</fpage><lpage>844</lpage></citation></ref><ref id="B54"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Random Forests</article-title><source>Machine Learning</source><year>2001</year><volume>45</volume><fpage>5</fpage><lpage>32</lpage></citation></ref><ref id="B55"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Freund</surname><given-names>Y</given-names></name><name><surname>Schapire</surname><given-names>RE</given-names></name></person-group><article-title>A decision-theoretic generalization of on-line learning and an application to boosting</article-title><source>EuroCOLT</source><year>1995</year><fpage>23</fpage><lpage>37</lpage></citation></ref><ref id="B56"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>ZH</given-names></name></person-group><article-title>Improve Computer-Aided Diagnosis With Machine Learning Techniques Using Undiagnosed Samples</article-title><source>IEEE Transactions on Systems, Man, and Cybernetics, Part A</source><year>2007</year><volume>37</volume><fpage>1088</fpage><lpage>1098</lpage></citation></ref></ref-list></back></article>